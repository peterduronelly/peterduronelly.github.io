[{"authors":["Peter Duronelly"],"categories":[],"content":"Machine learning is not the ideal tool for time series forecasting for a number of reasons, but, as I will demonstrate it in a future post, limited models can be built for short-term forecasting exercises. One aspect of time series data is, however, that you can’t split your observations randomly into train and test subsets: you train on an early interval and test on a later one. Standard ML libraries, such as scikit-learn, don’t provide a tool for that. This post will show how we can split time series data for a machine learning model.\nSplitting data between training and testing for time series is different from cross-sectional data as the underlying population is not a set of different individual units but the same observable thing(s) in different time periods. The model is trained on an early part of the observed period and we want to test it on the more recent interval. This assumes a few things, first and foremost the persistence of the patterns over time, but this is not only critical for ML modeling. Standard time series analysis methods, let them be stochastic or non-stochastic, also assume this kind of persistence.\nI am planning a longer post on demonstrating the viability of ML in time series predictions, but before that I’d like to show a simple solution to the train-test-split problem when the split cannot happen randomly. I am using a set of macroeconomic variables to forecast the development of Case-Shiller Home Price Index, where I also want to illustrate the importance of domain knowledge in the modeling exercise.\nThe data is coming from FRED, the St. Louis Fed Economic Database in a csv file. Since feature engineering is best done in pandas, the input object for the train-tests-split method is also a pandas data frame. The method’s extra output beyond the subsets is the list of feature names.\nimport numpy as np import pandas as pd import datetime hp = pd.read_csv(\u0026quot;house_price_changes.csv\u0026quot;) hp.head()  The first column should be the date column, but the csv downloaded from FRED does not have either the proper name or the proper format. Looking at the schema tells us that we have 32 variables, 119 periods and our data starts with an unnamed ‘non-null object’, which should be the date column.\nhp.info()  Using the datatime.date() function we can transform the uninterpretable mess into a neat date variable.\nhp.rename(columns={'Unnamed: 0': 'date'}, inplace=True) hp['date'] = pd.to_datetime(hp['date']) hp.info()  The easiest way to split a dataframe by column values is using a mask. For instance if we want to split our data for dates before and after (not before) January 1st, 2013 we can simply say:\nmask = hp['date'] \u0026lt; datetime.date(2013,1,1) hp_train = hp[mask] hp_test = hp[-mask]  To get things organized we’d better write a function to train-test-split our dataframe and to get the feature names accordingly. We need the following steps:\n Split to train and test. Drop the date column if it is not a feature in the model. Split both train and test to X (features) and y (target). Get the list of column names. Convert the two X and y dataframes to arrays.  def train_test_split_timeseries(input_dataframe, target, timecolumn, year, month, day, dropdates = True): \u0026quot;\u0026quot;\u0026quot; The function splits a dataframe containing a time series into non-random train and test subsets. The last observation in the train data is the latest datetime value in the data which precedes the breakpoint given by the (year, month, day) value. The first observation in the test data is the breakpoint given by the (year, month, day) value or the first observation afterwards. Parameters: input_dataframe (Pandas dataframe): The data file with the time series data. target (string): Name of the target variable in the input dataframe. timecolumn (string): The name of the time colummn for splitting the dataframe (usually a date column). year, month, day (int): The year, month, day components of the breakpoint. dropdates (boolean): Whether or not to drop the date column to produce the train/test data. Defaults to True. Returns: X_train (array): A numpy array of training input data. y_train (array): A numpy array of training target data. X_test (array): A numpy array of test input data. y_test (array): A numpy array of test target data. feature_names (list): A list of feature names used in the input matrix. \u0026quot;\u0026quot;\u0026quot; # Split to train and test periods. model_df = input_dataframe target = target timecolumn = timecolumn mask = model_df[timecolumn] \u0026lt; datetime.date(year,month,day) model_df_train = model_df[mask] model_df_test = model_df[-mask] # Drop date column if dropdates = True if dropdates: model_df_train = model_df_train.drop(['date'], axis=1) model_df_test = model_df_test.drop(['date'], axis=1) # Split both train and test to X (input) and y (target) X_train = model_df_train.drop([target], axis=1) y_train = model_df_train[target] X_test = model_df_test.drop([target], axis=1) y_test = model_df_test[target] # Get column names for variable importance feature_names = list(X_train) # Convert X_train, X_test, y_train, y_test to numpy arrays X_train = X_train.as_matrix() X_test = X_test.as_matrix() y_train = y_train.as_matrix() y_test = y_test.as_matrix() return X_train, y_train, X_test, y_test, feature_names  We can now put the function in use. In this particular exercise the target variable is the quarterly change of the Case Shiller Home Price Index (\u0026lsquo;Case_Shiller_HPI_chg\u0026rsquo;), the last period in the train set is Q4 2012, and the first period in the test set is Q1 2013.\nX_train, y_train, X_test, y_test, feature_names = train_test_split_timeseries(hp, 'Case_Shiller_HPI_chg', 'date', 2013, 1,1)  When applying the function we get the following output objects.\nprint('Feature_names') print(feature_names) print() print(\u0026quot;X_train type:\u0026quot;, type(X_train), \u0026quot;, shape:\u0026quot;, X_train.shape) print(\u0026quot;y_train type:\u0026quot;, type(y_train), \u0026quot;, shape:\u0026quot;, y_train.shape) print(\u0026quot;X_test type:\u0026quot;, type(X_test), \u0026quot;, shape:\u0026quot;, X_test.shape) print(\u0026quot;y_test type:\u0026quot;, type(y_test), \u0026quot;, shape:\u0026quot;, y_test.shape)  Now we have our train and test subsets for time series forecasting. The model will be trained on the first interval. The patterns learned on this interval will then be projected to the second interval to test the model. This is what I will show in the next post.\nCodes are at the usual place.\n","date":1542725104,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542725104,"objectID":"1ea027f6d57a5ac42db0cc8bf344a00a","permalink":"https://peterduronelly.github.io/post/train-test-splitting-time-series-data/","publishdate":"2018-11-20T15:45:04+01:00","relpermalink":"/post/train-test-splitting-time-series-data/","section":"post","summary":"Machine learning is not the ideal tool for time series forecasting for a number of reasons, but, as I will demonstrate it in a future post, limited models can be built for short-term forecasting exercises. One aspect of time series data is, however, that you can’t split your observations randomly into train and test subsets: you train on an early interval and test on a later one. Standard ML libraries, such as scikit-learn, don’t provide a tool for that.","tags":["Python","ML"],"title":"Train Test Splitting Time Series Data","type":"post"},{"authors":["Peter Duronelly"],"categories":[],"content":"In a previous post I showed how to use data science tools to find hidden features in unstructured text and analyzed how the complexity of the lyrics of Beatles songs changed over time. In this post I do a little follow-up and compare complete works of The Beatles with that of two others using the same methodology and metrics. Comparing Beatles with other musicians may help put the original numbers into the perspective.\nI downloaded lyrics from the progressive rock band ‘The Alan Parsons Project’ and Nobel laureate Bob Dylan. Alan Parsons is an audio engineer, musician and song writer, who was one of the engineering architects of The Beatles’ Abbey Road and Let It Be albums, and Pink Floyd’s Dark Side Of The Moon LP. He later created an unusual formation consisting of him and his composer partner Eric Wolfson as permanent group members, supplemented by a group of session musicians who played on their albums with more or less regularity. Their music was of a more intellectual nature with a more equal focus on lyrics and melody.\nBob Dylan can now be equally considered both as a poet and a musician, and comparing him and The Alan Parson Project to The Beatles makes an interesting text analysis. As a side note: I was also entertaining the idea to include Iron Maiden in the comparison, but the excessive work of Bruce Dickinson \u0026amp; Co would have made the already long download even longer, so I stayed with my original idea. (It takes quite a bit of time to download lyrics with geniusR.)\nFirst, I did the cosine similarity comparison, but this time on the artists and not on the individual albums. According to the cosine matrix Bob Dylan is equally different from The Beatles and The Alan Parsons Project: their cosine is 0.67 and 0.62, respectively. In a two-dimensional space this would be an angle of approximately 50 degrees. The cosine between Beatles and The Alan Parsons Project, however, is only 0.41, which is ‘equivalent’ to a two-dimensional angle of 65 degrees. It looks, that, despite their partially shared history, The Beatles and Alan Parsons have less in common than any of them with Bob Dylan.\nalbums \u0026lt;- tibble( artist = c( rep(\u0026quot;The Beatles\u0026quot;, 13), rep(\u0026quot;Bob Dylan\u0026quot;, 36), rep(\u0026quot;The Alan Parsons Project\u0026quot;, 10) ) , album = c( \u0026quot;Please Please Me\u0026quot;, \u0026quot;With The Beatles\u0026quot;, \u0026quot;A Hard Day s Night\u0026quot;, \u0026quot;Beatles For Sale\u0026quot;, \u0026quot;Help\u0026quot;, \u0026quot;Rubber Soul\u0026quot;, \u0026quot;Revolver\u0026quot;, \u0026quot;Sgt Pepper s Lonely Hearts Club Band\u0026quot;, \u0026quot;Magical Mystery Tour\u0026quot;, \u0026quot;The Beatles The White Album\u0026quot;, \u0026quot;Yellow Submarine\u0026quot;, \u0026quot;Abbey Road\u0026quot;, \u0026quot;Let It Be\u0026quot;, \u0026quot;Bob dylan\u0026quot;, \u0026quot;The freewheelin bob dylan\u0026quot;, \u0026quot;Another side of bob dylan\u0026quot;, \u0026quot;The times they are a changin\u0026quot;, \u0026quot;Bringing it all back home\u0026quot;, \u0026quot;Highway 61 revisited\u0026quot;, \u0026quot;Blonde on blonde\u0026quot;, \u0026quot;John wesley harding\u0026quot;, \u0026quot;Nashville skyline\u0026quot;, \u0026quot;New morning\u0026quot;, \u0026quot;Self portrait\u0026quot;, \u0026quot;Pat garrett billy the kid\u0026quot;, \u0026quot;Triplicate\u0026quot;, \u0026quot;Blood on the tracks\u0026quot;, \u0026quot;The basement tapes\u0026quot;, \u0026quot;Desire\u0026quot;, \u0026quot;Street legal\u0026quot;, \u0026quot;Slow train coming\u0026quot;, \u0026quot;Saved\u0026quot;, \u0026quot;Shot of love\u0026quot;, \u0026quot;Infidels\u0026quot;, \u0026quot;Empire burlesque\u0026quot;, \u0026quot;Knocked out loaded\u0026quot;, \u0026quot;Down in the groove\u0026quot;, \u0026quot;Oh mercy\u0026quot;, \u0026quot;Under the red sky\u0026quot;, \u0026quot;Good as i been to you\u0026quot;, \u0026quot;World gone wrong\u0026quot;, \u0026quot;Time out of mind\u0026quot;, \u0026quot;Love and theft\u0026quot;, \u0026quot;Modern times\u0026quot;, \u0026quot;Together through life\u0026quot;, \u0026quot;Christmas in the heart\u0026quot;, \u0026quot;Tempest\u0026quot;, \u0026quot;Shadows in the night\u0026quot;, \u0026quot;Fallen angels\u0026quot;, \u0026quot;Tales of mystery and imagination edgar allan poe\u0026quot;, \u0026quot;I robot\u0026quot;, \u0026quot;Pyramid\u0026quot;, \u0026quot;Eve\u0026quot;, \u0026quot;The turn of a friendly card\u0026quot;, \u0026quot;Eye in the sky\u0026quot;, \u0026quot;Ammonia avenue\u0026quot;, \u0026quot;Vulture culture\u0026quot;, \u0026quot;Stereotomy\u0026quot;, \u0026quot;Gaudi\u0026quot; ) ) all_lyrics \u0026lt;- album_lyrics \u0026lt;- albums %\u0026gt;% mutate(tracks = map2(artist, album, genius_album)) full_lyrics \u0026lt;- all_lyrics %\u0026gt;% unnest(tracks) %\u0026gt;% arrange(desc(artist))  tidy_lyrics \u0026lt;- full_lyrics %\u0026gt;% unnest_tokens(word, text) %\u0026gt;% filter(nchar(word)\u0026gt;2)%\u0026gt;% anti_join(stop_words) %\u0026gt;% group_by(artist) %\u0026gt;% count(word, sort = TRUE) %\u0026gt;% ungroup() total_full_words \u0026lt;- tidy_lyrics %\u0026gt;% group_by(artist) %\u0026gt;% summarize(total = sum(n)) tidy_lyrics \u0026lt;- left_join(tidy_lyrics, total_full_words) tidy_lyrics \u0026lt;- tidy_lyrics %\u0026gt;% mutate(freq = n / total) full_cos \u0026lt;- tidy_lyrics %\u0026gt;% select(artist, word, freq) full_cos_w \u0026lt;- spread(full_cos, key = artist, value = freq) full_cos_w[is.na(full_cos_w)] \u0026lt;- 0 full_cos_w_matrix \u0026lt;- data.matrix(full_cos_w, rownames.force = NA) full_cos_w_matrix \u0026lt;- full_cos_w_matrix[, -1] cosine_matrix \u0026lt;- cosine(full_cos_w_matrix) cm \u0026lt;- data.frame(cosine_matrix) pander(cm, caption = \u0026quot;Cosine similarity matrix\u0026quot;)  Next, I turned to the text complexity measures. Here I used the core album lists, which had 10 albums for The Alan Parsons Project and 36 (!) for Bob Dylan. Just as in the previous case, stop words are also included in the calculations.\nartists \u0026lt;- full_lyrics %\u0026gt;% distinct(artist) full_text_complexity \u0026lt;- data.table(artist = character(), mGunningFog = numeric(), mARI = numeric()) for(i in 1:nrow(artists)){ artista \u0026lt;- artists[[1]][i] l \u0026lt;- list(artista) temp_lyr \u0026lt;- full_lyrics %\u0026gt;% filter(artist == artista) %\u0026gt;% distinct(text) rwstat \u0026lt;- readability_word_stats(temp_lyr[,1]) l \u0026lt;- list.append(l, 0.4*(rwstat$n.words/nrow(temp_lyr)) + 100*(rwstat$n.complexes/rwstat$n.words)) l \u0026lt;- list.append(l, 5.89*(rwstat$n.chars/rwstat$n.words) - 0.3*(nrow(temp_lyr)/rwstat$n.words) - 15.8) full_text_complexity \u0026lt;- rbind(full_text_complexity, l) } ggplot(data = full_text_complexity, aes(mARI, mGunningFog)) + geom_point(color = \u0026quot;darkblue\u0026quot;) + geom_text(aes(x = mARI, y = mGunningFog, label = artist), hjust=1, vjust=-0.5) + theme_bw() + labs(title = \u0026quot;Text complexity comparison\u0026quot;) + theme(plot.title = element_text(size = rel(1.25))) + xlim(5.5,7.5) + ylim(5,7)  Beatles fares quite poorly compared to both Bob Dylan and The Alan Parsons Project: the group\u0026rsquo;s lyrics is less complex on average than that of the other two musicians. But if we go back to the previous post, we\u0026rsquo;ll see that on an individual album basis the Beatles has nothing to feel bad about. Their late albums (Abbey Road, Let It Be and Sgt. Pepper) meet Bob Dylan or Alan Parsons standards.\nThis does not imply that Paul McCartney should have been given the Nobel Prize as these complexity metrics are only one kind of the many measures of text quality. Nevertheless, it is still interesting to see the numbers and it is a lot of fun to put together an analysis of this kind.\nCodes are at the usual place.\n","date":1539088515,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539088515,"objectID":"4853e2d449c321bf205984d306e21e49","permalink":"https://peterduronelly.github.io/post/comparing-beatles-and-bob-dylan/","publishdate":"2018-10-09T14:35:15+02:00","relpermalink":"/post/comparing-beatles-and-bob-dylan/","section":"post","summary":"In a previous post I showed how to use data science tools to find hidden features in unstructured text and analyzed how the complexity of the lyrics of Beatles songs changed over time. In this post I do a little follow-up and compare complete works of The Beatles with that of two others using the same methodology and metrics. Comparing Beatles with other musicians may help put the original numbers into the perspective.","tags":["R","tidytext","unstructured text data"],"title":"Comparing Beatles and Bob Dylan","type":"post"},{"authors":["Peter Duronelly"],"categories":[],"content":" Stargazer is a neat tool to present model estimates. It accepts a fairly large number of object-types and creates nice-looking, ready-to-publish outputs of their main parameters. In many cases, however, the default settings do not give us the proper numerical results, and customizing the output is not that straightforward. This is part one in a two-part series on how to customize stargazer.\nWhen I first encountered stargazer I already had a problem with the model outputs the package created: in cross-sectional data the observations are often of different sizes, which leads to heteroskedastic model residuals where simple standard errors are useless for measuring variable significance. Heteroskedasticity requires ‘robust’ standard errors to calculate p-values, but there is no flag in stargazer to switch from simple to robust standard errors. The same problem emerges with panel models, where, for basically the same reason, ‘clustered’ standard errors need to be calculated and applied. If the output is based on the wrong errors, then the model cannot be presented with stargazer. However, I haven’t been able to find any other package which can create such a tidily formatted model output.\nThe good news is that stargazer can be fed externally with the right standard errors, which then results in the proper output. This post shows you how.\nlibrary(wbstats) # to retrieve World Bank development data to demonstrate stargazer functionalities library(dplyr) library(data.table) library(plm) library(stargazer) # this is the package what we are interested in library(sandwich) # to calculate robust and clustered stanard errors library(pander)  I am using the wbstats package to download data from the World Bank development database. As countries are of different sizes, running a regression of any kind requires ‘non-standard’ standard errors to calculate variable significance. We will do a cross-section and a panel regression to demonstrate stargazer options.\nIn a simple model I try to explain the number of patent applications in a country with real PPP GDP and population size. The goal of this exercise is not about building the right model, the regressions are for demonstrational purposes only.\ndata \u0026lt;- wb(indicator = c(\u0026quot;NY.GDP.MKTP.PP.KD\u0026quot;, # WB (World Bank) code for PPP GDP per capita in 2011 $ \u0026quot;IP.PAT.RESD\u0026quot;, # WB code for patent applications by residents \u0026quot;SP.POP.TOTL\u0026quot;), # WB code for population startdate = 2000, enddate = 2015) countries \u0026lt;- wbcountries() data \u0026lt;- merge(data, countries[c(\u0026quot;iso2c\u0026quot;, \u0026quot;region\u0026quot;)], by = \u0026quot;iso2c\u0026quot;, all.x = TRUE) data \u0026lt;- subset(subset(data, region != \u0026quot;Aggregates\u0026quot;)) data$indicatorID[data$indicatorID == \u0026quot;NY.GDP.MKTP.PP.KD\u0026quot;] \u0026lt;- \u0026quot;GDP\u0026quot; data$indicatorID[data$indicatorID == \u0026quot;IP.PAT.RESD\u0026quot;] \u0026lt;- \u0026quot;patent_applications\u0026quot; data$indicatorID[data$indicatorID == \u0026quot;SP.POP.TOTL\u0026quot;] \u0026lt;- \u0026quot;population\u0026quot; data \u0026lt;- dcast(data, iso2c + country + date + region ~ indicatorID, value.var = 'value') names(data)[names(data) == \u0026quot;date\u0026quot;] \u0026lt;- \u0026quot;year\u0026quot; data$year \u0026lt;- as.numeric(data$year) data \u0026lt;- data %\u0026gt;% select(-iso2c, -region) data$population \u0026lt;- data$population / 10^6 # some rescaling data$GDP \u0026lt;- data$GDP / 10^9 data \u0026lt;- data[complete.cases(data),]  Without going into the details of the wbstat package we end up with a data frame where country and year are in columns 1 and 2, and the three other variables (GDP in USD bn, patent applications and population in millions) are in columns 3-5. (This is the default format for panels in R.) The panel is only slightly unbalanced: even if a large number of observations is missing the observations with missing values will not make much trouble. (The punbalanced() command measures how unbalanced the panel is, but this is also beyond the scope of this analysis.)\n\nRobust Standard Errors Now let’s do a simple cross sectional regression for 2011.\nlinear_regression \u0026lt;- lm(formula = patent_applications ~ GDP + population, data = data %\u0026gt;% filter(year == 2011))  Now comes the trick! The vcov() function in the sandwich package calculates the robust standard errors that we need for coefficient testing when model residuals are heteroskedastic. Vcov gives us the variable covariance matrix, which measures co-dependencies between the variables. It\u0026rsquo;s diagonal elements are the variances of the variables, which are used for variable importance testing. We need to extract these diagonal elements, take their square root, and feed them into stargazer.\nrobust_standard_errors \u0026lt;- vcov(linear_regression, sandwich) robust_standard_errors \u0026lt;- sqrt(diag(robust_standard_errors))  Our standard erorrs are the following.\ndf = data.frame(robust_standard_errors) pander(df)  This is the input We can feed into stargazer using the ‘se’ option from the method’s parameter set. It is important to note that the object containing the errors should be given as a ‘list’, even if we only show one model with stargazer.\nstargazer(linear_regression, title = \u0026quot;Linear regression\u0026quot;, se = list(robust_standard_errors), type = \u0026quot;html\u0026quot;, out = \u0026quot;linear regression.html\u0026quot;)  The \u0026lsquo;out\u0026rsquo; parameter in stargazer defines the name of the output file we create, and \u0026lsquo;type\u0026rsquo; defines the file\u0026rsquo;s extension. Once we run the command we will get the following (kind of) neatly formatted output which includes our externally fed standard errors and the relevant t-statistics based on those standard errors.\nClustered Standard Errors Now let’s do two panel regressions using the whole dataset: a fixed-effects (FE) model and a first difference (FD) model. We will include both models in our stargazer output for which we need to supply two sets of standard errors. What is robust standard errors in cross sectional regressions is ‘clustered’ standard errors in panels. This time the vcovHC() function will take care of the standard errors.\nFirst the two panels. As I said before, model validity is not an issue here, I just want to have standard errors.\np_data = pdata.frame(data) # we need to create a 'panel data frame' for panel regressions fe_regression \u0026lt;- plm(patent_applications ~ GDP + population, # FE model data = p_data, model = \u0026quot;within\u0026quot;, effect = \u0026quot;twoways\u0026quot;) fd_regression \u0026lt;- plm(diff(patent_applications) ~ diff(GDP, lag = 2) + # FD model diff(diff(GDP), lag = c(0:1)) + diff(population, lag = 2) + diff(diff(population), lag = c(0:1)), data = p_data, model = 'pooling')  From these two models we extract the clustered standard errors the same way, but now with the vcocHC() function which handles clustered errors in panel models.\nclustered_standard_errors_fe \u0026lt;- vcovHC(fe_regression, type = \u0026quot;HC0\u0026quot;, cluster = \u0026quot;group\u0026quot;) clustered_standard_errors_fe \u0026lt;- sqrt(diag(clustered_standard_errors_fe)) clustered_standard_errors_fd \u0026lt;- vcovHC(fd_regression, type = \u0026quot;HC0\u0026quot;, cluster = \u0026quot;group\u0026quot;) clustered_standard_errors_fd \u0026lt;- sqrt(diag(clustered_standard_errors_fd))  Once we have the standard errors ready, we add them as stargazer parameters. Note, that they are given as lists again.\nstargazer(fe_regression, fd_regression, title = \u0026quot;Panel regressions\u0026quot;, se = list(clustered_standard_errors_fe, clustered_standard_errors_fd), type = \u0026quot;html\u0026quot;, out = \u0026quot;panel regression.html\u0026quot;)  Now the stargazer output looks like this.\nWe have everything we need in our model output, and stargazer uses the proper standard error set for both models to coefficient testing. We even have more than we really need, and the labels are kind of messy. There are more options to customize stargazer and make it look better, but it is the subject of the other post on stargazer.\nCodes at the usual place.\n","date":1538482474,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538482474,"objectID":"4eb5115d65375d9990b0deec79fd6238","permalink":"https://peterduronelly.github.io/post/robust-and-clustered-standard-errors-in-stargazer/","publishdate":"2018-10-02T14:14:34+02:00","relpermalink":"/post/robust-and-clustered-standard-errors-in-stargazer/","section":"post","summary":"Stargazer is a neat tool to present model estimates. It accepts a fairly large number of object-types and creates nice-looking, ready-to-publish outputs of their main parameters. In many cases, however, the default settings do not give us the proper numerical results, and customizing the output is not that straightforward. This is part one in a two-part series on how to customize stargazer.\nWhen I first encountered stargazer I already had a problem with the model outputs the package created: in cross-sectional data the observations are often of different sizes, which leads to heteroskedastic model residuals where simple standard errors are useless for measuring variable significance.","tags":["R","stargazer","modeling"],"title":"Robust and Clustered Standard Errors in Stargazer","type":"post"},{"authors":["Peter Duronelly"],"categories":[],"content":"Bootstrap sampling is a widely used method in machine learning and in statistics. The main idea is that we try to decrease overfitting and the chance of myopic tree-building if run our algorithm multiple times using the same data, but always taking a different sample with repetitions from our original data. (For instance, random forest builds the trees using repeated bootstrap samples.) On a machine learning class one of my class mates asked what percentage of the original data shows up in the bootstrapped sample. Let’s have a look!\nimport random import matplotlib.pyplot as plt %matplotlib inline import pandas as pd  In bootstrap samples we select elements from our original sample in the same number as the size of the original data at hand using repetitions. This is a poster case for Central Limit Theorem, thus the expected value of our sample mean will be bang in line with the mean of the original data.\nLet’s have a dataset of 1,000 observations, from which we take 10,000 samples of 1,000 elements. This number of sampling helps us visualize and calculate our sample properties.\nWhen bootstrapping, some original observations show up multiple times and some others will be missing from the bootstrapped samples. We are about finding the number of distinct elements in the bootstrapped samples. If the samples are lists, then turning these lists into sets helps us calculate the number of distinct elements, since sets don’t have the same value more than once.\nsample_sizes = [] for i in range(10000): sample = [] for j in range(1000): sample.append(round(random.randint(1,1000))) sample_sizes.append(len(set(sample)))  Turning the sample size list into a data frame we can easily get the most basic statistics.\ndf = pd.DataFrame(sample_sizes) df.columns = [\u0026quot;sample sizes\u0026quot;] df.describe().style.format('{:.2f}')  We have taken 10,000 samples of 1,000 observations with repetitions from the 1,000 observations. On average, 632 observations show up in the bootstrapped samples, and this number is between 625 and 639 in the half of the samples. These results are extremely stable: if you run this simulation multiple times the average will always be 632.\nIt shouldn’t come as a surprise that the distribution of the number of distinct elements is almost perfectly normal.\nfig = plt.figure(figsize = (10,6)) plt.hist(df[\u0026quot;sample sizes\u0026quot;], bins = list(range(590, 680, 2)), rwidth=0.9, color = 'k') plt.title(\u0026quot;Bootstrap sample sizes\\noriginal sample size = 1000\u0026quot;) plt.xlabel(\u0026quot;number of distinct elements in the sample\u0026quot;) plt.ylabel(\u0026quot;frequency\u0026quot;) plt.show()  Codes are at the usual place.\n","date":1537727312,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537727312,"objectID":"6ff01b59c689ff57373148221c52f13b","permalink":"https://peterduronelly.github.io/post/bootstrap-samples/","publishdate":"2018-09-23T20:28:32+02:00","relpermalink":"/post/bootstrap-samples/","section":"post","summary":"Bootstrap sampling is a widely used method in machine learning and in statistics. The main idea is that we try to decrease overfitting and the chance of myopic tree-building if run our algorithm multiple times using the same data, but always taking a different sample with repetitions from our original data. (For instance, random forest builds the trees using repeated bootstrap samples.) On a machine learning class one of my class mates asked what percentage of the original data shows up in the bootstrapped sample.","tags":["sampling"],"title":"Bootstrap Samples","type":"post"},{"authors":["Peter Duronelly"],"categories":[],"content":"Clustering is one of the well-known unsupervised learning tools. In the standard case you have an observation matrix where observations are in rows and variables which describe them are in columns. But data can also be structured in a different way, just like the distance matrix on a map. In this case observations are by both rows and columns and each element in the observation matrix is a measure of distance, or dissimilarity, between any two observations. This structure can also serve as a basis for clustering, just as you can cluster cities based on the respective distances between any two of them.\nWhen I first encountered this problem I did not find a solution in the standard scikit-learn library which you automatically call when doing a clustering exercise. SciPy, fortunately, has a solution. Let\u0026rsquo;s see how it works!\nimport pandas as pd import numpy as np import matplotlib.pyplot as plt import math from scipy.cluster.hierarchy import linkage, dendrogram, fcluster  The exercise is done on the hundred largest US cities. The list is coming from https://gist.github.com/Miserlou/c5cd8364bf9b2420bb29#file-cities-json in a very neat format. (Thanks to Rick Jones for the data!) It includes not only the city, the state and the population but the latitude/longitude coordinates of the cities wich can be used to calculate an approximate distance measure between the cities.\nfilepath = ... cities = pd.read_json(filepath) cities.head()  By selecting the top 100 cities we get sufficient number of observations and we are still able to interpret the result through simple visualization.\ntop100 = cities[0:100] fig = plt.figure() ax = fig.add_axes([0,0,1,1]) ax = plt.scatter(top100[\u0026quot;longitude\u0026quot;], top100[\u0026quot;latitude\u0026quot;]) plt.title(\u0026quot;The Locations of the 100 Largest US Cities\u0026quot;) plt.show()  A more fancy map could be generated using the folium package but this simple scatter plot is sufficient for our purposes. As all real, non-simulated data, our city dataset does not show a very obvious pattern. Cities are scattered seemingly randomly on the map, and we have two outliers, Anchorage on the North-West and Honolulu on the South-West, which are very far from all other points.\nNow let\u0026rsquo;s calculate the distances between the cities using the geopy package. The distances can be interpreted as dissimilarities: the larger the distance between two observations the less similar they are. This is may literally not be the case with cities (two distant cities can easily be alike), but when we think in a more abstract space the concept makes sense. Also, city clusters are interpreted in terms of geographical distances, so using them in this exercise helps us understand the method and its results.\ndistance_matrix = np.empty(shape = [100,100]) for i in range(100): for j in range(100): coords_i = (top100.iloc[i][\u0026quot;latitude\u0026quot;], top100.iloc[i][\u0026quot;longitude\u0026quot;]) coords_j = (top100.iloc[j][\u0026quot;latitude\u0026quot;], top100.iloc[j][\u0026quot;longitude\u0026quot;]) distance_matrix[i,j] = geopy.distance.vincenty(coords_i, coords_j).km  Before we move on to build the clusters and visualize the results let\u0026rsquo;s create a list of city names where the name of each city is followed by the state. This helps us interpret the cluster tree, the so-called \u0026lsquo;dendrogram\u0026rsquo;.\ncities[\u0026quot;city, state\u0026quot;] = top100[\u0026quot;city\u0026quot;].map(str) + \u0026quot;, \u0026quot; + top100[\u0026quot;state\u0026quot;] citynames = cities[\u0026quot;city, state\u0026quot;][0:100].tolist() citynames[0:10]  [\u0026lsquo;New York, New York\u0026rsquo;,\n\u0026nbsp;\u0026lsquo;Los Angeles, California\u0026rsquo;, \u0026nbsp; \u0026lsquo;Chicago, Illinois\u0026rsquo;, \u0026nbsp;\u0026lsquo;Houston, Texas\u0026rsquo;, \u0026nbsp;\u0026lsquo;Philadelphia, Pennsylvania\u0026rsquo;, \u0026nbsp;\u0026lsquo;Phoenix, Arizona\u0026rsquo;,\n\u0026nbsp;\u0026lsquo;San Antonio, Texas\u0026rsquo;,\n\u0026nbsp;\u0026lsquo;San Diego, California\u0026rsquo;,\n\u0026nbsp;\u0026lsquo;Dallas, Texas\u0026rsquo;,\n\u0026nbsp;\u0026lsquo;San Jose, California\u0026rsquo;]\nNo we get to the most important part! The meat of the module is the linkage function (scipy.cluster.hierarchy.linkage(…)), which runs the actual clustering algorithms. It can handle both an n x n distance matrix and a regular n x m observation matrix as input. If we form the clusters on a distance matrix, we need its condensed format: the upper diagonal elements (excluding the diagonals) have to be converted into a vector (a list) with a length of n(n-1)/2. The output is a (n-1)x4 NumPy array Z, called linkage matrix, which contains the hierarchical clustering algorithm’s encoded results.\nEach row in Z represents one iteration. In each row i, clusters with indices of Z[i,0] and Z[i,1] are combined in a new cluster, based on the distance between them which is in Z[i,2]. The number of elements in the newly formed cluster is Z[i,3].\nThis is a hierarchical clustering method: we start with elementary clusters (the observations) which are merged into larger and larger clusters. At the end all observations form a single cluster.\nClusters contain observations which are close to each other. Closeness can be defined between observations (cluster elements) and the clusters themselves. To calculate distances between two clusters we need to define two parameters:\n We need to define a distance metric, which measures the distances between the elements of one cluster and the elements of the other cluster. The default is the Euclidean distance which is sufficient for our clustering project.\n We also need to define a method to sum up the distances between the individual elements of two clusters to come up with a single value for the distance between them. Just as in the case of the measures of central tendency, we are looking for the typical distance between the elements of any two clusters. The method sets how this typical distance is defined.\n  While the distance metric can also make a difference, the most important parameter in cluster formation is the method. There are various options (methods) to define how far two clusters are from each other: the distance between the two closest elements, or the one between two most distant ones, or some sort of average of the distances between elements of cluster a and cluster b, etc. If cluster a and cluster b have u and v number of elements, respectively, we will have u*v distances which we can use to find the distance between clusters a and b. This is the modeler’s choice and, as I will show it later, it can create markedly different cluster structures. The methods are very well explained in the SciPy documentation and I urge you to check it out. At this point, however, we only need to keep in mind that the method is the most important parameter we need to set.\nThe most evident difference between clustering methods is how they handle outliers (Anchorage and Honolulu). We will see that merging the mainland and the coastal areas will be completely different in some cases.\nAnd now comes the trick! SciPy does not use the whole dissimilarity matrix for the calculations, only a list of the upper diagonal elements. The length of the list is n(n-1)/2 and SciPy automatically calculates the number of observations it needs to cluster.\nl = [] # upper triangular elements of the distance matrix for i in range(0, distance_matrix.shape[0]): for j in range(i + 1, distance_matrix.shape[0]): l.append(distance_matrix[i,j]) Z = linkage(l, method=\u0026quot;ward\u0026quot;) # this is the meat of the thing!  Once we have the linkage matrix, which is basically a log of which observation or cluster got merged in each step, we can visualize the results.\nfig = plt.figure(figsize=(25, 10)) plt.title(\u0026quot;Hierarchical Clustering Dendrogram\\nLinkage = ward\u0026quot;) plt.xlabel('city, state') plt.ylabel('distance') dn = dendrogram(Z, labels = citynames, leaf_rotation=90., leaf_font_size=10., show_contracted=True) plt.show()  First we used the \u0026lsquo;ward\u0026rsquo; method which is an optimization algorithm: it aims to minimize within-cluster variance. Cities belong to two major clusters: the ones with green linkages can be called \u0026lsquo;The West\u0026rsquo;, while the ones with red linkages can be considered \u0026lsquo;The East\u0026rsquo; or \u0026lsquo;East + Midwest\u0026rsquo;. Our outliers (Anchorage and Honolulu) belong to the West, which makes sense, and they form a cluster together before being linked to the other Western cities. Before they are linked, however, mainland Western/West Coast cities are merged into clusters at higher and higher levels of aggregation. Only then come the two overseas cities.\nCities in Arizona are merged into their own clusters, then Nevada and California. Boise City, Idaho goes to Nevada/California after being merged with the cluster formed by Seattle and Portland. Everything else belong to the East Coast, including cities in Minnesota, Wisconsin and Texas. The dendrogram is sort of ‘smooth’ or ‘balanced’, thanks to the algorithm\u0026rsquo;s variance minimalization.\nThe vertical axis shows the distance, or the level of dissimilarities between the clusters. The longer the vertical lines the more dissimilar the two clusters which are merged in that step. The lenght of the two vertical blue lines shows that this method considers the West Coast, which includes the outlier cities, to be very different from everything else.\nThe structure changes a lot when cluster distances are defined as the average distance between the members of the two clusters. The fact that cluster formation is not variance-optimized results in an unbalanced-looking tree.\nZ = linkage(l, method=\u0026quot;average\u0026quot;) fig = plt.figure(figsize=(25, 10)) plt.title(\u0026quot;Hierarchical Clustering Dendrogram\\nLinkage = average\u0026quot;) plt.xlabel('city, state') plt.ylabel('distance') dn = dendrogram(Z, labels = citynames, leaf_rotation=90., leaf_font_size=10., show_contracted=True) plt.show()  The most visible difference, as I mentioned before, is how the outliers are handled. Here they are merged with other mainland cities only at the end, only after the other 98 cities have been joined together. There is more emphasis on their uniqueness than before, when within-cluster variance ruled cluster formation. East Coast and West Coast are made up from mostly the same cities as before, but their aggregation, from individual cities to larger and larger formations has a somewhat different profile.\nIt is also interesting to see how the trio of Boise City and Seattle \u0026amp; Portland is linked to Arizona and California. In the ‘ward’ optimization mechanism they are merged with California before they together are put together with Arizona. In this latter case Arizona, California and Nevada are merged before the trio of Boise, Seattle \u0026amp; Portland joins them as the last West Coast areas.\nNo one is better than the other, and both the nature of the data and the target of the modeling exercise are important in selecting the best method. In the case of the US cities, for example, if we think that the mainland cities share more with each other than with Anchorage or Honolulu, ‘average’ is a better choice. If we simply want a split into East and West, then ‘ward’ will be our method.\nAs a summary: clustering is possible in Python when the data does not come as an n x p matrix of n observations and p variables, but as an n x n dissimilarity or distance matrix. The home of the algorithm is the SciPy package, and depending on the method, we can have very different results.\nThe codes are at the usual place.\n","date":1536914016,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536914016,"objectID":"1431b7231e6818c939f031089e0fdce6","permalink":"https://peterduronelly.github.io/post/clustering-on-dissimilarity-matrix/","publishdate":"2018-09-14T10:33:36+02:00","relpermalink":"/post/clustering-on-dissimilarity-matrix/","section":"post","summary":"Clustering is one of the well-known unsupervised learning tools. In the standard case you have an observation matrix where observations are in rows and variables which describe them are in columns. But data can also be structured in a different way, just like the distance matrix on a map. In this case observations are by both rows and columns and each element in the observation matrix is a measure of distance, or dissimilarity, between any two observations.","tags":["Python","SciPy","clustering"],"title":"Clustering on a Dissimilarity Matrix","type":"post"},{"authors":["Peter Duronelly"],"categories":[],"content":" The Beatles became a hit through its sometimes simple but always powerful music but it has never been famous for its poetry. The group\u0026rsquo;s lyrics, however, did change during the band\u0026rsquo;s short existence and we can use text analysis to track these changes. This post is about measuring the change in the complexity of the group\u0026rsquo;s lyrics, from the Please, Please Me to the Abbey Road albums, showing how we can use basic data secience tools to find really fancy patterns in unstructured text data.\nThis piece is a shortened version of a final project for a Data Science on Unstructured Text Data course held by Facebook\u0026rsquo;s Eduardo Ariño de la Rubia. The course introduced the tidytext package and the basics of text analysis in R. At the end of the course students had to present their skills through a freely chosen analysis project. Although tidytext does not directly cover text complexity, to me it was somehow an obvious choice.\nThis is a technical post, but most of the hard stuff is concentrated in the code blocks. If you are only interested in the power of data science, feel free to disregard these blocks and concentrate on the text and the plots only. You will still be able to get the message.\nWhen you learn English as a foreign language you inevitably bump into The Beatles early on. The songs are well known, and even a beginner student can easily understand the lyrics. This is not only because the members were singing in nice English, but because their early text is damned simple. \u0026lsquo;She loves you, yeah, yeah, yeah.\u0026rsquo; Not that of a challenging text, right?\nBut when you listen to The Beatles a little more, you realize that as time went by their songs got more and more sophisticated. \u0026lsquo;Strawberry Fields Forever\u0026rsquo; does have more depth than \u0026lsquo;A Hard Day\u0026rsquo;s Night\u0026rsquo;. Since we are into data science, it is obvious to ask: can we measure this change in sophistication? Can we trace the development also in their lyrics? As the members went from their early twenties towards their thirties, did they move from their simple but powerful origins towards something more mature?\nIn the next few lines I am analyzing The Beatles\u0026rsquo; thirteen albums of \u0026lsquo;core catalogues\u0026rsquo; from \u0026lsquo;Please Please Me\u0026rsquo; to \u0026lsquo;Let It Be\u0026rsquo;, published between 1964 and 1970. It is amazing but the most influential pop group of all times existed for less than a decade, and this short period was enough to issue thirteen albums and to turn the world upside down. The group had quite a few extra collections, live recordings and greatest hit compilations (the last one, according to wikipedia, in 2013), but these thirteen albums make up the the actual works of the group.\nFor the project I used the newly developed geniusR package by Josiah Parry, which downloads lyrics and metadata from the genius.com homepage. This package was of enormous help for the analysis. A Glance At The Beatles As a starter I imported the necessary packages. I like starting all analysis with the packages, having them in one single chunk for a better overview. Also, when I later need to add further packages I just scroll back to the first chunk to enter the extra library command.\nlibrary(geniusR) library(tidyverse) library(tidytext) library(tidyr) library(tibble) library(dplyr) library(purrr) library(stringr) library(syllable) library(ggplot2) library(scales) library(gridExtra) library(lsa) library(rlist) library(data.table)  Downloading the text is simple with geniusR: you define the artist, the number of albums to download, and the album titles. Album titles should be entered as the last part of the urls on the genius.com webpage without hyphens. Apostrophes are omitted. You can also download albums of multiple artists entering the author, # of albums multiple times as a vector. See the documentation and Josiah\u0026rsquo;s github for details.\nIt takes a while until your text downloads, but you end up with a nice (tidy!) tibble which serves as the basis for further analysis.\nalbums \u0026lt;- tibble( artist = rep(\u0026quot;The Beatles\u0026quot;, 13), album = c( \u0026quot;Please Please Me\u0026quot;, \u0026quot;With The Beatles\u0026quot;, \u0026quot;A Hard Day s Night\u0026quot;, \u0026quot;Beatles For Sale\u0026quot;, \u0026quot;Help\u0026quot;, \u0026quot;Rubber Soul\u0026quot;, \u0026quot;Revolver\u0026quot;, \u0026quot;Sgt Pepper s Lonely Hearts Club Band\u0026quot;, \u0026quot;Magical Mystery Tour\u0026quot;, \u0026quot;The Beatles The White Album\u0026quot;, \u0026quot;Yellow Submarine\u0026quot;, \u0026quot;Abbey Road\u0026quot;, \u0026quot;Let It Be\u0026quot; ) ) album_lyrics \u0026lt;- albums %\u0026gt;% mutate(tracks = map2(artist, album, genius_album)) beatles_lyrics \u0026lt;- album_lyrics %\u0026gt;% unnest(tracks) beatles_albums \u0026lt;- beatles_lyrics %\u0026gt;% distinct(album)  As the most obvious starting point of any text analysis, I checked the per album frequencies of non-stop words across these albums. (For the wider audience: stop words are the most common, \u0026lsquo;functional\u0026rsquo; words in a language.) In order to draw an arch of change, I plotted simple word frequency charts for Please Please Me (1963), Help (1965), Magical Mystery Tour (1967) and Let It Be (1970). Can we see any difference in the words used?\ntidy_beatles \u0026lt;- beatles_lyrics %\u0026gt;% unnest_tokens(word, text) %\u0026gt;% filter(nchar(word)\u0026gt;2)%\u0026gt;% anti_join(stop_words) %\u0026gt;% group_by(album) %\u0026gt;% count(word, sort = TRUE) %\u0026gt;% ungroup() total_words \u0026lt;- tidy_beatles %\u0026gt;% group_by(album) %\u0026gt;% summarize(total = sum(n)) tidy_beatles \u0026lt;- left_join(tidy_beatles, total_words) tidy_beatles \u0026lt;- tidy_beatles %\u0026gt;% mutate(freq = n / total) ppm \u0026lt;- tidy_beatles %\u0026gt;% filter(str_detect(album, \u0026quot;Please\u0026quot;))%\u0026gt;% arrange(desc(freq)) %\u0026gt;% top_n(10)%\u0026gt;% mutate(word = factor(word, levels = rev(unique(word)))) %\u0026gt;% ggplot(aes(word, freq, fill=I(\u0026quot;steelblue3\u0026quot;), col=I(\u0026quot;black\u0026quot;))) + geom_col(show.legend = FALSE) + labs(x = NULL, y = \u0026quot;frequency\u0026quot;) + coord_flip() + theme_bw() + labs(title = \u0026quot;Word frequency in Please Please Me\u0026quot;) + theme(plot.title = element_text(size = rel(1))) + scale_y_continuous(labels = percent) help \u0026lt;- tidy_beatles %\u0026gt;% filter(str_detect(album, \u0026quot;Help\u0026quot;))%\u0026gt;% arrange(desc(freq)) %\u0026gt;% top_n(10)%\u0026gt;% mutate(word = factor(word, levels = rev(unique(word)))) %\u0026gt;% ggplot(aes(word, freq, fill=I(\u0026quot;steelblue3\u0026quot;), col=I(\u0026quot;black\u0026quot;))) + geom_col(show.legend = FALSE) + labs(x = NULL, y = \u0026quot;frequency\u0026quot;) + coord_flip() + theme_bw() + labs(title = \u0026quot;Word frequency in Help\u0026quot;) + theme(plot.title = element_text(size = rel(1))) + scale_y_continuous(labels = percent) mys \u0026lt;- tidy_beatles %\u0026gt;% filter(str_detect(album, \u0026quot;Mystery\u0026quot;))%\u0026gt;% arrange(desc(freq)) %\u0026gt;% top_n(10)%\u0026gt;% mutate(word = factor(word, levels = rev(unique(word)))) %\u0026gt;% ggplot(aes(word, freq, fill=I(\u0026quot;steelblue3\u0026quot;), col=I(\u0026quot;black\u0026quot;))) + geom_col(show.legend = FALSE) + labs(x = NULL, y = \u0026quot;frequency\u0026quot;) + coord_flip() + theme_bw() + labs(title = \u0026quot;Word frequency in Magical Myster Tour\u0026quot;) + theme(plot.title = element_text(size = rel(1))) + scale_y_continuous(labels = percent) lib \u0026lt;- tidy_beatles %\u0026gt;% filter(str_detect(album, \u0026quot;Let\u0026quot;))%\u0026gt;% arrange(desc(freq)) %\u0026gt;% top_n(10)%\u0026gt;% mutate(word = factor(word, levels = rev(unique(word)))) %\u0026gt;% ggplot(aes(word, freq, fill=I(\u0026quot;steelblue3\u0026quot;), col=I(\u0026quot;black\u0026quot;))) + geom_col(show.legend = FALSE) + labs(x = NULL, y = \u0026quot;frequency\u0026quot;) + coord_flip() + theme_bw() + labs(title = \u0026quot;Word frequency in Let It Be\u0026quot;) + theme(plot.title = element_text(size = rel(1))) + scale_y_continuous(labels = percent) grid.arrange(ppm, help, mys, lib, nrow = 2)  Love makes it into the first 10 in three of the albums, leading the pack in Please Please Me and, to my little surprise, Magical Mystery Tour. Interestingly, it is missing from the top 10 in Let It Be, the last album. It looks, love was not of primary interest by 1970, the year when the members decided to go their own separate ways. Of course, per album word frequency depends largely on the songs\u0026rsquo; topic selection: \u0026lsquo;mother\u0026rsquo; goes to number 2 in Magical Mystery Tour due to the many repetitions of the line \u0026lsquo;Your mother should know\u0026rsquo; in the song of the same title.\nThis simple exercise shows that working with lyrics can be very tricky. Lines are repeated very often, and melody dominates sentence building. As a matter of fact, sentences can only be poorly defined by regular text analysis algorithms in songs, which, as we will see later, makes measuring text complexity somewhat difficult. Measuring Similarity Across Core Albums In order to asses how much the group changed over the course of these seven years I measured the similarity of each album to Please Please Me, the very first LP. More and more sophisticated lyrics would result in larger and larger differences in text, measured by cosine similarity.\nI calculated cosine similarity based on word frequency vectors, where each album is vector of frequencies of words in a union of sets of words from each album. The word list is a product of a full join of all words from the all the albums, and the cosine for each album is a similarity measure between that particular album and the benchmark Please Please Me. This word list excludes stop words, of course.\ncos \u0026lt;- tidy_beatles %\u0026gt;% select(album, word, freq) cos_w \u0026lt;- spread(cos, key = album, value = freq) # This is a matrix where entries are frequencies of words in the the various albums. NAs are replaced by zeros in the next command. cos_w[is.na(cos_w)] \u0026lt;- 0 cos_w \u0026lt;- cos_w %\u0026gt;% select(-word) title \u0026lt;- beatles_albums[[1]][1] a \u0026lt;- cos_w %\u0026gt;% select(as.character(title)) cosines \u0026lt;- data.table(album = character(), cosines = numeric()) for(i in 2:nrow(beatles_albums)){ title1 \u0026lt;- beatles_albums[[1]][i] l \u0026lt;- list(title1) b \u0026lt;- cos_w %\u0026gt;% select(as.character(title1)) l \u0026lt;- list.append(l, round(sum(a*b)/sqrt(sum(a^2)*sum(b^2)),3)) cosines \u0026lt;- rbind(cosines, l) } cosines \u0026lt;- data.frame(cosines) cosines \u0026lt;- cosines%\u0026gt;% arrange(desc(cosines)) cosines$album \u0026lt;- factor(cosines$album, levels = cosines$album[order(cosines$cosines)]) ggplot(cosines) + geom_col(aes(album, cosines, fill=I(\u0026quot;steelblue3\u0026quot;), col=I(\u0026quot;black\u0026quot;)),show.legend = F) + theme_bw() + coord_flip() + labs(title = \u0026quot;Cosine similarities with Please Please Me\u0026quot;, y = \u0026quot;cosine similarity\u0026quot;) + theme(plot.title = element_text(size = rel(1.25))) + ylim(0,1)  It is not surprising that \u0026lsquo;A Hard Day\u0026rsquo;s Night\u0026rsquo; is very similar to Please Please Me, but Abbey Road (no. 12) also shares a lot with it. Sgt. Pepper\u0026rsquo;s (no. 8) is an interesting album: it is the most distinct one amongst the core and, as we will see it later, by some measure it has more complex lyrics than any of the other LPs. (St. Pepper\u0026rsquo;s was the first album of the \u0026lsquo;studio years\u0026rsquo;, when the band was finally freed from the burden of permanent touring. It was an experimental album which took 700 hours to record.) Text Complexity Next I turned to text complexity. Can we see an arch of change as the group gets older, grows confidence, and starts to have something else to say than \u0026lsquo;I love you\u0026rsquo;?\nThere are various measures of text complexity, and all aims to assess the readability of prosaic test. Since these metrics have been developed to prose, applying them to lyrics, which is basically poetry, is not straightforward. In songs and poems text serves the melody (rhythm), and, for the lack of proper punctuation, standard algorithms cannot detect where sentences start and end. The basis of complexity metrics is usually the number of words in a sentence, the lengths of the words, and the ratio of complex words within all words. These measures are focusing on how the text is built, and they don\u0026rsquo;t filter for stop words, as the use of these stop words is also a sign of sophistication. These are, obviously, the simplest measures and they do not account for other dimensions of complexity, like the frequency with which people use these words or the types of texts where these words are typical. By these metrics, for instance, the word \u0026lsquo;championship\u0026rsquo; is way more elaborate than the word \u0026lsquo;cat\u0026rsquo;, although the context where the former is mostly used may not be more academic than the one for the latter.\nIn this exercise I use the modified version of two complexity measures. The \u0026lsquo;Automated Readability Index\u0026rsquo; uses characters, words and sentences so, that\nARI = 4.71 x (characters/words) + 0.5 x (words/sentences) - 21.43  The \u0026lsquo;Gunning Fog\u0026rsquo; score is based on words, sentences, and the ratio of complex words: Gunning Fog = 0.4 x ( (words/sentences) + 100 x (complex words/words) )  A word is considered to be complex if it has at least 3 syllables. Higher complexity scores indicate more complicated text.\nSince sentences are loosely defined in lyrics, I replaced them by lines, despite that sentences can be of multiple lines. Lines are the main building blocks of song text, so using them as a proxy is a viable option. As the original formulas are this way \u0026lsquo;modified\u0026rsquo;, I denote these measures as mARI and mGunningFog. In this form they are probably more imperfect as they originally are, but they do show meaningful patterns even in our case.\nIn order to calculate these measures for each album I looped through the album titles and selected distinct lines for analysis. The reason for using distinct lines is that in songs lines are many times repeated for the sake of the melody, serving as chorus, and complete verses can be reused to fill the melody with sufficient amount of text for the vocals.\nFor the complexity metrics I used the \u0026lsquo;syllable\u0026rsquo; package, which produces certain text statistics from which these scores can be calculated. The result is in the next chart.\ntext_complexity \u0026lt;- data.table(album = character(), mGunningFog = numeric(), mARI = numeric()) for(i in 1:nrow(beatles_albums)){ name \u0026lt;- beatles_albums[[1]][i] l \u0026lt;- list(name) temp_lyr \u0026lt;- beatles_lyrics %\u0026gt;% filter(album == as.character(name)) %\u0026gt;% distinct(text) rwstat \u0026lt;- readability_word_stats(temp_lyr[,1]) l \u0026lt;- list.append(l, 0.4*(rwstat$n.words/nrow(temp_lyr)) + 100*(rwstat$n.complexes/rwstat$n.words)) l \u0026lt;- list.append(l, 4.71*(rwstat$n.chars/rwstat$n.words) + 0.5*(rwstat$n.words/nrow(temp_lyr)) - 21.43) text_complexity \u0026lt;- rbind(text_complexity, l) } ggplot(data = text_complexity, aes(mARI, mGunningFog)) + geom_point(color = \u0026quot;darkblue\u0026quot;) + geom_text(aes(x = mARI, y = mGunningFog, label = album), hjust=1, vjust=-0.5) + theme_bw() + labs(title = \u0026quot;Text complexity of Beatles albums\u0026quot;) + theme(plot.title = element_text(size = rel(1.25))) + ylim(4,8) + xlim(-3,1)  While the two metrics put the albums in slightly different orders, the trend is obvious. The first albums (Please Please Me, A Hard Day\u0026rsquo;s Night, With The Beatles) are of fairly simple text, but as time goes by sophistication increases. Let It Be (no. 13) and Abbey Road (no. 12) have fairly high readings by ARI, and Let It Be is the single most complex according to Gunning Fox. Note that Sgt. Pepper (no. 8) is of relatively high complexity: it is the most complex by ARI and the third most complex by Gunning Fog. Remember, Sgt. Pepper is the most dissimilar album compared to the Please Please Me benchmark by cosine similarity.\nIf we look at the publication dates of these albums (not shown here) we do see a development in the group\u0026rsquo;s artistic performance. Four out of the last five albums (\u0026lsquo;Magical Mystery Tour\u0026rsquo;, \u0026lsquo;The Beatles The White Album\u0026rsquo;, \u0026lsquo;Abbey Road\u0026rsquo;, \u0026lsquo;Let It Be\u0026rsquo;) are in the top right corner of the chart, while the first five LPs are in the bottom left. Summary This short analysis tries to provide an insight into the carrier of The Beatles through their lyrics. Songs were very simple at the beginning, but as their music got more mature, their lyrics followed on. Measures of text complexity, just as those of similarity, can also reveal the uniqueness of the St. Pepper\u0026rsquo;s album.\nAmong the tools, tidytext, syllable, and of course the geniusR packages were used to compile this report.\nCodes can be found at https://github.com/peterduronelly/blogcodes/blob/master/02-Text-Complexity-Analysis-Of-Beatles_Lyrics-With-R.R.\n","date":1536838748,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536838748,"objectID":"824c2858be38fc58c3ac2464db1b8d34","permalink":"https://peterduronelly.github.io/post/text-complexity-analysis-of-beatles-lyrics-with-r/","publishdate":"2018-09-13T13:39:08+02:00","relpermalink":"/post/text-complexity-analysis-of-beatles-lyrics-with-r/","section":"post","summary":"The Beatles became a hit through its sometimes simple but always powerful music but it has never been famous for its poetry. The group\u0026rsquo;s lyrics, however, did change during the band\u0026rsquo;s short existence and we can use text analysis to track these changes. This post is about measuring the change in the complexity of the group\u0026rsquo;s lyrics, from the Please, Please Me to the Abbey Road albums, showing how we can use basic data secience tools to find really fancy patterns in unstructured text data.","tags":["R","tidytext","unstructured text data"],"title":"Text Complexity Analysis of Beatles Lyrics With R","type":"post"},{"authors":["Peter Duronelly"],"categories":[],"content":"One of the things we need to manage in data analysis is recources. When we have large amounts of (\u0026lsquo;big\u0026rsquo;) data this can become a serious issue. One of the cases when we need to consider whether we really need all the data we have is when we have a lot of zeros in our database, and these zeroes happen to be irrelevant for our calculations. Python\u0026rsquo;s SciPy library has a solution to store and handle sparse data matrices which contain a large number of irrelevant zero values.\nIn order to demonstrate how this works let\u0026rsquo;s first import the necessary packages.\nimport numpy as np from scipy.sparse import coo_matrix from scipy.sparse import save_npz import matplotlib import matplotlib.pyplot as plt %matplotlib inline import random  Where would we come across sparse data? Let\u0026rsquo;s imagine you are running an e-commerce site where people buy various products. Some poeple buy only one item, others buy multiple products. You want to see how clients are related, or linked, to each other based on their purchase patterns: if some customers buy the same or almost the same set of products, these clients are similar.\nMoreover, these linkages create a network clients, and this network can be used for recommendations, coupons, etc. Without going into the details how such a network can be built and used let\u0026rsquo;s see how we get to sparse data when we analyze buying patterns.\nFor the sake of simplicity let\u0026rsquo;s assume that there are 15 clients and 30 products. Each client buys between 2 and 8 pieces of items.\nclients = list(range(0,15)) # 15 clients dic = {} # a dictionary of purchases: keys are clients and values are lists of items bought for i in range(0,15): key = clients[i] random.seed(i) l = random.sample(range(0,30), random.randint(2,8)) dic[key] = l  We can print out what itemms clients have purchased.\nfor key in dic.keys(): print(dic[key])  [12, 24, 13, 1, 8, 16, 15, 29]\n[18, 27, 25]\n[27, 1, 2, 29, 11, 5, 23, 21]\n[18, 17, 4]\n[9, 3, 23]\n[8, 23, 11, 25, 22, 28]\n[18, 26, 2, 15, 24, 8, 1, 0]\n[4, 12, 20, 1]\n[11, 12, 4]\n[19, 11, 8, 4, 5]\n[1, 13, 15, 18, 0, 6]\n[27, 17, 29, 24, 14]\n[8, 21, 16, 11, 4]\n[9, 21, 29, 25]\n[19, 22]\nClients No. 0, 2 and 6 have bought eight items, and only one has bought two. Now we can build a matrix of clients where the matrix elements show the number of common items on their shopping history. Each aij element of this matrix tells the number of items bought both by client i and client j . The ith diagonal element shows the number of items bought buy client i. This is going to be a symmetric matrix, of course.\nTo build the matrix we need to define the row and column indices of the non-zero matrix elements, and the values of the elements. These are going to be lists of equal sizes, which serve as inputs for the sparse matrix. Since the matrix is symmetric, we don\u0026rsquo;t need to calculate and store the lower diagonal elements to save space.\nr = 0 # the index of the row of the matrix c = 0 # the index of the columns of the matrix counter = 0 row_indices = [] # row indices of the non-zero values column_indices = [] # column indices of the non-zero values matrix_elements = [] # the non-zero values themselves for key_r in dic.keys(): # key_r is the key for the rows x = dic[key_r] for key_c in dic.keys(): # key_c is the key for the columns if c \u0026gt;= r: y = dic[key_c] common_set = list(set(x) \u0026amp; set(y)) common_set_size = len(common_set) if common_set_size \u0026gt; 0: row_indices.append(r) column_indices.append(c) matrix_elements.append(common_set_size) c = c + 1 r = r + 1 c = 0  Once we have the lists we can build the sparse matrix using the coo_matrix function. This function takes numpy arrays as inputs so we need to convert our lists to arrays. When building the matrix we need to add the non-zero elements first, then the indices as a tuple. Finally we need to define the size (shape) of the matrix. The resulting object will be of type scipy.sparse.coo.coo_matrix.\nrow = np.array(row_indices) col = np.array(column_indices) val = np.array(matrix_elements) mx = coo_matrix((val, (row, col)), shape=(15, 15))  We can view the content of the matrix using the .toarray() command.\nmx.toarray()  array([[8, 0, 2, 0, 0, 1, 4, 2, 1, 1, 3, 2, 2, 1, 0],\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; [0, 3, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0],\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; [0, 0, 8, 0, 1, 2, 2, 1, 1, 2, 1, 2, 2, 2, 0],\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; [0, 0, 0, 3, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; [0, 0, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; [0, 0, 0, 0, 0, 6, 1, 0, 1, 2, 0, 0, 2, 1, 1],\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; [0, 0, 0, 0, 0, 0, 8, 1, 0, 1, 4, 1, 1, 0, 0],\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; [0, 0, 0, 0, 0, 0, 0, 4, 2, 1, 1, 0, 1, 0, 0],\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; [0, 0, 0, 0, 0, 0, 0, 0, 3, 2, 0, 0, 2, 0, 0],\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; [0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 3, 0, 1],\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0],\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 1, 0],\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 1, 0],\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0],\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]])\nWe can also visualize the matrix using the matplotlib library. This visualization shows where the non-zero values are located in the matrix, but it does not make a difference based on their values. All non-zero entries are indicated by the same color.\nplt.spy(mx) plt.show()  The chart has a slighlty different outlook if we use the matrix as a numpy array, and not as scipy.sparse.coo.coo_matrix object as plot input. The chart is built a little faster and has a different coloring scheme. The visualization, however, does not help much when we have really large amount of data, with thousands of rows and columns in our matrix: the screen resolution will not be able to differentiate between the white and non-white areas. This makes this visulization tool kind of useless in those cases when we really need to use the sparse matrix fuctionality for our resource management.\nmx_as_array = mx.toarray() plt.spy(mx_as_array) plt.show()  Finally we can save the scipy.sparse.coo.coo_matrix object using the save_npz() command. For later use, the load_npz() command imports the sparse matrix to our project.\nsave_npz('/path...', mx)  The codes can be find at my github.\n","date":1536830233,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536830233,"objectID":"565170abb88b9c1a8b436823ae41989e","permalink":"https://peterduronelly.github.io/post/sparse-matrices-in-python/","publishdate":"2018-09-13T11:17:13+02:00","relpermalink":"/post/sparse-matrices-in-python/","section":"post","summary":"One of the things we need to manage in data analysis is recources. When we have large amounts of (\u0026lsquo;big\u0026rsquo;) data this can become a serious issue. One of the cases when we need to consider whether we really need all the data we have is when we have a lot of zeros in our database, and these zeroes happen to be irrelevant for our calculations. Python\u0026rsquo;s SciPy library has a solution to store and handle sparse data matrices which contain a large number of irrelevant zero values.","tags":["Python","SciPy"],"title":"Sparse Matrices in Python","type":"post"},{"authors":["Peter Duronelly"],"categories":[],"content":" Who Am I? I am originally trainded as an economist with a Finance major and a Statistics minor. I spent 21 years on financial markets as an analyst and asset manager. After a while, though, I got interested in disruptive technologies, mostly data and data science, which are closely linked to my previous academic studies. Statistics is especially close to my heart as I used to teach it to sophomore students on the Budapest University of Economics (currently Corvinus University). To do something about my new passion, I quit my job in 2017 and completed a data science-focused master's program on the Central European University in Budapest (MSc in Business Analytics). The core of the program was Statistics, Machine Learning, some intro to neural networks, and data infrastructure in the cloud. The program was built in R and I also learnt Python (which I like better than R).  Why Do I Write a Blog? I have encountered a lot of interesting things during my studies. I had superinteresting assignments for which I had to dig out a bunch of fancy solutions in the docs, on stackoverflow, and from other people's blogs. This was when I decided to add some more to the already large and rapidly expanding content space on the net, so that I may be able to give some help to those who are looking for those things I was struggling with. Also, some of the things I did was about really cool topics (like text complexity analysis, or clustering on a dissimilarity matrix) and I think they are worth to be shared. Finally, there were some issues which took me months to solve and I hope these posts will shorten this time for those who find my posts during their search.  All blog codes will be on github in their entirety, under https://github.com/peterduronelly/blogcodes.  ","date":1536573358,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536573358,"objectID":"2428ec7a948d62d1e37f65fbab8a7988","permalink":"https://peterduronelly.github.io/post/test/","publishdate":"2018-09-10T11:55:58+02:00","relpermalink":"/post/test/","section":"post","summary":"Who Am I? I am originally trainded as an economist with a Finance major and a Statistics minor. I spent 21 years on financial markets as an analyst and asset manager. After a while, though, I got interested in disruptive technologies, mostly data and data science, which are closely linked to my previous academic studies. Statistics is especially close to my heart as I used to teach it to sophomore students on the Budapest University of Economics (currently Corvinus University).","tags":[],"title":"Introduction","type":"post"}]