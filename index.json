[{"authors":["Peter Duronelly"],"categories":[],"content":" The Beatles became hit through its powerful music but it is not famous for its poetry. But the group\u0026rsquo;s lyrics did change during the band\u0026rsquo;s short existence and we can use text analysis to track these changes. This post is about measuring the change in the complexity of the group\u0026rsquo;s lyrics, from Please, Please Me to Abbey Road, showing how we can use basic data secience tools to find really fancy patterns in unstructured text data.\nThis piece is a shortened version of a final project for a Data Science on Unstructured Text Data course held by Facebook\u0026rsquo;s Eduardo Ariño de la Rubia. The course introduced the tidytext package and the basics of text analysis in R. At the end of the course students had to present their skills through a freely chosen analysis project. Although tidytext does not directly cover text complexity, to me it was somehow an obvious choice.\nThis is a technical post, but most of the hard stuff is concentrated in the code blocks. If you are only interested in the power of data science, feel free to disregard these blocks and concentrate on the text and the plots only. You will still be able to get the message.\nWhen you learn English as a foreign language you inevitably bump into The Beatles early on. The songs are well known, and even a beginner student can easily understand the lyrics. This is not only because the members were singing in nice English, but because their early text is damned simple. \u0026lsquo;She loves you, yeah, yeah, yeah.\u0026rsquo; Not that of a challenging text, right?\nBut when you listen to The Beatles a little more, you realize that as time went by their songs got more and more sophisticated. \u0026lsquo;Strawberry Fields Forever\u0026rsquo; does have more depth than \u0026lsquo;A Hard Day\u0026rsquo;s Night\u0026rsquo;. Since we are into data science, it is obvious to ask: can we measure this change in sophistication? Can we trace the development also in their lyrics? As the members went from their early twenties towards their thirties, did they move from their simple but powerful origins towards something more mature?\nIn the next few lines I am analyzing The Beatles\u0026rsquo; thirteen albums of \u0026lsquo;core catalogues\u0026rsquo; from \u0026lsquo;Please Please Me\u0026rsquo; to \u0026lsquo;Let It Be\u0026rsquo;, published between 1964 and 1970. It is amazing but the most influential pop group of all times existed for less than a decade, and this short period was enough to issue thirteen albums and to turn the world upside down. The group had quite a few extra collections, live recordings and greatest hit compilations (the last one, according to wikipedia, in 2013), but these thirteen albums make up the the actual works of the group.\nFor the project I used the newly developed geniusR package by Josiah Parry, which downloads lyrics and metadata from the genius.com homepage. This package was of enormous help for the analysis. A Glance At The Beatles As a starter I imported the necessary packages. I like starting all analysis with the packages, having them in one single chunk for a better overview. Also, when I later need to add further packages I just scroll back to the first chunk to enter the extra library command.\nlibrary(geniusR) library(tidyverse) library(tidytext) library(tidyr) library(tibble) library(dplyr) library(purrr) library(stringr) library(syllable) library(ggplot2) library(scales) library(gridExtra) library(lsa) library(rlist) library(data.table)  Downloading the text is simple with geniusR: you define the artist, the number of albums to download, and the album titles. Album titles should be entered as the last part of the urls on the genius.com webpage without hyphens. Apostrophes are omitted. You can also download albums of multiple artists entering the author, # of albums multiple times as a vector. See the documentation and Josiah\u0026rsquo;s github for details.\nIt takes a while until your text downloads, but you end up with a nice (tidy!) tibble which serves as the basis for further analysis.\nalbums \u0026lt;- tibble( artist = rep(\u0026quot;The Beatles\u0026quot;, 13), album = c( \u0026quot;Please Please Me\u0026quot;, \u0026quot;With The Beatles\u0026quot;, \u0026quot;A Hard Day s Night\u0026quot;, \u0026quot;Beatles For Sale\u0026quot;, \u0026quot;Help\u0026quot;, \u0026quot;Rubber Soul\u0026quot;, \u0026quot;Revolver\u0026quot;, \u0026quot;Sgt Pepper s Lonely Hearts Club Band\u0026quot;, \u0026quot;Magical Mystery Tour\u0026quot;, \u0026quot;The Beatles The White Album\u0026quot;, \u0026quot;Yellow Submarine\u0026quot;, \u0026quot;Abbey Road\u0026quot;, \u0026quot;Let It Be\u0026quot; ) ) album_lyrics \u0026lt;- albums %\u0026gt;% mutate(tracks = map2(artist, album, genius_album)) beatles_lyrics \u0026lt;- album_lyrics %\u0026gt;% unnest(tracks) beatles_albums \u0026lt;- beatles_lyrics %\u0026gt;% distinct(album)  As the most obvious starting point of any text analysis, I checked the per album frequencies of non-stop words across these albums. (For the wider audience: stop words are the most common, \u0026lsquo;functional\u0026rsquo; words in a language.) In order to draw an arch of change, I plotted simple word frequency charts for Please Please Me (1963), Help (1965), Magical Mystery Tour (1967) and Let It Be (1970). Can we see any difference in the words used?\ntidy_beatles \u0026lt;- beatles_lyrics %\u0026gt;% unnest_tokens(word, text) %\u0026gt;% filter(nchar(word)\u0026gt;2)%\u0026gt;% anti_join(stop_words) %\u0026gt;% group_by(album) %\u0026gt;% count(word, sort = TRUE) %\u0026gt;% ungroup() total_words \u0026lt;- tidy_beatles %\u0026gt;% group_by(album) %\u0026gt;% summarize(total = sum(n)) tidy_beatles \u0026lt;- left_join(tidy_beatles, total_words) tidy_beatles \u0026lt;- tidy_beatles %\u0026gt;% mutate(freq = n / total) ppm \u0026lt;- tidy_beatles %\u0026gt;% filter(str_detect(album, \u0026quot;Please\u0026quot;))%\u0026gt;% arrange(desc(freq)) %\u0026gt;% top_n(10)%\u0026gt;% mutate(word = factor(word, levels = rev(unique(word)))) %\u0026gt;% ggplot(aes(word, freq, fill=I(\u0026quot;steelblue3\u0026quot;), col=I(\u0026quot;black\u0026quot;))) + geom_col(show.legend = FALSE) + labs(x = NULL, y = \u0026quot;frequency\u0026quot;) + coord_flip() + theme_bw() + labs(title = \u0026quot;Word frequency in Please Please Me\u0026quot;) + theme(plot.title = element_text(size = rel(1))) + scale_y_continuous(labels = percent) help \u0026lt;- tidy_beatles %\u0026gt;% filter(str_detect(album, \u0026quot;Help\u0026quot;))%\u0026gt;% arrange(desc(freq)) %\u0026gt;% top_n(10)%\u0026gt;% mutate(word = factor(word, levels = rev(unique(word)))) %\u0026gt;% ggplot(aes(word, freq, fill=I(\u0026quot;steelblue3\u0026quot;), col=I(\u0026quot;black\u0026quot;))) + geom_col(show.legend = FALSE) + labs(x = NULL, y = \u0026quot;frequency\u0026quot;) + coord_flip() + theme_bw() + labs(title = \u0026quot;Word frequency in Help\u0026quot;) + theme(plot.title = element_text(size = rel(1))) + scale_y_continuous(labels = percent) mys \u0026lt;- tidy_beatles %\u0026gt;% filter(str_detect(album, \u0026quot;Mystery\u0026quot;))%\u0026gt;% arrange(desc(freq)) %\u0026gt;% top_n(10)%\u0026gt;% mutate(word = factor(word, levels = rev(unique(word)))) %\u0026gt;% ggplot(aes(word, freq, fill=I(\u0026quot;steelblue3\u0026quot;), col=I(\u0026quot;black\u0026quot;))) + geom_col(show.legend = FALSE) + labs(x = NULL, y = \u0026quot;frequency\u0026quot;) + coord_flip() + theme_bw() + labs(title = \u0026quot;Word frequency in Magical Myster Tour\u0026quot;) + theme(plot.title = element_text(size = rel(1))) + scale_y_continuous(labels = percent) lib \u0026lt;- tidy_beatles %\u0026gt;% filter(str_detect(album, \u0026quot;Let\u0026quot;))%\u0026gt;% arrange(desc(freq)) %\u0026gt;% top_n(10)%\u0026gt;% mutate(word = factor(word, levels = rev(unique(word)))) %\u0026gt;% ggplot(aes(word, freq, fill=I(\u0026quot;steelblue3\u0026quot;), col=I(\u0026quot;black\u0026quot;))) + geom_col(show.legend = FALSE) + labs(x = NULL, y = \u0026quot;frequency\u0026quot;) + coord_flip() + theme_bw() + labs(title = \u0026quot;Word frequency in Let It Be\u0026quot;) + theme(plot.title = element_text(size = rel(1))) + scale_y_continuous(labels = percent) grid.arrange(ppm, help, mys, lib, nrow = 2)  Love makes it into the first 10 in three of the albums, leading the pack in Please Please Me and, to my little surprise, Magical Mystery Tour. Interestingly, it is missing from the top 10 in Let It Be, the last album. It looks, love was not of primary interest by 1970, the year when the members decided to go their own separate ways. Of course, per album word frequency depends largely on the songs\u0026rsquo; topic selection: \u0026lsquo;mother\u0026rsquo; goes to number 2 in Magical Mystery Tour due to the many repetitions of the line \u0026lsquo;Your mother should know\u0026rsquo; in the song of the same title.\nThis simple exercise shows that working with lyrics can be very tricky. Lines are repeated very often, and melody dominates sentence building. As a matter of fact, sentences can only be poorly defined by regular text analysis algorithms in songs, which, as we will see later, makes measuring text complexity somewhat difficult. Measuring Similarity Across Core Albums In order to asses how much the group changed over the course of these seven years I measured the similarity of each album to Please Please Me, the very first LP. More and more sophisticated lyrics would result in larger and larger differences in text, measured by cosine similarity.\nI calculated cosine similarity based on word frequency vectors, where each album is vector of frequencies of words in a union of sets of words from each album. The word list is a product of a full join of all words from the all the albums, and the cosine for each album is a similarity measure between that particular album and the benchmark Please Please Me. This word list excludes stop words, of course.\ncos \u0026lt;- tidy_beatles %\u0026gt;% select(album, word, freq) cos_w \u0026lt;- spread(cos, key = album, value = freq) # This is a matrix where entries are frequencies of words in the the various albums. NAs are replaced by zeros in the next command. cos_w[is.na(cos_w)] \u0026lt;- 0 cos_w \u0026lt;- cos_w %\u0026gt;% select(-word) title \u0026lt;- beatles_albums[[1]][1] a \u0026lt;- cos_w %\u0026gt;% select(as.character(title)) cosines \u0026lt;- data.table(album = character(), cosines = numeric()) for(i in 2:nrow(beatles_albums)){ title1 \u0026lt;- beatles_albums[[1]][i] l \u0026lt;- list(title1) b \u0026lt;- cos_w %\u0026gt;% select(as.character(title1)) l \u0026lt;- list.append(l, round(sum(a*b)/sqrt(sum(a^2)*sum(b^2)),3)) cosines \u0026lt;- rbind(cosines, l) } cosines \u0026lt;- data.frame(cosines) cosines \u0026lt;- cosines%\u0026gt;% arrange(desc(cosines)) cosines$album \u0026lt;- factor(cosines$album, levels = cosines$album[order(cosines$cosines)]) ggplot(cosines) + geom_col(aes(album, cosines, fill=I(\u0026quot;steelblue3\u0026quot;), col=I(\u0026quot;black\u0026quot;)),show.legend = F) + theme_bw() + coord_flip() + labs(title = \u0026quot;Cosine similarities with Please Please Me\u0026quot;, y = \u0026quot;cosine similarity\u0026quot;) + theme(plot.title = element_text(size = rel(1.25))) + ylim(0,1)  It is not surprising that \u0026lsquo;A Hard Day\u0026rsquo;s Night\u0026rsquo; is very similar to Please Please Me, but Abbey Road (no. 12) also shares a lot with it. Sgt. Pepper\u0026rsquo;s (no. 8) is an interesting album: it is the most distinct one amongst the core and, as we will see it later, by some measure it has more complex lyrics than any of the other LPs. (St. Pepper\u0026rsquo;s was the first album of the \u0026lsquo;studio years\u0026rsquo;, when the band was finally freed from the burden of permanent touring. It was an experimental album which took 700 hours to record.) Text Complexity Next I turned to text complexity. Can we see an arch of change as the group gets older, grows confidence, and starts to have something else to say than \u0026lsquo;I love you\u0026rsquo;?\nThere are various measures of text complexity, and all aims to assess the readability of prosaic test. Since these metrics have been developed to prose, applying them to lyrics, which is basically poetry, is not straightforward. In songs and poems text serves the melody (rhythm), and, for the lack of proper punctuation, standard algorithms cannot detect where sentences start and end. The basis of complexity metrics is usually the number of words in a sentence, the lengths of the words, and the ratio of complex words within all words. These measures are focusing on how the text is built, and they don\u0026rsquo;t filter for stop words, as the use of these stop words is also a sign of sophistication. These are, obviously, the simplest measures and they do not account for other dimensions of complexity, like the frequency with which people use these words or the types of texts where these words are typical. By these metrics, for instance, the word \u0026lsquo;championship\u0026rsquo; is way more elaborate than the word \u0026lsquo;cat\u0026rsquo;, although the context where the former is mostly used may not be more academic than the one for the latter.\nIn this exercise I use the modified version of two complexity measures. The \u0026lsquo;Automated Readability Index\u0026rsquo; uses characters, words and sentences so, that\nARI = 4.71 x (characters/words) + 0.5 x (words/sentences) - 21.43  The \u0026lsquo;Gunning Fog\u0026rsquo; score is based on words, sentences, and the ratio of complex words: Gunning Fog = 0.4 x ( (words/sentences) + 100 x (complex words/words) )  A word is considered to be complex if it has at least 3 syllables. Higher complexity scores indicate more complicated text.\nSince sentences are loosely defined in lyrics, I replaced them by lines, despite that sentences can be of multiple lines. Lines are the main building blocks of song text, so using them as a proxy is a viable option. As the original formulas are this way \u0026lsquo;modified\u0026rsquo;, I denote these measures as mARI and mGunningFog. In this form they are probably more imperfect as they originally are, but they do show meaningful patterns even in our case.\nIn order to calculate these measures for each album I looped through the album titles and selected distinct lines for analysis. The reason for using distinct lines is that in songs lines are many times repeated for the sake of the melody, serving as chorus, and complete verses can be reused to fill the melody with sufficient amount of text for the vocals.\nFor the complexity metrics I used the \u0026lsquo;syllable\u0026rsquo; package, which produces certain text statistics from which these scores can be calculated. The result is in the next chart.\ntext_complexity \u0026lt;- data.table(album = character(), mGunningFog = numeric(), mARI = numeric()) for(i in 1:nrow(beatles_albums)){ name \u0026lt;- beatles_albums[[1]][i] l \u0026lt;- list(name) temp_lyr \u0026lt;- beatles_lyrics %\u0026gt;% filter(album == as.character(name)) %\u0026gt;% distinct(text) rwstat \u0026lt;- readability_word_stats(temp_lyr[,1]) l \u0026lt;- list.append(l, 0.4*(rwstat$n.words/nrow(temp_lyr)) + 100*(rwstat$n.complexes/rwstat$n.words)) l \u0026lt;- list.append(l, 4.71*(rwstat$n.chars/rwstat$n.words) + 0.5*(rwstat$n.words/nrow(temp_lyr)) - 21.43) text_complexity \u0026lt;- rbind(text_complexity, l) } ggplot(data = text_complexity, aes(mARI, mGunningFog)) + geom_point(color = \u0026quot;darkblue\u0026quot;) + geom_text(aes(x = mARI, y = mGunningFog, label = album), hjust=1, vjust=-0.5) + theme_bw() + labs(title = \u0026quot;Text complexity of Beatles albums\u0026quot;) + theme(plot.title = element_text(size = rel(1.25))) + ylim(4,8) + xlim(-3,1)  While the two metrics put the albums in slightly different orders, the trend is obvious. The first albums (Please Please Me, A Hard Day\u0026rsquo;s Night, With The Beatles) are of fairly simple text, but as time goes by sophistication increases. Let It Be (no. 13) and Abbey Road (no. 12) have fairly high readings by ARI, and Let It Be is the single most complex according to Gunning Fox. Note that Sgt. Pepper (no. 8) is of relatively high complexity: it is the most complex by ARI and the third most complex by Gunning Fog. Remember, Sgt. Pepper is the most dissimilar album compared to the Please Please Me benchmark by cosine similarity.\nIf we look at the publication dates of these albums (not shown here) we do see a development in the group\u0026rsquo;s artistic performance. Four out of the last five albums (\u0026lsquo;Magical Mystery Tour\u0026rsquo;, \u0026lsquo;The Beatles The White Album\u0026rsquo;, \u0026lsquo;Abbey Road\u0026rsquo;, \u0026lsquo;Let It Be\u0026rsquo;) are in the top right corner of the chart, while the first five LPs are in the bottom left. Summary This short analysis tries to provide an insight into the carrier of The Beatles through their lyrics. Songs were very simple at the beginning, but as their music got more mature, their lyrics followed on. Measures of text complexity, just as those of similarity, can also reveal the uniqueness of the St. Pepper\u0026rsquo;s album.\nAmong the tools, tidytext, syllable, and of course the geniusR packages were used to compile this report.\n","date":1536838748,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536838748,"objectID":"824c2858be38fc58c3ac2464db1b8d34","permalink":"https://peterduronelly.github.io/post/text-complexity-analysis-of-beatles-lyrics-with-r/","publishdate":"2018-09-13T13:39:08+02:00","relpermalink":"/post/text-complexity-analysis-of-beatles-lyrics-with-r/","section":"post","summary":"The Beatles became hit through its powerful music but it is not famous for its poetry. But the group\u0026rsquo;s lyrics did change during the band\u0026rsquo;s short existence and we can use text analysis to track these changes. This post is about measuring the change in the complexity of the group\u0026rsquo;s lyrics, from Please, Please Me to Abbey Road, showing how we can use basic data secience tools to find really fancy patterns in unstructured text data.","tags":["R","tidytext","unstructured text data"],"title":"Text Complexity Analysis of Beatles Lyrics With R","type":"post"},{"authors":["Peter Duronelly"],"categories":[],"content":"One of the things we need to manage in data analysis is recources. When we have large amounts of (\u0026lsquo;big\u0026rsquo;) data this can become a serious issue. One of the cases when we need to consider whether we really need all the data we have is when we have a lot of zeros in our database, and these zeroes happen to be irrelevant for our calculations. Python\u0026rsquo;s SciPy library has a solution to store and handle sparse data matrices which contain a large number of irrelevant zero values.\nIn order to demonstrate how this works let\u0026rsquo;s first import the necessary packages.\nimport numpy as np from scipy.sparse import coo_matrix from scipy.sparse import save_npz import matplotlib import matplotlib.pyplot as plt %matplotlib inline import random  Where would we come across sparse data? Let\u0026rsquo;s imagine you are running an e-commerce site where people buy various products. Some poeple buy only one item, others buy multiple products. You want to see how clients are related, or linked, to each other based on their purchase patterns: if some customers buy the same or almost the same set of products, these clients are similar.\nMoreover, these linkages create a network clients, and this network can be used for recommendations, coupons, etc. Without going into the details how such a network can be built and used let\u0026rsquo;s see how we get to sparse data when we analyze buying patterns.\nFor the sake of simplicity let\u0026rsquo;s assume that there are 15 clients and 30 products. Each client buys between 2 and 8 pieces of items.\nclients = list(range(0,15)) # 15 clients dic = {} # a dictionary of purchases: keys are clients and values are lists of items bought for i in range(0,15): key = clients[i] random.seed(i) l = random.sample(range(0,30), random.randint(2,8)) dic[key] = l  We can print out what itemms clients have purchased.\nfor key in dic.keys(): print(dic[key])  [12, 24, 13, 1, 8, 16, 15, 29]\n[18, 27, 25]\n[27, 1, 2, 29, 11, 5, 23, 21]\n[18, 17, 4]\n[9, 3, 23]\n[8, 23, 11, 25, 22, 28]\n[18, 26, 2, 15, 24, 8, 1, 0]\n[4, 12, 20, 1]\n[11, 12, 4]\n[19, 11, 8, 4, 5]\n[1, 13, 15, 18, 0, 6]\n[27, 17, 29, 24, 14]\n[8, 21, 16, 11, 4]\n[9, 21, 29, 25]\n[19, 22]\nClients No. 0, 2 and 6 have bought eight items, and only one has bought two. Now we can build a matrix of clients where the matrix elements show the number of common items on their shopping history. Each aij element of this matrix tells the number of items bought both by client i and client j . The ith diagonal element shows the number of items bought buy client i. This is going to be a symmetric matrix, of course.\nTo build the matrix we need to define the row and column indices of the non-zero matrix elements, and the values of the elements. These are going to be lists of equal sizes, which serve as inputs for the sparse matrix. Since the matrix is symmetric, we don\u0026rsquo;t need to calculate and store the lower diagonal elements to save space.\nr = 0 # the index of the row of the matrix c = 0 # the index of the columns of the matrix counter = 0 row_indices = [] # row indices of the non-zero values column_indices = [] # column indices of the non-zero values matrix_elements = [] # the non-zero values themselves for key_r in dic.keys(): # key_r is the key for the rows x = dic[key_r] for key_c in dic.keys(): # key_c is the key for the columns if c \u0026gt;= r: y = dic[key_c] common_set = list(set(x) \u0026amp; set(y)) common_set_size = len(common_set) if common_set_size \u0026gt; 0: row_indices.append(r) column_indices.append(c) matrix_elements.append(common_set_size) c = c + 1 r = r + 1 c = 0  Once we have the lists we can build the sparse matrix using the coo_matrix function. This function takes numpy arrays as inputs so we need to convert our lists to arrays. When building the matrix we need to add the non-zero elements first, then the indices as a tuple. Finally we need to define the size (shape) of the matrix. The resulting object will be of type scipy.sparse.coo.coo_matrix.\nrow = np.array(row_indices) col = np.array(column_indices) val = np.array(matrix_elements) mx = coo_matrix((val, (row, col)), shape=(15, 15))  We can view the content of the matrix using the .toarray() command.\nmx.toarray()  array([[8, 0, 2, 0, 0, 1, 4, 2, 1, 1, 3, 2, 2, 1, 0],\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; [0, 3, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0],\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; [0, 0, 8, 0, 1, 2, 2, 1, 1, 2, 1, 2, 2, 2, 0],\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; [0, 0, 0, 3, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; [0, 0, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; [0, 0, 0, 0, 0, 6, 1, 0, 1, 2, 0, 0, 2, 1, 1],\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; [0, 0, 0, 0, 0, 0, 8, 1, 0, 1, 4, 1, 1, 0, 0],\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; [0, 0, 0, 0, 0, 0, 0, 4, 2, 1, 1, 0, 1, 0, 0],\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; [0, 0, 0, 0, 0, 0, 0, 0, 3, 2, 0, 0, 2, 0, 0],\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; [0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 3, 0, 1],\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0],\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 1, 0],\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 1, 0],\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0],\n\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]])\nWe can also visualize the matrix using the matplotlib library. This visualization shows where the non-zero values are located in the matrix, but it does not make a difference based on their values. All non-zero entries are indicated by the same color.\nplt.spy(mx) plt.show()  The chart has a slighlty different outlook if we use the matrix as a numpy array, and not as scipy.sparse.coo.coo_matrix object as plot input. The chart is built a little faster and has a different coloring scheme. The visualization, however, does not help much when we have really large amount of data, with thousands of rows and columns in our matrix: the screen resolution will not be able to differentiate between the white and non-white areas. This makes this visulization tool kind of useless in those cases when we really need to use the sparse matrix fuctionality for our resource management.\nmx_as_array = mx.toarray() plt.spy(mx_as_array) plt.show()  Finally we can save the scipy.sparse.coo.coo_matrix object using the save_npz() command. For later use, the load_npz() command imports the sparse matrix to our project.\nsave_npz('/path...', mx)  ","date":1536830233,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536830233,"objectID":"565170abb88b9c1a8b436823ae41989e","permalink":"https://peterduronelly.github.io/post/sparse-matrices-in-python/","publishdate":"2018-09-13T11:17:13+02:00","relpermalink":"/post/sparse-matrices-in-python/","section":"post","summary":"One of the things we need to manage in data analysis is recources. When we have large amounts of (\u0026lsquo;big\u0026rsquo;) data this can become a serious issue. One of the cases when we need to consider whether we really need all the data we have is when we have a lot of zeros in our database, and these zeroes happen to be irrelevant for our calculations. Python\u0026rsquo;s SciPy library has a solution to store and handle sparse data matrices which contain a large number of irrelevant zero values.","tags":["Python","SciPy"],"title":"Sparse Matrices in Python","type":"post"},{"authors":["Peter Duronelly"],"categories":[],"content":" Who Am I? I am originally trainded as an economist with a Finance major and a Statistics minor. I spent 21 years on financial markets as an analyst and asset manager. After a while, though, I got interested in disruptive technologies, mostly data and data science, which are closely linked to my previous academic studies. Statistics is especially close to my heart as I used to teach it to sophomore students on the Budapest University of Economics (currently Corvinus University). To do something about my new passion, I quit my job in 2017 and completed a data science-focused master's program on the Central European University in Budapest (MSc in Business Analytics). The core of the program was Statistics, Machine Learning, some intro to neural networks, and data infrastructure in the cloud. The program was built in R and I also learnt Python (which I like better than R).  Why Do I Write a Blog? I have encountered a lot of interesting things during my studies. I had superinteresting assignments for which I had to dig out a bunch of fancy solutions in the docs, on stackoverflow, and from other people's blogs. This was when I decided to add some more to the already large and rapidly expanding content space on the net, so that I may be able to give some help to those who are looking for those things I was struggling with. Also, some of the things I did was about really cool topics (like text complexity analysis, or clustering on a dissimilarity matrix) and I think they are worth to be shared. Finally, there were some issues which took me months to solve and I hope these posts will shorten this time for those who find my posts during their search.  All blog codes will be on github in their entirety, under https://github.com/peterduronelly/blogcodes.  ","date":1536573358,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536573358,"objectID":"2428ec7a948d62d1e37f65fbab8a7988","permalink":"https://peterduronelly.github.io/post/test/","publishdate":"2018-09-10T11:55:58+02:00","relpermalink":"/post/test/","section":"post","summary":"Who Am I? I am originally trainded as an economist with a Finance major and a Statistics minor. I spent 21 years on financial markets as an analyst and asset manager. After a while, though, I got interested in disruptive technologies, mostly data and data science, which are closely linked to my previous academic studies. Statistics is especially close to my heart as I used to teach it to sophomore students on the Budapest University of Economics (currently Corvinus University).","tags":[],"title":"Introduction","type":"post"}]