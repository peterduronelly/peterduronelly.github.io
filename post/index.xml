<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Tiny Little Things in Data Science</title>
    <link>https://peterduronelly.github.io/post/</link>
    <description>Recent content in Posts on Tiny Little Things in Data Science</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 01 Jan 2017 00:00:00 +0100</lastBuildDate>
    
	<atom:link href="https://peterduronelly.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Comparing Beatles and Bob Dylan</title>
      <link>https://peterduronelly.github.io/post/comparing-beatles-and-bob-dylan/</link>
      <pubDate>Tue, 09 Oct 2018 14:35:15 +0200</pubDate>
      
      <guid>https://peterduronelly.github.io/post/comparing-beatles-and-bob-dylan/</guid>
      <description>In a previous post I showed how to use data science tools to find hidden features in unstructured text and analyzed how the complexity of the lyrics of Beatles songs changed over time. In this post I do a little follow-up and compare complete works of The Beatles with that of two others using the same methodology and metrics. Comparing Beatles with other musicians may help put the original numbers into the perspective.</description>
    </item>
    
    <item>
      <title>Robust and Clustered Standard Errors in Stargazer</title>
      <link>https://peterduronelly.github.io/post/robust-and-clustered-standard-errors-in-stargazer/</link>
      <pubDate>Tue, 02 Oct 2018 14:14:34 +0200</pubDate>
      
      <guid>https://peterduronelly.github.io/post/robust-and-clustered-standard-errors-in-stargazer/</guid>
      <description>Stargazer is a neat tool to present model estimates. It accepts a fairly large number of object-types and creates nice-looking, ready-to-publish outputs of their main parameters. In many cases, however, the default settings do not give us the proper numerical results, and customizing the output is not that straightforward. This is part one in a two-part series on how to customize stargazer.
When I first encountered stargazer I already had a problem with the model outputs the package created: in cross-sectional data the observations are often of different sizes, which leads to heteroskedastic model residuals where simple standard errors are useless for measuring variable significance.</description>
    </item>
    
    <item>
      <title>Bootstrap Samples</title>
      <link>https://peterduronelly.github.io/post/bootstrap-samples/</link>
      <pubDate>Sun, 23 Sep 2018 20:28:32 +0200</pubDate>
      
      <guid>https://peterduronelly.github.io/post/bootstrap-samples/</guid>
      <description>Bootstrap sampling is a widely used method in machine learning and in statistics. The main idea is that we try to decrease overfitting and the chance of myopic tree-building if run our algorithm multiple times using the same data, but always taking a different sample with repetitions from our original data. (For instance, random forest builds the trees using repeated bootstrap samples.) On a machine learning class one of my class mates asked what percentage of the original data shows up in the bootstrapped sample.</description>
    </item>
    
    <item>
      <title>Clustering on a Dissimilarity Matrix</title>
      <link>https://peterduronelly.github.io/post/clustering-on-dissimilarity-matrix/</link>
      <pubDate>Fri, 14 Sep 2018 10:33:36 +0200</pubDate>
      
      <guid>https://peterduronelly.github.io/post/clustering-on-dissimilarity-matrix/</guid>
      <description>Clustering is one of the well-known unsupervised learning tools. In the standard case you have an observation matrix where observations are in rows and variables which describe them are in columns. But data can also be structured in a different way, just like the distance matrix on a map. In this case observations are by both rows and columns and each element in the observation matrix is a measure of distance, or dissimilarity, between any two observations.</description>
    </item>
    
    <item>
      <title>Text Complexity Analysis of Beatles Lyrics With R</title>
      <link>https://peterduronelly.github.io/post/text-complexity-analysis-of-beatles-lyrics-with-r/</link>
      <pubDate>Thu, 13 Sep 2018 13:39:08 +0200</pubDate>
      
      <guid>https://peterduronelly.github.io/post/text-complexity-analysis-of-beatles-lyrics-with-r/</guid>
      <description>The Beatles became a hit through its sometimes simple but always powerful music but it has never been famous for its poetry. The group&amp;rsquo;s lyrics, however, did change during the band&amp;rsquo;s short existence and we can use text analysis to track these changes. This post is about measuring the change in the complexity of the group&amp;rsquo;s lyrics, from the Please, Please Me to the Abbey Road albums, showing how we can use basic data secience tools to find really fancy patterns in unstructured text data.</description>
    </item>
    
    <item>
      <title>Sparse Matrices in Python</title>
      <link>https://peterduronelly.github.io/post/sparse-matrices-in-python/</link>
      <pubDate>Thu, 13 Sep 2018 11:17:13 +0200</pubDate>
      
      <guid>https://peterduronelly.github.io/post/sparse-matrices-in-python/</guid>
      <description>One of the things we need to manage in data analysis is recources. When we have large amounts of (&amp;lsquo;big&amp;rsquo;) data this can become a serious issue. One of the cases when we need to consider whether we really need all the data we have is when we have a lot of zeros in our database, and these zeroes happen to be irrelevant for our calculations. Python&amp;rsquo;s SciPy library has a solution to store and handle sparse data matrices which contain a large number of irrelevant zero values.</description>
    </item>
    
    <item>
      <title>Introduction</title>
      <link>https://peterduronelly.github.io/post/test/</link>
      <pubDate>Mon, 10 Sep 2018 11:55:58 +0200</pubDate>
      
      <guid>https://peterduronelly.github.io/post/test/</guid>
      <description>Who Am I? I am originally trainded as an economist with a Finance major and a Statistics minor. I spent 21 years on financial markets as an analyst and asset manager. After a while, though, I got interested in disruptive technologies, mostly data and data science, which are closely linked to my previous academic studies. Statistics is especially close to my heart as I used to teach it to sophomore students on the Budapest University of Economics (currently Corvinus University).</description>
    </item>
    
  </channel>
</rss>