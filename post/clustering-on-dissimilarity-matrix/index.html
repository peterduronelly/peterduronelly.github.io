<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 2.4.0">
  <meta name="generator" content="Hugo 0.48" />
  <meta name="author" content="Peter Duronelly">

  
  
  
  
    
  
  <meta name="description" content="Clustering is one of the well-known unsupervised learning tools. In the standard case you have an observation matrix where observations are in rows and variables which describe them are in columns. But data can also be structured in a different way, just like the distance matrix on a map. In this case observations are by both rows and columns and each element in the observation matrix is a measure of distance, or dissimilarity, between any two observations.">

  
  <link rel="alternate" hreflang="en-us" href="https://peterduronelly.github.io/post/clustering-on-dissimilarity-matrix/">

  


  

  
  
  
  <meta name="theme-color" content="#0095eb">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" crossorigin="anonymous">
        
      
    

    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700%7cRoboto:400,400italic,700%7cRoboto&#43;Mono">
  

  <link rel="stylesheet" href="/styles.css">
  

  
  
  

  
  <link rel="alternate" href="https://peterduronelly.github.io/index.xml" type="application/rss+xml" title="Tiny Little Things in Data Science">
  <link rel="feed" href="https://peterduronelly.github.io/index.xml" type="application/rss+xml" title="Tiny Little Things in Data Science">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://peterduronelly.github.io/post/clustering-on-dissimilarity-matrix/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="Tiny Little Things in Data Science">
  <meta property="og:url" content="https://peterduronelly.github.io/post/clustering-on-dissimilarity-matrix/">
  <meta property="og:title" content="Clustering on a Dissimilarity Matrix | Tiny Little Things in Data Science">
  <meta property="og:description" content="Clustering is one of the well-known unsupervised learning tools. In the standard case you have an observation matrix where observations are in rows and variables which describe them are in columns. But data can also be structured in a different way, just like the distance matrix on a map. In this case observations are by both rows and columns and each element in the observation matrix is a measure of distance, or dissimilarity, between any two observations.">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2018-09-14T10:33:36&#43;02:00">
  
  <meta property="article:modified_time" content="2018-09-14T10:33:36&#43;02:00">
  

  

  

  <title>Clustering on a Dissimilarity Matrix | Tiny Little Things in Data Science</title>

</head>
<body id="top" data-spy="scroll" data-target="#toc" data-offset="71" >

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">Tiny Little Things in Data Science</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      
      <ul class="nav navbar-nav navbar-right">
        

        

        
        
        
          
        

        <li class="nav-item">
          <a href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a href="/#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  


  <div class="article-container">
    <h1 itemprop="name">Clustering on a Dissimilarity Matrix</h1>

    

<div class="article-metadata">

  
  
  
  <div>
    
    <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">Peter Duronelly</span>
    </span>
    
  </div>
  

  <span class="article-date">
    
    <meta content="2018-09-14 10:33:36 &#43;0200 CEST" itemprop="datePublished">
    <time datetime="2018-09-14 10:33:36 &#43;0200 CEST" itemprop="dateModified">
      Sep 14, 2018
    </time>
  </span>
  <span itemscope itemprop="publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Peter Duronelly">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    9 min read
  </span>
  

  
  

  
  
  
  

  
  
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=Clustering%20on%20a%20Dissimilarity%20Matrix&amp;url=https%3a%2f%2fpeterduronelly.github.io%2fpost%2fclustering-on-dissimilarity-matrix%2f"
         target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fpeterduronelly.github.io%2fpost%2fclustering-on-dissimilarity-matrix%2f"
         target="_blank" rel="noopener">
        <i class="fab fa-facebook-f"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fpeterduronelly.github.io%2fpost%2fclustering-on-dissimilarity-matrix%2f&amp;title=Clustering%20on%20a%20Dissimilarity%20Matrix"
         target="_blank" rel="noopener">
        <i class="fab fa-linkedin-in"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=https%3a%2f%2fpeterduronelly.github.io%2fpost%2fclustering-on-dissimilarity-matrix%2f&amp;title=Clustering%20on%20a%20Dissimilarity%20Matrix"
         target="_blank" rel="noopener">
        <i class="fab fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=Clustering%20on%20a%20Dissimilarity%20Matrix&amp;body=https%3a%2f%2fpeterduronelly.github.io%2fpost%2fclustering-on-dissimilarity-matrix%2f">
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>


    <div class="article-style" itemprop="articleBody">
      <p>Clustering is one of the well-known unsupervised learning tools. In the standard case you have an observation matrix
where observations are in rows and variables which describe them are in columns. But data can also be structured in a different
way, just like the distance matrix on a map. In this case observations are by <em>both rows and columns</em> and each element in the
observation matrix is a measure of distance, or <em>dissimilarity</em>, between any two observations. This structure can also serve as
a basis for clustering, just as you can cluster cities based on the respective distances between any two of them.</p>

<p>When I first encountered this problem I did not find a solution in the standard <em>scikit-learn</em> library which you automatically call
when doing a clustering exercise. <em>SciPy</em>, fortunately, has a solution. Let&rsquo;s see how it works!</p>

<pre><code class="language-python">import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import math
from scipy.cluster.hierarchy import linkage, dendrogram, fcluster
</code></pre>

<p>The exercise is done on the hundred largest US cities. The list is coming from
<a href="https://gist.github.com/Miserlou/c5cd8364bf9b2420bb29#file-cities-json" target="_blank">https://gist.github.com/Miserlou/c5cd8364bf9b2420bb29#file-cities-json</a> in a very neat format.
(Thanks to <a href="https://gist.github.com/Miserlou" target="_blank">Rick Jones</a> for the data!)
It includes not only the city, the state and the population but the latitude/longitude coordinates of the cities
wich can be used to calculate an approximate distance measure between the cities.</p>

<pre><code class="language-python">filepath = ...
cities = pd.read_json(filepath)
cities.head()
</code></pre>

<p><img src="/img/03_clustering_dissimilarity_01_resized.jpg" alt="alt text" /></p>

<p>By selecting the top 100 cities we get sufficient number of observations and we are still able to interpret the result through simple
visualization.</p>

<pre><code class="language-python">top100 = cities[0:100]

fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
ax = plt.scatter(top100[&quot;longitude&quot;], top100[&quot;latitude&quot;])
plt.title(&quot;The Locations of the 100 Largest US Cities&quot;)
plt.show()
</code></pre>

<p><img src="/img/03_clustering_dissimilarity_02_resized.jpg" alt="alt text" /></p>

<p>A more fancy map could be generated using the <a href="https://python-visualization.github.io/folium/docs-v0.6.0/" target="_blank"><em>folium</em> package</a> but this
simple scatter plot is sufficient for our purposes. As all real, non-simulated data, our city dataset does not show a very obvious pattern.
Cities are scattered seemingly randomly on the map, and we have two outliers, Anchorage on the North-West and Honolulu on the South-West, which are
very far from all other points.</p>

<p>Now let&rsquo;s calculate the distances between the cities using the geopy package. The distances can be interpreted as <em>dissimilarities</em>:
the larger the distance between two observations the less similar they are. This is may literally not be the case with cities
(two distant cities can easily be alike), but when we think in a more abstract space the concept makes sense.
Also, city clusters are interpreted in terms of geographical distances, so using them in this exercise helps us
understand the method and its results.</p>

<pre><code class="language-python">distance_matrix = np.empty(shape = [100,100])

for i in range(100):
    for j in range(100):
        coords_i = (top100.iloc[i][&quot;latitude&quot;], top100.iloc[i][&quot;longitude&quot;])
        coords_j = (top100.iloc[j][&quot;latitude&quot;], top100.iloc[j][&quot;longitude&quot;])
        distance_matrix[i,j] = geopy.distance.vincenty(coords_i, coords_j).km
</code></pre>

<p>Before we move on to build the clusters and visualize the results let&rsquo;s create a list of city names where the name of each city is
followed by the state. This helps us interpret the cluster tree, the so-called <em>&lsquo;dendrogram&rsquo;</em>.</p>

<pre><code class="language-python">cities[&quot;city, state&quot;] = top100[&quot;city&quot;].map(str) + &quot;, &quot; + top100[&quot;state&quot;]
citynames = cities[&quot;city, state&quot;][0:100].tolist()
citynames[0:10]
</code></pre>

<p>[&lsquo;New York, New York&rsquo;,<br>
&nbsp;&lsquo;Los Angeles, California&rsquo;, <br>
&nbsp; &lsquo;Chicago, Illinois&rsquo;, <br>
&nbsp;&lsquo;Houston, Texas&rsquo;, <br>
&nbsp;&lsquo;Philadelphia, Pennsylvania&rsquo;, <br>
&nbsp;&lsquo;Phoenix, Arizona&rsquo;,<br>
&nbsp;&lsquo;San Antonio, Texas&rsquo;,<br>
&nbsp;&lsquo;San Diego, California&rsquo;,<br>
&nbsp;&lsquo;Dallas, Texas&rsquo;,<br>
&nbsp;&lsquo;San Jose, California&rsquo;]</p>

<p>No we get to the most important part! The meat of the module is the <em>linkage function</em> (<em>scipy.cluster.hierarchy.linkage(…)</em>),
which runs the actual clustering algorithms. It can handle both an <em>n x n</em> distance matrix and a regular <em>n x m</em> observation matrix
as input. If we form the clusters on a distance matrix, we need its condensed format: the upper diagonal elements
(excluding the diagonals) have to be converted into a vector (a list) with a length of <em>n(n-1)/2</em>.
The output is a <em>(n-1)x4</em> NumPy array <strong><em>Z</em></strong>, called <em>linkage matrix</em>, which contains the hierarchical clustering algorithm’s encoded results.</p>

<p>Each row in <strong><em>Z</em></strong> represents one iteration. In each row <em>i</em>, clusters with indices of <strong><em>Z</em></strong><em>[i,0]</em> and <strong><em>Z</em></strong><em>[i,1]</em> are combined in a
new cluster, based on the distance between them which is in <strong><em>Z</em></strong><em>[i,2]</em>. The number of elements in the newly formed cluster is <strong><em>Z</em></strong><em>[i,3]</em>.</p>

<p>This is a <em>hierarchical clustering</em> method: we start with elementary clusters (the observations) which are
merged into larger and larger clusters. At the end all observations form a single cluster.</p>

<p>Clusters contain observations which are close to each other. Closeness can be defined between observations (cluster elements) and the
clusters themselves. To calculate distances between two clusters we need to define two parameters:</p>

<ul>
<li><p>We need to define a <em>distance metric</em>, which measures the distances between the elements of one cluster and
the elements of the other cluster. The default is the Euclidean distance which is sufficient for our clustering project.</p></li>

<li><p>We also need to define a <em>method</em> to sum up the distances between the individual elements of two clusters to come up
with a single value for the distance between them. Just as in the case of the measures of central tendency, we are looking
for the <em>typical distance</em> between the elements of any two clusters. The <em>method</em> sets how this typical distance is defined.</p></li>
</ul>

<p>While the distance metric can also make a difference, the most important parameter in cluster formation is the <em>method</em>.
There are various options (methods) to define how far two clusters are from each other: the distance between the two closest
elements, or the one between two most distant ones, or some sort of average of the distances between elements of cluster
<em>a</em> and cluster <em>b</em>, etc. If cluster <em>a</em> and cluster <em>b</em> have <em>u</em> and <em>v</em> number of elements, respectively, we will have <em>u*v</em>
distances which we can use to find the distance between clusters <em>a</em> and <em>b</em>. This is the modeler’s choice and, as I will
show it later, it can create markedly different cluster structures. The methods are very well explained in the <a href="http://scipy.github.io/devdocs/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage" target="_blank">SciPy
documentation</a>
and I urge you to check it out. At this point, however, we only need to keep in mind that the method is the most important
parameter we need to set.</p>

<p>The most evident difference between clustering methods is how they handle outliers (Anchorage and Honolulu). We will see
that merging the mainland and the coastal areas will be completely different in some cases.</p>

<p>And now comes the trick! SciPy does not use the whole dissimilarity matrix for the calculations, only a list of the upper
diagonal elements. The length of the list is <em>n(n-1)/2</em> and SciPy automatically calculates the number of observations
it needs to cluster.</p>

<pre><code class="language-python">l = [] # upper triangular elements of the distance matrix 

for i in range(0, distance_matrix.shape[0]):
    for j in range(i + 1, distance_matrix.shape[0]):
        l.append(distance_matrix[i,j])

Z = linkage(l, method=&quot;ward&quot;) # this is the meat of the thing!
</code></pre>

<p>Once we have the linkage matrix, which is basically a log of which observation or cluster got merged in each step,
we can visualize the results.</p>

<pre><code class="language-python">fig = plt.figure(figsize=(25, 10))
plt.title(&quot;Hierarchical Clustering Dendrogram\nLinkage = ward&quot;)
plt.xlabel('city, state')
plt.ylabel('distance')
dn = dendrogram(Z,
               labels = citynames,
    leaf_rotation=90.,
    leaf_font_size=10.,
    show_contracted=True)
plt.show()
</code></pre>

<p><img src="/img/03_clustering_dissimilarity_03_resized.jpg" alt="alt text" /></p>

<p>First we used the <em>&lsquo;ward&rsquo;</em> method which is an optimization algorithm: it aims to minimize within-cluster variance.
Cities belong to two major clusters: the ones with green linkages can be called &lsquo;The West&rsquo;, while the ones with red linkages
can be considered &lsquo;The East&rsquo; or &lsquo;East + Midwest&rsquo;.
Our outliers (Anchorage and Honolulu) belong to the West, which makes sense, and they form a
cluster together before being linked to the other Western cities. Before they are linked, however, mainland Western/West Coast
cities are merged into clusters at higher and higher levels of aggregation. Only then come the two overseas cities.</p>

<p>Cities in Arizona are merged into their own clusters, then Nevada and California. Boise City, Idaho goes to Nevada/California
after being merged with the cluster formed by Seattle and Portland. Everything else belong to the East Coast,
including cities in Minnesota, Wisconsin and Texas. The dendrogram is sort of ‘smooth’ or ‘balanced’, thanks to
the algorithm&rsquo;s variance minimalization.</p>

<p>The vertical axis shows the distance, or the level of dissimilarities between the clusters. The longer the vertical lines the more
dissimilar the two clusters which are merged in that step. The lenght of the two vertical blue lines shows
that this method considers the West Coast, which includes the outlier cities, to be very different from everything else.</p>

<p>The structure changes a lot when cluster distances are defined as the average distance between the members of the two clusters.
The fact that cluster formation is not variance-optimized results in an unbalanced-looking tree.</p>

<pre><code class="language-python">Z = linkage(l, method=&quot;average&quot;)

fig = plt.figure(figsize=(25, 10))
plt.title(&quot;Hierarchical Clustering Dendrogram\nLinkage = average&quot;)
plt.xlabel('city, state')
plt.ylabel('distance')
dn = dendrogram(Z,
               labels = citynames,
    leaf_rotation=90.,
    leaf_font_size=10.,
    show_contracted=True)
plt.show()
</code></pre>

<p><img src="/img/03_clustering_dissimilarity_04_resized.jpg" alt="alt text" /></p>

<p>The most visible difference, as I mentioned before, is how the outliers are handled. Here they are merged
with other mainland cities only at the end, <em>only after</em> the other 98 cities have been joined together. There is
more emphasis on their uniqueness than before, when within-cluster variance ruled cluster formation. East
Coast and West Coast are made up from mostly the same cities as before, but their aggregation, from individual
cities to larger and larger formations has a somewhat different profile.</p>

<p>It is also interesting to see how the trio of Boise City and Seattle &amp; Portland is linked to Arizona and California.
In the ‘ward’ optimization mechanism they are merged with California before they together are put together with
Arizona. In this latter case Arizona, California and Nevada are merged before the trio of Boise, Seattle &amp; Portland
joins them as the last West Coast areas.</p>

<p>No one is better than the other, and both the nature of the data and the target of the modeling exercise are
important in selecting the best method. In the case of the US cities, for example, if we think that the mainland
cities share more with each other than with Anchorage or Honolulu, ‘average’ is a better choice. If we simply want
a split into East and West, then ‘ward’ will be our method.</p>

<p>As a summary: clustering is possible in Python when the data does not come as an <em>n x p</em> matrix of <em>n</em> observations
and <em>p</em> variables, but as an <em>n x n</em> dissimilarity or distance matrix. The home of the algorithm is the SciPy package,
and depending on the method, we can have very different results.</p>

<p>The codes are at the <a href="https://github.com/peterduronelly/blogcodes/blob/master/03-Clustering-Based-On-A-Distance-Matrix.ipynb" target="_blank">usual place</a>.</p>

    </div>

    


<div class="article-tags">
  
  <a class="label label-default" href="https://peterduronelly.github.io/tags/python/">Python</a>
  
  <a class="label label-default" href="https://peterduronelly.github.io/tags/scipy/">SciPy</a>
  
  <a class="label label-default" href="https://peterduronelly.github.io/tags/clustering/">clustering</a>
  
</div>




    
    
    <div class="article-widget">
      <div class="hr-light"></div>
      <h3>Related</h3>
      <ul>
        
        <li><a href="/post/sparse-matrices-in-python/">Sparse Matrices in Python</a></li>
        
      </ul>
    </div>
    

    

    


  </div>
</article>

<footer class="site-footer">
  <div class="container">

    

    <p class="powered-by">

      &copy; 2018 &middot; 

      Powered by the
      <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
      <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fas fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
        
      

      
      
    

    <script src="/js/hugo-academic.js"></script>
    

    
    

    
    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    <script src="/js/search.js"></script>
    

    
    

  </body>
</html>

