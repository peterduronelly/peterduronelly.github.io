<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tiny Little Things in Data Science on Tiny Little Things in Data Science</title>
    <link>https://peterduronelly.github.io/</link>
    <description>Recent content in Tiny Little Things in Data Science on Tiny Little Things in Data Science</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 +0200</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Clustering on a Dissimilarity Matrix</title>
      <link>https://peterduronelly.github.io/post/clustering-on-dissimilarity-matrix/</link>
      <pubDate>Fri, 14 Sep 2018 10:33:36 +0200</pubDate>
      
      <guid>https://peterduronelly.github.io/post/clustering-on-dissimilarity-matrix/</guid>
      <description>&lt;p&gt;Clustering is one of the well-known unsupervised learning tools. In the standard case you have an observation matrix
where observations are in rows and variables which describe them are in columns. But data can also be structured in a different
way, just like the distance matrix on a map. In this case observations are by &lt;em&gt;both rows and columns&lt;/em&gt; and each element in the
observation matrix is a measure of distance, or &lt;em&gt;dissimilarity&lt;/em&gt;, between any two observations. This structure can also serve as
a basis for clustering, just as you can cluster cities based on the respective distances between any two of them.&lt;/p&gt;

&lt;p&gt;When I first encountered this problem I did not find a solution in the standard &lt;em&gt;scikit-learn&lt;/em&gt; library which you automatically call
when doing a clustering exercise. &lt;em&gt;SciPy&lt;/em&gt;, fortunately, has a solution. Let&amp;rsquo;s see how it works!&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import math
from scipy.cluster.hierarchy import linkage, dendrogram, fcluster
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The exercise is done on the hundred largest US cities. The list is coming from
&lt;a href=&#34;https://gist.github.com/Miserlou/c5cd8364bf9b2420bb29#file-cities-json&#34; target=&#34;_blank&#34;&gt;https://gist.github.com/Miserlou/c5cd8364bf9b2420bb29#file-cities-json&lt;/a&gt; in a very neat format.
(Thanks to &lt;a href=&#34;https://gist.github.com/Miserlou&#34; target=&#34;_blank&#34;&gt;Rick Jones&lt;/a&gt; for the data!)
It includes not only the city, the state and the population but the latitude/longitude coordinates of the cities
wich can be used to calculate an approximate distance measure between the cities.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;filepath = ...
cities = pd.read_json(filepath)
cities.head()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://peterduronelly.github.io/img/03_clustering_dissimilarity_01_resized.jpg&#34; alt=&#34;alt text&#34; /&gt;&lt;/p&gt;

&lt;p&gt;By selecting the top 100 cities we get sufficient number of observations and we are still able to interpret the result through simple
visualization.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;top100 = cities[0:100]

fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
ax = plt.scatter(top100[&amp;quot;longitude&amp;quot;], top100[&amp;quot;latitude&amp;quot;])
plt.title(&amp;quot;The Locations of the 100 Largest US Cities&amp;quot;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://peterduronelly.github.io/img/03_clustering_dissimilarity_02_resized.jpg&#34; alt=&#34;alt text&#34; /&gt;&lt;/p&gt;

&lt;p&gt;A more fancy map could be generated using the &lt;a href=&#34;https://python-visualization.github.io/folium/docs-v0.6.0/&#34; target=&#34;_blank&#34;&gt;&lt;em&gt;folium&lt;/em&gt; package&lt;/a&gt; but this
simple scatter plot is sufficient for our purposes. As all real, non-simulated data, our city dataset does not show a very obvious pattern.
Cities are scattered seemingly randomly on the map, and we have two outliers, Anchorage on the North-West and Honolulu on the South-West, which are
very far from all other points.&lt;/p&gt;

&lt;p&gt;Now let&amp;rsquo;s calculate the distances between the cities using the geopy package. The distances can be interpreted as &lt;em&gt;dissimilarities&lt;/em&gt;:
the larger the distance between two observations the less similar they are. This is may literally not be the case with cities
(two distant cities can easily be alike), but when we think in a more abstract space the concept makes sense.
Also, city clusters are interpreted in terms of geographical distances, so using them in this exercise helps us
understand the method and its results.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;distance_matrix = np.empty(shape = [100,100])

for i in range(100):
    for j in range(100):
        coords_i = (top100.iloc[i][&amp;quot;latitude&amp;quot;], top100.iloc[i][&amp;quot;longitude&amp;quot;])
        coords_j = (top100.iloc[j][&amp;quot;latitude&amp;quot;], top100.iloc[j][&amp;quot;longitude&amp;quot;])
        distance_matrix[i,j] = geopy.distance.vincenty(coords_i, coords_j).km
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Before we move on to build the clusters and visualize the results let&amp;rsquo;s create a list of city names where the name of each city is
followed by the state. This helps us interpret the cluster tree, the so-called &lt;em&gt;&amp;lsquo;dendrogram&amp;rsquo;&lt;/em&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cities[&amp;quot;city, state&amp;quot;] = top100[&amp;quot;city&amp;quot;].map(str) + &amp;quot;, &amp;quot; + top100[&amp;quot;state&amp;quot;]
citynames = cities[&amp;quot;city, state&amp;quot;][0:100].tolist()
citynames[0:10]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;[&amp;lsquo;New York, New York&amp;rsquo;,&lt;br&gt;
&amp;nbsp;&amp;lsquo;Los Angeles, California&amp;rsquo;, &lt;br&gt;
&amp;nbsp; &amp;lsquo;Chicago, Illinois&amp;rsquo;, &lt;br&gt;
&amp;nbsp;&amp;lsquo;Houston, Texas&amp;rsquo;, &lt;br&gt;
&amp;nbsp;&amp;lsquo;Philadelphia, Pennsylvania&amp;rsquo;, &lt;br&gt;
&amp;nbsp;&amp;lsquo;Phoenix, Arizona&amp;rsquo;,&lt;br&gt;
&amp;nbsp;&amp;lsquo;San Antonio, Texas&amp;rsquo;,&lt;br&gt;
&amp;nbsp;&amp;lsquo;San Diego, California&amp;rsquo;,&lt;br&gt;
&amp;nbsp;&amp;lsquo;Dallas, Texas&amp;rsquo;,&lt;br&gt;
&amp;nbsp;&amp;lsquo;San Jose, California&amp;rsquo;]&lt;/p&gt;

&lt;p&gt;No we get to the most important part! The meat of the module is the &lt;em&gt;linkage function&lt;/em&gt; (&lt;em&gt;scipy.cluster.hierarchy.linkage(…)&lt;/em&gt;),
which runs the actual clustering algorithms. It can handle both an &lt;em&gt;n x n&lt;/em&gt; distance matrix and a regular &lt;em&gt;n x m&lt;/em&gt; observation matrix
as input. If we form the clusters on a distance matrix, we need its condensed format: the upper diagonal elements
(excluding the diagonals) have to be converted into a vector (a list) with a length of &lt;em&gt;n(n-1)/2&lt;/em&gt;.
The output is a &lt;em&gt;(n-1)x4&lt;/em&gt; NumPy array &lt;strong&gt;&lt;em&gt;Z&lt;/em&gt;&lt;/strong&gt;, called &lt;em&gt;linkage matrix&lt;/em&gt;, which contains the hierarchical clustering algorithm’s encoded results.&lt;/p&gt;

&lt;p&gt;Each row in &lt;strong&gt;&lt;em&gt;Z&lt;/em&gt;&lt;/strong&gt; represents one iteration. In each row &lt;em&gt;i&lt;/em&gt;, clusters with indices of &lt;strong&gt;&lt;em&gt;Z&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;[i,0]&lt;/em&gt; and &lt;strong&gt;&lt;em&gt;Z&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;[i,1]&lt;/em&gt; are combined in a
new cluster, based on the distance between them which is in &lt;strong&gt;&lt;em&gt;Z&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;[i,2]&lt;/em&gt;. The number of elements in the newly formed cluster is &lt;strong&gt;&lt;em&gt;Z&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;[i,3]&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;This is a &lt;em&gt;hierarchical clustering&lt;/em&gt; method: we start with elementary clusters (the observations) which are
merged into larger and larger clusters. At the end all observations form a single cluster.&lt;/p&gt;

&lt;p&gt;Clusters contain observations which are close to each other. Closeness can be defined between observations (cluster elements) and the
clusters themselves. To calculate distances between two clusters we need to define two parameters:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We need to define a &lt;em&gt;distance metric&lt;/em&gt;, which measures the distances between the elements of one cluster and
the elements of the other cluster. The default is the Euclidean distance which is sufficient for our clustering project.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;We also need to define a &lt;em&gt;method&lt;/em&gt; to sum up the distances between the individual elements of two clusters to come up
with a single value for the distance between them. Just as in the case of the measures of central tendency, we are looking
for the &lt;em&gt;typical distance&lt;/em&gt; between the elements of any two clusters. The &lt;em&gt;method&lt;/em&gt; sets how this typical distance is defined.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While the distance metric can also make a difference, the most important parameter in cluster formation is the &lt;em&gt;method&lt;/em&gt;.
There are various options (methods) to define how far two clusters are from each other: the distance between the two closest
elements, or the one between two most distant ones, or some sort of average of the distances between elements of cluster
&lt;em&gt;a&lt;/em&gt; and cluster &lt;em&gt;b&lt;/em&gt;, etc. If cluster &lt;em&gt;a&lt;/em&gt; and cluster &lt;em&gt;b&lt;/em&gt; have &lt;em&gt;u&lt;/em&gt; and &lt;em&gt;v&lt;/em&gt; number of elements, respectively, we will have &lt;em&gt;u*v&lt;/em&gt;
distances which we can use to find the distance between clusters &lt;em&gt;a&lt;/em&gt; and &lt;em&gt;b&lt;/em&gt;. This is the modeler’s choice and, as I will
show it later, it can create markedly different cluster structures. The methods are very well explained in the &lt;a href=&#34;http://scipy.github.io/devdocs/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage&#34; target=&#34;_blank&#34;&gt;SciPy
documentation&lt;/a&gt;
and I urge you to check it out. At this point, however, we only need to keep in mind that the method is the most important
parameter we need to set.&lt;/p&gt;

&lt;p&gt;The most evident difference between clustering methods is how they handle outliers (Anchorage and Honolulu). We will see
that merging the mainland and the coastal areas will be completely different in some cases.&lt;/p&gt;

&lt;p&gt;And now comes the trick! SciPy does not use the whole dissimilarity matrix for the calculations, only a list of the upper
diagonal elements. The length of the list is &lt;em&gt;n(n-1)/2&lt;/em&gt; and SciPy automatically calculates the number of observations
it needs to cluster.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;l = [] # upper triangular elements of the distance matrix 

for i in range(0, distance_matrix.shape[0]):
    for j in range(i + 1, distance_matrix.shape[0]):
        l.append(distance_matrix[i,j])

Z = linkage(l, method=&amp;quot;ward&amp;quot;) # this is the meat of the thing!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once we have the linkage matrix, which is basically a log of which observation or cluster got merged in each step,
we can visualize the results.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig = plt.figure(figsize=(25, 10))
plt.title(&amp;quot;Hierarchical Clustering Dendrogram\nLinkage = ward&amp;quot;)
plt.xlabel(&#39;city, state&#39;)
plt.ylabel(&#39;distance&#39;)
dn = dendrogram(Z,
               labels = citynames,
    leaf_rotation=90.,
    leaf_font_size=10.,
    show_contracted=True)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://peterduronelly.github.io/img/03_clustering_dissimilarity_03_resized.jpg&#34; alt=&#34;alt text&#34; /&gt;&lt;/p&gt;

&lt;p&gt;First we used the &lt;em&gt;&amp;lsquo;ward&amp;rsquo;&lt;/em&gt; method which is an optimization algorithm: it aims to minimize within-cluster variance.
Cities belong to two major clusters: the ones with green linkages can be called &amp;lsquo;The West&amp;rsquo;, while the ones with red linkages
can be considered &amp;lsquo;The East&amp;rsquo; or &amp;lsquo;East + Midwest&amp;rsquo;.
Our outliers (Anchorage and Honolulu) belong to the West, which makes sense, and they form a
cluster together before being linked to the other Western cities. Before they are linked, however, mainland Western/West Coast
cities are merged into clusters at higher and higher levels of aggregation. Only then come the two overseas cities.&lt;/p&gt;

&lt;p&gt;Cities in Arizona are merged into their own clusters, then Nevada and California. Boise City, Idaho goes to Nevada/California
after being merged with the cluster formed by Seattle and Portland. Everything else belong to the East Coast,
including cities in Minnesota, Wisconsin and Texas. The dendrogram is sort of ‘smooth’ or ‘balanced’, thanks to
the algorithm&amp;rsquo;s variance minimalization.&lt;/p&gt;

&lt;p&gt;The vertical axis shows the distance, or the level of dissimilarities between the clusters. The longer the vertical lines the more
dissimilar the two clusters which are merged in that step. The lenght of the two vertical blue lines shows
that this method considers the West Coast, which includes the outlier cities, to be very different from everything else.&lt;/p&gt;

&lt;p&gt;The structure changes a lot when cluster distances are defined as the average distance between the members of the two clusters.
The fact that cluster formation is not variance-optimized results in an unbalanced-looking tree.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Z = linkage(l, method=&amp;quot;average&amp;quot;)

fig = plt.figure(figsize=(25, 10))
plt.title(&amp;quot;Hierarchical Clustering Dendrogram\nLinkage = average&amp;quot;)
plt.xlabel(&#39;city, state&#39;)
plt.ylabel(&#39;distance&#39;)
dn = dendrogram(Z,
               labels = citynames,
    leaf_rotation=90.,
    leaf_font_size=10.,
    show_contracted=True)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://peterduronelly.github.io/img/03_clustering_dissimilarity_04_resized.jpg&#34; alt=&#34;alt text&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The most visible difference, as I mentioned before, is how the outliers are handled. Here they are merged
with other mainland cities only at the end, &lt;em&gt;only after&lt;/em&gt; the other 98 cities have been joined together. There is
more emphasis on their uniqueness than before, when within-cluster variance ruled cluster formation. East
Coast and West Coast are made up from mostly the same cities as before, but their aggregation, from individual
cities to larger and larger formations has a somewhat different profile.&lt;/p&gt;

&lt;p&gt;It is also interesting to see how the trio of Boise City and Seattle &amp;amp; Portland is linked to Arizona and California.
In the ‘ward’ optimization mechanism they are merged with California before they together are put together with
Arizona. In this latter case Arizona, California and Nevada are merged before the trio of Boise, Seattle &amp;amp; Portland
joins them as the last West Coast areas.&lt;/p&gt;

&lt;p&gt;No one is better than the other, and both the nature of the data and the target of the modeling exercise are
important in selecting the best method. In the case of the US cities, for example, if we think that the mainland
cities share more with each other than with Anchorage or Honolulu, ‘average’ is a better choice. If we simply want
a split into East and West, then ‘ward’ will be our method.&lt;/p&gt;

&lt;p&gt;As a summary: clustering is possible in Python when the data does not come as an &lt;em&gt;n x p&lt;/em&gt; matrix of &lt;em&gt;n&lt;/em&gt; observations
and &lt;em&gt;p&lt;/em&gt; variables, but as an &lt;em&gt;n x n&lt;/em&gt; dissimilarity or distance matrix. The home of the algorithm is the SciPy package,
and depending on the method, we can have very different results.&lt;/p&gt;

&lt;p&gt;The codes are at the &lt;a href=&#34;https://github.com/peterduronelly/blogcodes/blob/master/03-Clustering-Based-On-A-Distance-Matrix.ipynb&#34; target=&#34;_blank&#34;&gt;usual place&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Text Complexity Analysis of Beatles Lyrics With R</title>
      <link>https://peterduronelly.github.io/post/text-complexity-analysis-of-beatles-lyrics-with-r/</link>
      <pubDate>Thu, 13 Sep 2018 13:39:08 +0200</pubDate>
      
      <guid>https://peterduronelly.github.io/post/text-complexity-analysis-of-beatles-lyrics-with-r/</guid>
      <description>

&lt;p&gt;The Beatles became a hit through its sometimes simple but always powerful music but it has never been famous for its poetry. The group&amp;rsquo;s
lyrics, however, did change during the band&amp;rsquo;s short existence and we can use text analysis to track these changes.
This post is about measuring the change in the complexity of the group&amp;rsquo;s lyrics, from the Please, Please Me
to the Abbey Road albums, showing how we can use basic data secience tools to find really fancy patterns in unstructured
text data.&lt;/p&gt;

&lt;p&gt;This piece is a shortened version of a final project for a Data Science
on Unstructured Text Data course held by Facebook&amp;rsquo;s Eduardo Ariño de la
Rubia. The course introduced the tidytext package and the basics of text
analysis in R. At the end of the course students had to present their
skills through a freely chosen analysis project. Although tidytext does
not directly cover text complexity, to me it was somehow an obvious
choice.&lt;/p&gt;

&lt;p&gt;This is a technical post, but most of the hard stuff is concentrated in the
code blocks. If you are only interested in the power of data science, feel free
to disregard these blocks and concentrate on the text and the plots only. You
will still be able to get the message.&lt;/p&gt;

&lt;p&gt;When you learn English as a foreign language you inevitably bump into
The Beatles early on. The songs are well known, and even a beginner
student can easily understand the lyrics. This is not only because the
members were singing in nice English, but because their early text is
damned simple. &amp;lsquo;She loves you, yeah, yeah, yeah.&amp;rsquo; Not that of a
challenging text, right?&lt;/p&gt;

&lt;p&gt;But when you listen to The Beatles a little more, you realize that as
time went by their songs got more and more sophisticated. &amp;lsquo;Strawberry
Fields Forever&amp;rsquo; does have more depth than &amp;lsquo;A Hard Day&amp;rsquo;s Night&amp;rsquo;. Since we
are into data science, it is obvious to ask: can we measure this change
in sophistication? Can we trace the development also in their lyrics? As
the members went from their early twenties towards their thirties, did
they move from their simple but powerful origins towards something more
mature?&lt;/p&gt;

&lt;p&gt;In the next few lines I am analyzing The Beatles&amp;rsquo; thirteen albums of
&amp;lsquo;core catalogues&amp;rsquo; from &amp;lsquo;Please Please Me&amp;rsquo; to &amp;lsquo;Let It Be&amp;rsquo;, published
between 1964 and 1970. It is amazing but the most influential pop group
of all times existed for less than a decade, and this short period was
enough to issue thirteen albums and to turn the world upside down. The
group had quite a few extra collections, live recordings and greatest
hit compilations (the last one, according to wikipedia, in 2013), but
these thirteen albums make up the the actual works of the group.&lt;/p&gt;

&lt;p&gt;For the project I used the newly developed &lt;a href=&#34;https://github.com/JosiahParry/geniusR&#34; target=&#34;_blank&#34;&gt;geniusR package&lt;/a&gt; by Josiah Parry, which downloads lyrics and
metadata from the genius.com homepage. This package was of enormous help
for the analysis.
&lt;br&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;a-glance-at-the-beatles&#34;&gt;A Glance At The Beatles&lt;/h2&gt;

&lt;p&gt;As a starter I imported the necessary packages. I like starting all
analysis with the packages, having them in one single chunk for a better
overview. Also, when I later need to add further packages I just scroll
back to the first chunk to enter the extra library command.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-.r&#34;&gt;library(geniusR)
library(tidyverse)
library(tidytext)
library(tidyr)
library(tibble)
library(dplyr)
library(purrr)
library(stringr)
library(syllable)
library(ggplot2)
library(scales)
library(gridExtra)
library(lsa)
library(rlist)
library(data.table)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Downloading the text is simple with geniusR: you define the artist, the
number of albums to download, and the album titles. Album titles should
be entered as the last part of the urls on the genius.com webpage
without hyphens. Apostrophes are omitted. You can also download albums
of multiple artists entering the author, # of albums multiple
times as a vector. See the documentation and Josiah&amp;rsquo;s github for
details.&lt;/p&gt;

&lt;p&gt;It takes a while until your text downloads, but you end up with a nice
(tidy!) tibble which serves as the basis for further analysis.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-.r&#34;&gt;albums &amp;lt;-  tibble(
  artist = 
    rep(&amp;quot;The Beatles&amp;quot;, 13),
  album = c(
    &amp;quot;Please Please Me&amp;quot;, &amp;quot;With The Beatles&amp;quot;, &amp;quot;A Hard Day s Night&amp;quot;,
    &amp;quot;Beatles For Sale&amp;quot;, &amp;quot;Help&amp;quot;, &amp;quot;Rubber Soul&amp;quot;,
    &amp;quot;Revolver&amp;quot;, &amp;quot;Sgt Pepper s Lonely Hearts Club Band&amp;quot;, &amp;quot;Magical Mystery Tour&amp;quot;,
    &amp;quot;The Beatles The White Album&amp;quot;, &amp;quot;Yellow Submarine&amp;quot;, &amp;quot;Abbey Road&amp;quot;,
    &amp;quot;Let It Be&amp;quot;
  )
)

album_lyrics &amp;lt;- albums %&amp;gt;% 
  mutate(tracks = map2(artist, album, genius_album))

beatles_lyrics &amp;lt;- album_lyrics %&amp;gt;% 
  unnest(tracks) 

beatles_albums &amp;lt;- beatles_lyrics %&amp;gt;%
  distinct(album)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As the most obvious starting point of any text analysis, I checked the
per album frequencies of non-stop words across these albums. (For the wider
audience: &lt;a href=&#34;https://en.wikipedia.org/wiki/Stop_words&#34; target=&#34;_blank&#34;&gt;stop words&lt;/a&gt; are the most common, &amp;lsquo;functional&amp;rsquo; words in a language.) In order to
draw an arch of change, I plotted simple word frequency charts for
Please Please Me (1963), Help (1965), Magical Mystery Tour (1967) and
Let It Be (1970). Can we see any difference in the words used?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-.r&#34;&gt;tidy_beatles &amp;lt;- beatles_lyrics %&amp;gt;%
  unnest_tokens(word, text) %&amp;gt;%
  filter(nchar(word)&amp;gt;2)%&amp;gt;%
  anti_join(stop_words) %&amp;gt;%
  group_by(album) %&amp;gt;%
  count(word, sort = TRUE) %&amp;gt;%
  ungroup()

total_words &amp;lt;- tidy_beatles %&amp;gt;% 
  group_by(album) %&amp;gt;% 
  summarize(total = sum(n))

tidy_beatles &amp;lt;- left_join(tidy_beatles, total_words)

tidy_beatles &amp;lt;- tidy_beatles %&amp;gt;%
  mutate(freq = n / total)
  
ppm &amp;lt;- tidy_beatles %&amp;gt;%
  filter(str_detect(album, &amp;quot;Please&amp;quot;))%&amp;gt;%
  arrange(desc(freq)) %&amp;gt;%
  top_n(10)%&amp;gt;%
  mutate(word = factor(word, levels = rev(unique(word)))) %&amp;gt;% 
  ggplot(aes(word, freq, fill=I(&amp;quot;steelblue3&amp;quot;), col=I(&amp;quot;black&amp;quot;))) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = &amp;quot;frequency&amp;quot;) +
  coord_flip() + 
  theme_bw() + 
  labs(title = &amp;quot;Word frequency in Please Please Me&amp;quot;) + 
  theme(plot.title = element_text(size = rel(1))) +
  scale_y_continuous(labels = percent)

help &amp;lt;- tidy_beatles %&amp;gt;%
  filter(str_detect(album, &amp;quot;Help&amp;quot;))%&amp;gt;%
  arrange(desc(freq)) %&amp;gt;%
  top_n(10)%&amp;gt;%
  mutate(word = factor(word, levels = rev(unique(word)))) %&amp;gt;% 
  ggplot(aes(word, freq, fill=I(&amp;quot;steelblue3&amp;quot;), col=I(&amp;quot;black&amp;quot;))) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = &amp;quot;frequency&amp;quot;) +
  coord_flip() + 
  theme_bw() + 
  labs(title = &amp;quot;Word frequency in Help&amp;quot;) + 
  theme(plot.title = element_text(size = rel(1))) +
  scale_y_continuous(labels = percent)

mys &amp;lt;- tidy_beatles %&amp;gt;%
  filter(str_detect(album, &amp;quot;Mystery&amp;quot;))%&amp;gt;%
  arrange(desc(freq)) %&amp;gt;%
  top_n(10)%&amp;gt;%
  mutate(word = factor(word, levels = rev(unique(word)))) %&amp;gt;% 
  ggplot(aes(word, freq, fill=I(&amp;quot;steelblue3&amp;quot;), col=I(&amp;quot;black&amp;quot;))) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = &amp;quot;frequency&amp;quot;) +
  coord_flip() + 
  theme_bw() + 
  labs(title = &amp;quot;Word frequency in Magical Myster Tour&amp;quot;) + 
  theme(plot.title = element_text(size = rel(1))) +
  scale_y_continuous(labels = percent)

lib &amp;lt;- tidy_beatles %&amp;gt;%
  filter(str_detect(album, &amp;quot;Let&amp;quot;))%&amp;gt;%
  arrange(desc(freq)) %&amp;gt;%
  top_n(10)%&amp;gt;%
  mutate(word = factor(word, levels = rev(unique(word)))) %&amp;gt;% 
  ggplot(aes(word, freq, fill=I(&amp;quot;steelblue3&amp;quot;), col=I(&amp;quot;black&amp;quot;))) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = &amp;quot;frequency&amp;quot;) +
  coord_flip() + 
  theme_bw() + 
  labs(title = &amp;quot;Word frequency in Let It Be&amp;quot;) + 
  theme(plot.title = element_text(size = rel(1))) +
  scale_y_continuous(labels = percent)

grid.arrange(ppm, help, mys, lib, nrow = 2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://peterduronelly.github.io/img/02_beatles_01_resize.jpg&#34; alt=&#34;alt text&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Love makes it into the first 10 in three of the albums, leading the pack
in Please Please Me and, to my little surprise, Magical Mystery Tour.
Interestingly, it is missing from the top 10 in Let It Be, the last
album. It looks, love was not of primary interest by 1970, the year when
the members decided to go their own separate ways. Of course, per album
word frequency depends largely on the songs&amp;rsquo; topic selection: &amp;lsquo;mother&amp;rsquo;
goes to number 2 in Magical Mystery Tour due to the many repetitions of
the line &amp;lsquo;Your mother should know&amp;rsquo; in the song of the same title.&lt;/p&gt;

&lt;p&gt;This simple exercise shows that working with lyrics can be very tricky.
Lines are repeated very often, and melody dominates sentence building.
As a matter of fact, sentences can only be poorly defined by regular
text analysis algorithms in songs, which, as we will see later, makes
measuring text complexity somewhat difficult.
&lt;br&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;measuring-similarity-across-core-albums&#34;&gt;Measuring Similarity Across Core Albums&lt;/h2&gt;

&lt;p&gt;In order to asses how much the group changed over the course of these
seven years I measured the similarity of each album to Please Please Me,
the very first LP. More and more sophisticated lyrics would result in
larger and larger differences in text, measured by cosine similarity.&lt;/p&gt;

&lt;p&gt;I calculated cosine similarity based on word frequency vectors, where
each album is vector of frequencies of words in a union of sets of words
from each album. The word list is a product of a full join of all words
from the all the albums, and the cosine for each album is a similarity
measure between that particular album and the benchmark Please Please
Me. This word list excludes stop words, of course.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-.r&#34;&gt;cos &amp;lt;- tidy_beatles %&amp;gt;%
  select(album, word, freq)

cos_w &amp;lt;- spread(cos, key = album, value = freq) # This is a matrix where entries are frequencies of words in the the various albums. NAs are replaced by zeros in the next command. 

cos_w[is.na(cos_w)] &amp;lt;- 0

cos_w &amp;lt;- cos_w %&amp;gt;%
  select(-word)

title &amp;lt;- beatles_albums[[1]][1]
a &amp;lt;- cos_w %&amp;gt;% select(as.character(title))


cosines &amp;lt;- data.table(album = character(),
                      cosines = numeric())

for(i in 2:nrow(beatles_albums)){
  title1 &amp;lt;- beatles_albums[[1]][i]
  l &amp;lt;- list(title1)
  b &amp;lt;- cos_w %&amp;gt;% select(as.character(title1))
  l &amp;lt;- list.append(l, round(sum(a*b)/sqrt(sum(a^2)*sum(b^2)),3))
  cosines &amp;lt;- rbind(cosines, l)
}

cosines &amp;lt;- data.frame(cosines)

cosines &amp;lt;- cosines%&amp;gt;%
  arrange(desc(cosines))

cosines$album &amp;lt;- factor(cosines$album, levels = cosines$album[order(cosines$cosines)])

ggplot(cosines) + 
  geom_col(aes(album, cosines, fill=I(&amp;quot;steelblue3&amp;quot;), col=I(&amp;quot;black&amp;quot;)),show.legend = F) +
  theme_bw() + coord_flip() + 
  labs(title = &amp;quot;Cosine similarities with Please Please Me&amp;quot;, y = &amp;quot;cosine similarity&amp;quot;) + 
  theme(plot.title = element_text(size = rel(1.25))) + 
  ylim(0,1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://peterduronelly.github.io/img/02_beatles_02_resize.jpg&#34; alt=&#34;alt text&#34; /&gt;&lt;/p&gt;

&lt;p&gt;It is not surprising that &amp;lsquo;A Hard Day&amp;rsquo;s Night&amp;rsquo; is very similar to Please
Please Me, but Abbey Road (no. 12) also shares a lot with it.
Sgt. Pepper&amp;rsquo;s (no. 8) is an interesting album: it is the most distinct
one amongst the core and, as we will see it later, by some measure it
has more complex lyrics than any of the other LPs. (St. Pepper&amp;rsquo;s was the
first album of the &amp;lsquo;studio years&amp;rsquo;, when the band was finally freed from
the burden of permanent touring. It was an experimental album which took
700 hours to record.)
&lt;br&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;text-complexity&#34;&gt;Text Complexity&lt;/h2&gt;

&lt;p&gt;Next I turned to text complexity. Can we see an arch of change as the
group gets older, grows confidence, and starts to have something else to
say than &amp;lsquo;I love you&amp;rsquo;?&lt;/p&gt;

&lt;p&gt;There are various measures of text complexity, and all aims to assess
the readability of prosaic test. Since these metrics have been developed
to prose, applying them to lyrics, which is basically poetry, is not
straightforward. In songs and poems text serves the melody (rhythm),
and, for the lack of proper punctuation, standard algorithms cannot
detect where sentences start and end. The basis of complexity metrics is
usually the number of words in a sentence, the lengths of the words, and
the ratio of complex words within all words. These measures are focusing
on how the text is built, and they don&amp;rsquo;t filter for stop words, as the
use of these stop words is also a sign of sophistication. These are,
obviously, the simplest measures and they do not account for other
dimensions of complexity, like the frequency with which people use these
words or the types of texts where these words are typical. By these
metrics, for instance, the word &amp;lsquo;championship&amp;rsquo; is way more elaborate
than the word &amp;lsquo;cat&amp;rsquo;, although the context where the former is mostly
used may not be more academic than the one for the latter.&lt;/p&gt;

&lt;p&gt;In this exercise I use the modified version of two complexity measures.
The &amp;lsquo;&lt;strong&gt;Automated Readability Index&lt;/strong&gt;&amp;rsquo; uses characters, words and
sentences so, that&lt;/p&gt;

&lt;p&gt;&lt;div class=text-center&gt;
ARI = 4.71 x (characters/words) + 0.5 x (words/sentences) - 21.43
&lt;/div&gt;
&lt;br&gt;
The &amp;lsquo;&lt;strong&gt;Gunning Fog&lt;/strong&gt;&amp;rsquo; score is based on words, sentences, and the ratio
of complex words:
&lt;br&gt;
&lt;div class=text-center&gt;
Gunning Fog = 0.4 x ( (words/sentences) + 100 x (complex words/words) )
&lt;/div&gt;
&lt;br&gt;
A word is considered to be complex if it has at least 3 syllables.
Higher complexity scores indicate more complicated text.&lt;/p&gt;

&lt;p&gt;Since sentences are loosely defined in lyrics, I replaced them by lines,
despite that sentences can be of multiple lines. Lines are the main
building blocks of song text, so using them as a proxy is a viable
option. As the original formulas are this way &amp;lsquo;modified&amp;rsquo;, I denote these
measures as mARI and mGunningFog. In this form they are probably more
imperfect as they originally are, but they do show meaningful patterns
even in our case.&lt;/p&gt;

&lt;p&gt;In order to calculate these measures for each album I looped through the
album titles and selected distinct lines for analysis. The reason for
using distinct lines is that in songs lines are many times repeated for
the sake of the melody, serving as chorus, and complete verses can be
reused to fill the melody with sufficient amount of text for the vocals.&lt;/p&gt;

&lt;p&gt;For the complexity metrics I used the &amp;lsquo;syllable&amp;rsquo; package, which produces
certain text statistics from which these scores can be calculated. The
result is in the next chart.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-.r&#34;&gt;text_complexity &amp;lt;- data.table(album = character(),
                              mGunningFog = numeric(),
                              mARI = numeric())

for(i in 1:nrow(beatles_albums)){
  name &amp;lt;- beatles_albums[[1]][i]
  l &amp;lt;-  list(name)
  temp_lyr &amp;lt;- beatles_lyrics %&amp;gt;%
    filter(album == as.character(name)) %&amp;gt;%
    distinct(text)
  rwstat &amp;lt;- readability_word_stats(temp_lyr[,1])
  l &amp;lt;-  list.append(l, 0.4*(rwstat$n.words/nrow(temp_lyr)) +
                      100*(rwstat$n.complexes/rwstat$n.words))
  l &amp;lt;- list.append(l, 4.71*(rwstat$n.chars/rwstat$n.words) +
                     0.5*(rwstat$n.words/nrow(temp_lyr)) - 21.43)
  text_complexity &amp;lt;- rbind(text_complexity, l)
}

ggplot(data = text_complexity, aes(mARI, mGunningFog)) + 
  geom_point(color = &amp;quot;darkblue&amp;quot;) +
  geom_text(aes(x = mARI, y = mGunningFog, label = album), hjust=1, vjust=-0.5) + 
  theme_bw() + labs(title = &amp;quot;Text complexity of Beatles albums&amp;quot;) + 
  theme(plot.title = element_text(size = rel(1.25)))  +
  ylim(4,8) + xlim(-3,1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://peterduronelly.github.io/img/02_beatles_03.jpg&#34; alt=&#34;alt text&#34; /&gt;&lt;/p&gt;

&lt;p&gt;While the two metrics put the albums in slightly different orders, the
trend is obvious. The first albums (Please Please Me, A Hard Day&amp;rsquo;s
Night, With The Beatles) are of fairly simple text, but as time goes by
sophistication increases. Let It Be (no. 13) and Abbey Road (no. 12)
have fairly high readings by ARI, and Let It Be is the single most
complex according to Gunning Fox. Note that Sgt. Pepper (no. 8) is of
relatively high complexity: it is the most complex by ARI and the third
most complex by Gunning Fog. Remember, Sgt. Pepper is the most
dissimilar album compared to the Please Please Me benchmark by cosine
similarity.&lt;/p&gt;

&lt;p&gt;If we look at the publication dates of these albums (not shown here) we
do see a development in the group&amp;rsquo;s artistic performance. Four out of
the last five albums (&amp;lsquo;Magical Mystery Tour&amp;rsquo;, &amp;lsquo;The Beatles The White
Album&amp;rsquo;, &amp;lsquo;Abbey Road&amp;rsquo;, &amp;lsquo;Let It Be&amp;rsquo;) are in the top right corner of the
chart, while the first five LPs are in the bottom left.
&lt;br&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;

&lt;p&gt;This short analysis tries to provide an insight into the carrier of The
Beatles through their lyrics. Songs were very simple at the beginning,
but as their music got more mature, their lyrics followed on. Measures
of text complexity, just as those of similarity, can also reveal the
uniqueness of the St. Pepper&amp;rsquo;s album.&lt;/p&gt;

&lt;p&gt;Among the tools, tidytext, syllable, and of course the geniusR packages
were used to compile this report.&lt;/p&gt;

&lt;p&gt;Codes can be found at &lt;a href=&#34;https://github.com/peterduronelly/blogcodes/blob/master/02-Text-Complexity-Analysis-Of-Beatles_Lyrics-With-R.R&#34; target=&#34;_blank&#34;&gt;https://github.com/peterduronelly/blogcodes/blob/master/02-Text-Complexity-Analysis-Of-Beatles_Lyrics-With-R.R&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sparse Matrices in Python</title>
      <link>https://peterduronelly.github.io/post/sparse-matrices-in-python/</link>
      <pubDate>Thu, 13 Sep 2018 11:17:13 +0200</pubDate>
      
      <guid>https://peterduronelly.github.io/post/sparse-matrices-in-python/</guid>
      <description>&lt;p&gt;One of the things we need to manage in data analysis is recources. When we have large amounts of (&amp;lsquo;big&amp;rsquo;) data this can become a serious issue.
One of the cases when we need to consider whether we really need all the data we have is when we have a lot of zeros in our database, and these zeroes
happen to be irrelevant for our calculations. Python&amp;rsquo;s &lt;em&gt;SciPy&lt;/em&gt; library has a solution to store and handle &lt;strong&gt;&lt;em&gt;sparse&lt;/em&gt;&lt;/strong&gt; data matrices which contain a
large number of irrelevant zero values.&lt;/p&gt;

&lt;p&gt;In order to demonstrate how this works let&amp;rsquo;s first import the necessary packages.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
from scipy.sparse import coo_matrix
from scipy.sparse import save_npz
import matplotlib
import matplotlib.pyplot as plt
%matplotlib inline
import random
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Where would we come across sparse data? Let&amp;rsquo;s imagine you are running an e-commerce site where people buy various products. Some poeple buy only one item, others buy multiple products. You want to see how clients are related, or linked, to each other based on their purchase patterns: if some customers buy the same or almost the same set of products, these clients are similar.&lt;/p&gt;

&lt;p&gt;Moreover, these linkages create a network clients, and this network can be used for recommendations, coupons, etc. Without going into the details how such a network can be built and used let&amp;rsquo;s see how we get to sparse data when we analyze buying patterns.&lt;/p&gt;

&lt;p&gt;For the sake of simplicity let&amp;rsquo;s assume that there are 15 clients and 30 products. Each client buys between 2 and 8 pieces of items.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;clients = list(range(0,15)) # 15 clients
dic = {} # a dictionary of purchases: keys are clients and values are lists of items bought
for i in range(0,15):
    key = clients[i]
    random.seed(i)
    l = random.sample(range(0,30), random.randint(2,8))
    dic[key] = l
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can print out what itemms clients have purchased.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for key in dic.keys():
    print(dic[key])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;[12, 24, 13, 1, 8, 16, 15, 29]&lt;br&gt;
[18, 27, 25]&lt;br&gt;
[27, 1, 2, 29, 11, 5, 23, 21]&lt;br&gt;
[18, 17, 4]&lt;br&gt;
[9, 3, 23]&lt;br&gt;
[8, 23, 11, 25, 22, 28]&lt;br&gt;
[18, 26, 2, 15, 24, 8, 1, 0]&lt;br&gt;
[4, 12, 20, 1]&lt;br&gt;
[11, 12, 4]&lt;br&gt;
[19, 11, 8, 4, 5]&lt;br&gt;
[1, 13, 15, 18, 0, 6]&lt;br&gt;
[27, 17, 29, 24, 14]&lt;br&gt;
[8, 21, 16, 11, 4]&lt;br&gt;
[9, 21, 29, 25]&lt;br&gt;
[19, 22]&lt;br&gt;&lt;/p&gt;

&lt;p&gt;Clients No. 0, 2 and 6 have bought eight items, and only one has bought two. Now we can build a matrix of clients where the matrix elements show the
number of common items on their shopping history. Each  a&lt;sub&gt;ij&lt;/sub&gt;  element of this matrix tells the number of items bought both by client &lt;em&gt;i&lt;/em&gt;
and client &lt;em&gt;j&lt;/em&gt; . The  i&lt;sup&gt;th&lt;/sup&gt;  diagonal element shows the number of items bought buy client &lt;em&gt;i&lt;/em&gt;. This is going to be a symmetric matrix,
of course.&lt;/p&gt;

&lt;p&gt;To build the matrix we need to define the row and column indices of the non-zero matrix elements, and the values of the elements. These are going to be
lists of equal sizes, which serve as inputs for the sparse matrix. Since the matrix is symmetric, we don&amp;rsquo;t need to calculate and store the lower
diagonal elements to save space.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;r = 0 # the index of the row of the matrix
c = 0 # the index of the columns of the matrix
counter = 0
row_indices = [] # row indices of the non-zero values
column_indices = []  # column indices of the non-zero values
matrix_elements = [] # the non-zero values themselves

for key_r in dic.keys(): # key_r is the key for the rows
    x = dic[key_r]
    for key_c in dic.keys(): # key_c is the key for the columns
        if c &amp;gt;= r:
            y = dic[key_c]
            common_set = list(set(x) &amp;amp; set(y))
            common_set_size = len(common_set)
            if common_set_size &amp;gt; 0:
                row_indices.append(r)
                column_indices.append(c)
                matrix_elements.append(common_set_size)
        c = c + 1
    r = r + 1
    c = 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once we have the lists we can build the sparse matrix using the &lt;em&gt;coo_matrix&lt;/em&gt; function. This function takes numpy arrays as inputs so we need to
convert our lists to arrays. When building the matrix we need to add the non-zero elements first, then the indices as a tuple. Finally we need to
define the size (shape) of the matrix. The resulting object will be of type &lt;em&gt;scipy.sparse.coo.coo_matrix&lt;/em&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;row  = np.array(row_indices)
col  = np.array(column_indices)
val = np.array(matrix_elements)
mx = coo_matrix((val, (row, col)), shape=(15, 15))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can view the content of the matrix using the .toarray() command.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mx.toarray()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;array([[8, 0, 2, 0, 0, 1, 4, 2, 1, 1, 3, 2, 2, 1, 0],&lt;br&gt;
    &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;   [0, 3, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0],&lt;br&gt;
       &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; [0, 0, 8, 0, 1, 2, 2, 1, 1, 2, 1, 2, 2, 2, 0],&lt;br&gt;
       &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; [0, 0, 0, 3, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0],&lt;br&gt;
       &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; [0, 0, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0],&lt;br&gt;
       &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; [0, 0, 0, 0, 0, 6, 1, 0, 1, 2, 0, 0, 2, 1, 1],&lt;br&gt;
       &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; [0, 0, 0, 0, 0, 0, 8, 1, 0, 1, 4, 1, 1, 0, 0],&lt;br&gt;
       &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; [0, 0, 0, 0, 0, 0, 0, 4, 2, 1, 1, 0, 1, 0, 0],&lt;br&gt;
       &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; [0, 0, 0, 0, 0, 0, 0, 0, 3, 2, 0, 0, 2, 0, 0],&lt;br&gt;
       &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; [0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 3, 0, 1],&lt;br&gt;
       &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0],&lt;br&gt;
       &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 1, 0],&lt;br&gt;
       &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 1, 0],&lt;br&gt;
       &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0],&lt;br&gt;
       &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]])&lt;/p&gt;

&lt;p&gt;We can also visualize the matrix using the matplotlib library. This visualization shows where the non-zero values are located in the matrix,
but it does not make a difference based on their values. All non-zero entries are indicated by the same color.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.spy(mx)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://peterduronelly.github.io/img/spars.png&#34; alt=&#34;alternative text for search engines&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The chart has a slighlty different outlook if we use the matrix as a numpy array, and not as scipy.sparse.coo.coo_matrix object as plot input.
The chart is built a little faster and has a different coloring scheme. The visualization, however, does not help much when we have really large
amount of data, with thousands of rows and columns in our matrix: the screen resolution will not be able to differentiate between the white and
non-white areas. This makes this visulization tool kind of useless in those cases when we really need to use the sparse matrix fuctionality for our resource
management.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mx_as_array = mx.toarray()
plt.spy(mx_as_array)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://peterduronelly.github.io/img/spars_2.png&#34; alt=&#34;alt text&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Finally we can save the scipy.sparse.coo.coo_matrix object using the &lt;em&gt;save_npz()&lt;/em&gt; command. For later use, the load_npz() command imports the
sparse matrix to our project.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;save_npz(&#39;/path...&#39;, mx)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The codes can be find at &lt;a href=&#34;https://github.com/peterduronelly/blogcodes/blob/master/01-Sparse-Matrices-in-Python.ipynb&#34; target=&#34;_blank&#34;&gt;my github&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introduction</title>
      <link>https://peterduronelly.github.io/post/test/</link>
      <pubDate>Mon, 10 Sep 2018 11:55:58 +0200</pubDate>
      
      <guid>https://peterduronelly.github.io/post/test/</guid>
      <description>

&lt;h2 id=&#34;who-am-i&#34;&gt;Who Am I?&lt;/h2&gt;

&lt;div class=text-justify&gt;
I am originally trainded as an economist with a Finance major and a Statistics minor. I spent 21 years on financial markets as an analyst and asset manager. 
After a while, though, I got interested in disruptive technologies, mostly data and data science, which are closely linked to my previous academic studies. 
Statistics is especially close to my heart as I used to teach it to sophomore students on the Budapest University of Economics (currently Corvinus University).

To do something about my new passion, I quit my job in 2017 and completed a data science-focused master&#39;s program on the Central European University in Budapest (MSc in Business 
Analytics). The core of the program was Statistics, Machine Learning, some intro to neural networks, and data infrastructure in the cloud. The program was 
built in R and I also learnt Python (which I like better than R). 


&lt;/div&gt;

&lt;h2 id=&#34;why-do-i-write-a-blog&#34;&gt;Why Do I Write a Blog?&lt;/h2&gt;

&lt;div class=text-justify&gt;
I have encountered a lot of interesting things during my studies. I had superinteresting assignments for which I had to dig out a bunch of fancy solutions
in the docs, on stackoverflow, and from other people&#39;s blogs. This was when I decided to add some more  to the already large and rapidly expanding content
space on the net, so that I may be able to give some help to those who are looking for those things I was struggling with. Also, some of the things I did
was about really cool topics (like text complexity analysis, or clustering on a dissimilarity matrix) and I think they are worth to be shared. Finally,
there were some issues which took me months to solve and I hope these posts will shorten this time for those who find my posts during their search. 
&lt;/div&gt;

&lt;div class=text-justify&gt;
All blog codes will be on github in their entirety, under https://github.com/peterduronelly/blogcodes. 

&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
