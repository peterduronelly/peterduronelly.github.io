<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tiny Little Things in Data Science on Tiny Little Things in Data Science</title>
    <link>https://peterduronelly.github.io/</link>
    <description>Recent content in Tiny Little Things in Data Science on Tiny Little Things in Data Science</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 +0200</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Train Test Splitting Time Series Data</title>
      <link>https://peterduronelly.github.io/post/train-test-splitting-time-series-data/</link>
      <pubDate>Tue, 20 Nov 2018 15:45:04 +0100</pubDate>
      
      <guid>https://peterduronelly.github.io/post/train-test-splitting-time-series-data/</guid>
      <description>&lt;p&gt;Machine learning is not the ideal tool for time series forecasting for a number of reasons, but, as I will demonstrate it in a future post, limited models can be built for short-term forecasting exercises. One aspect of time series data is, however, that you can’t split your observations randomly into train and test subsets: you train on an early interval and test on a later one. Standard ML libraries, such as scikit-learn, don’t provide a tool for that. This post will show how we can split time series data for a machine learning model.&lt;/p&gt;

&lt;p&gt;Splitting data between training and testing for time series is different from cross-sectional data as the underlying population is not a set of different individual units but the same observable thing(s) in different time periods. The model is trained on an early part of the observed period and we want to test it on the more recent interval. This assumes a few things, first and foremost the persistence of the patterns over time, but this is not only critical for ML modeling. Standard time series analysis methods, let them be stochastic or non-stochastic, also assume this kind of persistence.&lt;/p&gt;

&lt;p&gt;I am planning a longer post on demonstrating the viability of ML in time series predictions, but before that I’d like to show a simple solution to the train-test-split problem when the split cannot happen randomly. I am using a set of macroeconomic variables to forecast the development of Case-Shiller Home Price Index, where I also want to illustrate the importance of domain knowledge in the modeling exercise.&lt;/p&gt;

&lt;p&gt;The data is coming from &lt;a href=&#34;https://fred.stlouisfed.org/&#34; target=&#34;_blank&#34;&gt;FRED&lt;/a&gt;, the St. Louis Fed Economic Database in a csv file. Since feature engineering is best done in pandas, the input object for the train-tests-split method is also a pandas data frame. The method’s extra output beyond the subsets is the list of feature names.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import pandas as pd
import datetime

hp = pd.read_csv(&amp;quot;house_price_changes.csv&amp;quot;)
hp.head()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://peterduronelly.github.io/img/07_train_test_split_01.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The first column should be the date column, but the csv downloaded from FRED does not have either the proper name or the proper format. Looking at the schema tells us that we have 32 variables, 119 periods and our data starts with an unnamed ‘non-null object’, which should be the date column.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;hp.info()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://peterduronelly.github.io/img/07_train_test_split_02.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Using the datatime.date() function we can transform the uninterpretable mess into a neat date variable.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;hp.rename(columns={&#39;Unnamed: 0&#39;: &#39;date&#39;}, inplace=True)
hp[&#39;date&#39;] = pd.to_datetime(hp[&#39;date&#39;])
hp.info()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://peterduronelly.github.io/img/07_train_test_split_06.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The easiest way to split a dataframe by column values is using a mask. For instance if we want to split our data for dates before and after (not before) January 1st, 2013 we can simply say:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mask = hp[&#39;date&#39;] &amp;lt; datetime.date(2013,1,1)
hp_train = hp[mask]
hp_test = hp[-mask]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To get things organized we’d better write a function to train-test-split our dataframe and to get the feature names accordingly. We need the following steps:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Split to train and test.&lt;/li&gt;
&lt;li&gt;Drop the date column if it is not a feature in the model.&lt;/li&gt;
&lt;li&gt;Split both train and test to X (features) and y (target).&lt;/li&gt;
&lt;li&gt;Get the list of column names.&lt;/li&gt;
&lt;li&gt;Convert the two X and y dataframes to arrays.&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def train_test_split_timeseries(input_dataframe, target, timecolumn, year, month, day, dropdates = True):
    &amp;quot;&amp;quot;&amp;quot;
    The function splits a dataframe containing a time series into non-random train and test subsets. 
    The last observation in the train data is the latest datetime value in the data which precedes 
    the breakpoint given by the (year, month, day) value. The first observation in the test data is the 
    breakpoint given by the (year, month, day) value or the first observation afterwards.
    
    Parameters:
        input_dataframe (Pandas dataframe): The data file with the time series data.
        target (string): Name of the target variable in the input dataframe.
        timecolumn (string): The name of the time colummn for splitting the dataframe (usually a date column).
        year, month, day (int): The year, month, day components of the breakpoint.
        dropdates (boolean): Whether or not to drop the date column to produce the train/test data. Defaults to True. 
    
    Returns: 
        X_train (array): A numpy array of training input data.
        y_train (array): A numpy array of training target data.
        X_test (array): A numpy array of test input data.
        y_test (array): A numpy array of test target data.
        feature_names (array): A list of feature names used in the input matrix.
    &amp;quot;&amp;quot;&amp;quot;
    
    # Split to train and test periods.
    model_df = input_dataframe
    target = target
    timecolumn = timecolumn
    mask = model_df[timecolumn] &amp;lt; datetime.date(year,month,day)
    model_df_train = model_df[mask]
    model_df_test = model_df[-mask]
    
    # Drop date column if dropdates = True
    if dropdates:
        model_df_train = model_df_train.drop([&#39;date&#39;], axis=1)
        model_df_test = model_df_test.drop([&#39;date&#39;], axis=1)
    
    # Split both train and test to X (input) and y (target)
    X_train = model_df_train.drop([target], axis=1)
    y_train = model_df_train[target]
    
    X_test = model_df_test.drop([target], axis=1)
    y_test = model_df_test[target]
    
    # Get column names for variable importance
    feature_names = list(X_train)
    
    # Convert X_train, X_test, y_train, y_test to numpy arrays
    X_train = X_train.as_matrix()
    X_test = X_test.as_matrix()
    y_train = y_train.as_matrix()
    y_test = y_test.as_matrix()
    
    
    return X_train, y_train, X_test, y_test, feature_names
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can now put the function in use. In this particular exercise the target variable is the quarterly change of the Case Shiller Home Price Index (&amp;lsquo;Case_Shiller_HPI_chg&amp;rsquo;), the last period in the train set is Q4 2012, and the first period in the test set is Q1 2013.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X_train, y_train, X_test, y_test, feature_names = train_test_split_timeseries(hp, &#39;Case_Shiller_HPI_chg&#39;, &#39;date&#39;, 2013, 1,1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When applying the function we get the following output objects.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(&#39;Feature_names&#39;)
print(feature_names)
print()
print(&amp;quot;X_train type:&amp;quot;, type(X_train), &amp;quot;, shape:&amp;quot;, X_train.shape)
print(&amp;quot;y_train type:&amp;quot;, type(y_train), &amp;quot;, shape:&amp;quot;, y_train.shape)
print(&amp;quot;X_test type:&amp;quot;, type(X_test), &amp;quot;, shape:&amp;quot;, X_test.shape)
print(&amp;quot;y_test type:&amp;quot;, type(y_test), &amp;quot;, shape:&amp;quot;, y_test.shape)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://peterduronelly.github.io/img/07_train_test_split_05.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Now we have our train and test subsets for time series forecasting. The model will be trained on the first interval. The patterns learned on this interval will then be projected to the second interval to test the model. This is what I will show in the next post.&lt;/p&gt;

&lt;p&gt;Codes are at the &lt;a href=&#34;https://github.com/peterduronelly/blogcodes/blob/master/07_Train_Test_Splitting_Time_Series_Data.ipynb&#34; target=&#34;_blank&#34;&gt;usual place&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Comparing Beatles and Bob Dylan</title>
      <link>https://peterduronelly.github.io/post/comparing-beatles-and-bob-dylan/</link>
      <pubDate>Tue, 09 Oct 2018 14:35:15 +0200</pubDate>
      
      <guid>https://peterduronelly.github.io/post/comparing-beatles-and-bob-dylan/</guid>
      <description>&lt;p&gt;In a &lt;a href=&#34;https://peterduronelly.github.io/post/text-complexity-analysis-of-beatles-lyrics-with-r/&#34; target=&#34;_blank&#34;&gt;previous post&lt;/a&gt; I showed how to use data science tools to find hidden features in unstructured text and analyzed how the complexity of the lyrics of Beatles songs changed over time. In this post I do a little follow-up and compare complete works of The Beatles with that of two others using the same methodology and metrics. Comparing Beatles with other musicians may help put the original numbers into the perspective.&lt;/p&gt;

&lt;p&gt;I downloaded lyrics from the progressive rock band ‘The Alan Parsons Project’ and Nobel laureate Bob Dylan. Alan Parsons is an audio engineer, musician and song writer, who was one of the engineering architects of The Beatles’ Abbey Road and Let It Be albums, and Pink Floyd’s Dark Side Of The Moon LP. He later created an unusual formation consisting of him and his composer partner Eric Wolfson as permanent group members, supplemented by a group of session musicians who played on their albums with more or less regularity. Their music was of a more intellectual nature with a more equal focus on lyrics and melody.&lt;/p&gt;

&lt;p&gt;Bob Dylan can now be equally considered both as a poet and a musician, and comparing him and The Alan Parson Project to The Beatles makes an interesting text analysis. As a side note: I was also entertaining the idea to include Iron Maiden in the comparison, but the excessive work of Bruce Dickinson &amp;amp; Co would have made the already long download even longer, so I stayed with my original idea. (It takes quite a bit of time to download lyrics with geniusR.)&lt;/p&gt;

&lt;p&gt;First, I did the cosine similarity comparison, but this time on the artists and not on the individual albums. According to the cosine matrix Bob Dylan is equally different from The Beatles and The Alan Parsons Project: their cosine is 0.67 and 0.62, respectively. In a two-dimensional space this would be an angle of approximately 50 degrees. The cosine between Beatles and The Alan Parsons Project, however, is only 0.41, which is ‘equivalent’ to a two-dimensional angle of 65 degrees. It looks, that, despite their partially shared history, The Beatles and Alan Parsons have less in common than any of them with Bob Dylan.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-.r&#34;&gt;albums &amp;lt;-  tibble(
  artist = c(
    rep(&amp;quot;The Beatles&amp;quot;, 13),
    rep(&amp;quot;Bob Dylan&amp;quot;, 36),
    rep(&amp;quot;The Alan Parsons Project&amp;quot;, 10)
  )
    ,
  album = c(
    &amp;quot;Please Please Me&amp;quot;, &amp;quot;With The Beatles&amp;quot;, &amp;quot;A Hard Day s Night&amp;quot;,
    &amp;quot;Beatles For Sale&amp;quot;, &amp;quot;Help&amp;quot;, &amp;quot;Rubber Soul&amp;quot;,
    &amp;quot;Revolver&amp;quot;, &amp;quot;Sgt Pepper s Lonely Hearts Club Band&amp;quot;, &amp;quot;Magical Mystery Tour&amp;quot;,
    &amp;quot;The Beatles The White Album&amp;quot;, &amp;quot;Yellow Submarine&amp;quot;, &amp;quot;Abbey Road&amp;quot;,
    &amp;quot;Let It Be&amp;quot;,
    &amp;quot;Bob dylan&amp;quot;, &amp;quot;The freewheelin bob dylan&amp;quot;, &amp;quot;Another side of bob dylan&amp;quot;,
    &amp;quot;The times they are a changin&amp;quot;, &amp;quot;Bringing it all back home&amp;quot;, &amp;quot;Highway 61 revisited&amp;quot;,
    &amp;quot;Blonde on blonde&amp;quot;, &amp;quot;John wesley harding&amp;quot;, &amp;quot;Nashville skyline&amp;quot;,
    &amp;quot;New morning&amp;quot;, &amp;quot;Self portrait&amp;quot;, &amp;quot;Pat garrett billy the kid&amp;quot;,
    &amp;quot;Triplicate&amp;quot;, &amp;quot;Blood on the tracks&amp;quot;, &amp;quot;The basement tapes&amp;quot;,
    &amp;quot;Desire&amp;quot;, &amp;quot;Street legal&amp;quot;, &amp;quot;Slow train coming&amp;quot;,
    &amp;quot;Saved&amp;quot;, &amp;quot;Shot of love&amp;quot;, &amp;quot;Infidels&amp;quot;,
    &amp;quot;Empire burlesque&amp;quot;, &amp;quot;Knocked out loaded&amp;quot;, &amp;quot;Down in the groove&amp;quot;,
    &amp;quot;Oh mercy&amp;quot;, &amp;quot;Under the red sky&amp;quot;, &amp;quot;Good as i been to you&amp;quot;,
    &amp;quot;World gone wrong&amp;quot;, &amp;quot;Time out of mind&amp;quot;, &amp;quot;Love and theft&amp;quot;,
    &amp;quot;Modern times&amp;quot;, &amp;quot;Together through life&amp;quot;, &amp;quot;Christmas in the heart&amp;quot;,
    &amp;quot;Tempest&amp;quot;, &amp;quot;Shadows in the night&amp;quot;, &amp;quot;Fallen angels&amp;quot;,
    &amp;quot;Tales of mystery and imagination edgar allan poe&amp;quot;, &amp;quot;I robot&amp;quot;, &amp;quot;Pyramid&amp;quot;,
    &amp;quot;Eve&amp;quot;, &amp;quot;The turn of a friendly card&amp;quot;, &amp;quot;Eye in the sky&amp;quot;, 
    &amp;quot;Ammonia avenue&amp;quot;, &amp;quot;Vulture culture&amp;quot;, &amp;quot;Stereotomy&amp;quot;,
    &amp;quot;Gaudi&amp;quot;
  )
)

all_lyrics &amp;lt;- album_lyrics &amp;lt;- albums %&amp;gt;% 
  mutate(tracks = map2(artist, album, genius_album))

full_lyrics &amp;lt;- all_lyrics %&amp;gt;% 
  unnest(tracks) %&amp;gt;%
  arrange(desc(artist))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-.r&#34;&gt;tidy_lyrics &amp;lt;- full_lyrics %&amp;gt;%
  unnest_tokens(word, text) %&amp;gt;%
  filter(nchar(word)&amp;gt;2)%&amp;gt;%
  anti_join(stop_words) %&amp;gt;%
  group_by(artist) %&amp;gt;%
  count(word, sort = TRUE) %&amp;gt;%
  ungroup()

total_full_words &amp;lt;- tidy_lyrics %&amp;gt;% 
  group_by(artist) %&amp;gt;% 
  summarize(total = sum(n))

tidy_lyrics &amp;lt;- left_join(tidy_lyrics, total_full_words)

tidy_lyrics &amp;lt;- tidy_lyrics %&amp;gt;%
  mutate(freq = n / total)

full_cos &amp;lt;- tidy_lyrics %&amp;gt;%
  select(artist, word, freq)

full_cos_w &amp;lt;- spread(full_cos, key = artist, value = freq)

full_cos_w[is.na(full_cos_w)] &amp;lt;- 0

full_cos_w_matrix &amp;lt;- data.matrix(full_cos_w, rownames.force = NA)

full_cos_w_matrix &amp;lt;- full_cos_w_matrix[, -1]

cosine_matrix &amp;lt;- cosine(full_cos_w_matrix)

cm &amp;lt;- data.frame(cosine_matrix)

pander(cm, caption = &amp;quot;Cosine similarity matrix&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://peterduronelly.github.io/img/06_beatles_dylan_01.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Next, I turned to the text complexity measures. Here I used the core album lists, which had 10 albums for The Alan Parsons Project and 36 (!) for Bob Dylan. Just as in the previous case, stop words are also included in the calculations.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-.r&#34;&gt;artists &amp;lt;- full_lyrics %&amp;gt;%
  distinct(artist)
  
full_text_complexity &amp;lt;- data.table(artist = character(),
                              mGunningFog = numeric(),
                              mARI = numeric())

for(i in 1:nrow(artists)){
  artista &amp;lt;- artists[[1]][i]
  l &amp;lt;-  list(artista)
  temp_lyr &amp;lt;- full_lyrics %&amp;gt;%
    filter(artist == artista) %&amp;gt;%
    distinct(text)
  rwstat &amp;lt;- readability_word_stats(temp_lyr[,1])
  l &amp;lt;-  list.append(l, 0.4*(rwstat$n.words/nrow(temp_lyr)) +
                      100*(rwstat$n.complexes/rwstat$n.words))
  l &amp;lt;- list.append(l, 5.89*(rwstat$n.chars/rwstat$n.words) -
                     0.3*(nrow(temp_lyr)/rwstat$n.words) - 15.8)
  full_text_complexity &amp;lt;- rbind(full_text_complexity, l)
}

ggplot(data = full_text_complexity, aes(mARI, mGunningFog)) + 
  geom_point(color = &amp;quot;darkblue&amp;quot;) +
  geom_text(aes(x = mARI, y = mGunningFog, label = artist), hjust=1, vjust=-0.5) + 
  theme_bw() + labs(title = &amp;quot;Text complexity comparison&amp;quot;) + 
  theme(plot.title = element_text(size = rel(1.25)))  +
  xlim(5.5,7.5) + ylim(5,7)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://peterduronelly.github.io/img/06_beatles_dylan_02.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Beatles fares quite poorly compared to both Bob Dylan and The Alan Parsons Project: the group&amp;rsquo;s lyrics is less complex &lt;strong&gt;on average&lt;/strong&gt; than that of the other two musicians. But if we go back to the previous post, we&amp;rsquo;ll see that on an individual album basis the Beatles has nothing to feel bad about. Their late albums (Abbey Road, Let It Be and Sgt. Pepper) meet Bob Dylan or Alan Parsons standards.&lt;/p&gt;

&lt;p&gt;This does not imply that Paul McCartney should have been given the Nobel Prize as these complexity metrics are only one kind of the many measures of text quality. Nevertheless, it is still interesting to see the numbers and it is a lot of fun to put together an analysis of this kind.&lt;/p&gt;

&lt;p&gt;Codes are at the &lt;a href=&#34;https://github.com/peterduronelly/blogcodes/blob/master/06-Comparing-Beatles-And-Bob-Dylan.R&#34; target=&#34;_blank&#34;&gt;usual place&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Robust and Clustered Standard Errors in Stargazer</title>
      <link>https://peterduronelly.github.io/post/robust-and-clustered-standard-errors-in-stargazer/</link>
      <pubDate>Tue, 02 Oct 2018 14:14:34 +0200</pubDate>
      
      <guid>https://peterduronelly.github.io/post/robust-and-clustered-standard-errors-in-stargazer/</guid>
      <description>

&lt;p&gt;Stargazer is a neat tool to present model estimates. It accepts a fairly large number of object-types and creates nice-looking, ready-to-publish outputs of their main parameters. In many cases, however, the default settings do not give us the proper numerical results, and customizing the output is not that straightforward. This is part one in a two-part series on how to customize stargazer.&lt;/p&gt;

&lt;p&gt;When I first encountered stargazer I already had a problem with the model outputs the package created: in cross-sectional data the observations are often of different sizes, which leads to heteroskedastic model residuals where simple standard errors are useless for measuring variable significance. Heteroskedasticity requires &lt;em&gt;‘robust’&lt;/em&gt; standard errors to calculate p-values, but there is no flag in stargazer to switch from simple to robust standard errors. The same problem emerges with panel models, where, for basically the same reason, &lt;em&gt;‘clustered’&lt;/em&gt; standard errors need to be calculated and applied. If the output is based on the wrong errors, then the model cannot be presented with stargazer. However, I haven’t been able to find any other package which can create such a tidily formatted model output.&lt;/p&gt;

&lt;p&gt;The good news is that stargazer can be fed externally with the right standard errors, which then results in the proper output. This post shows you how.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-.r&#34;&gt;library(wbstats) # to retrieve World Bank development data to demonstrate stargazer functionalities
library(dplyr)
library(data.table)
library(plm)
library(stargazer) # this is the package what we are interested in
library(sandwich) # to calculate robust and clustered stanard errors
library(pander)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I am using the &lt;em&gt;wbstats&lt;/em&gt; package to download data from the World Bank development database. As countries are of different sizes, running a regression of any kind requires ‘non-standard’ standard errors to calculate variable significance. We will do a cross-section and a panel regression to demonstrate stargazer options.&lt;/p&gt;

&lt;p&gt;In a simple model I try to explain the number of patent applications in a country with real PPP GDP and population size. The goal of this exercise is not about building the right model, the regressions are for demonstrational purposes only.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-.r&#34;&gt;data &amp;lt;- wb(indicator = c(&amp;quot;NY.GDP.MKTP.PP.KD&amp;quot;, # WB (World Bank) code for PPP GDP per capita in 2011 $
                         &amp;quot;IP.PAT.RESD&amp;quot;, # WB code for patent applications by residents
                         &amp;quot;SP.POP.TOTL&amp;quot;), # WB code for population
                       startdate = 2000, 
                       enddate = 2015)

countries &amp;lt;- wbcountries() 

data &amp;lt;- merge(data, countries[c(&amp;quot;iso2c&amp;quot;, &amp;quot;region&amp;quot;)], 
                          by = &amp;quot;iso2c&amp;quot;, all.x = TRUE)
data &amp;lt;- subset(subset(data, region != &amp;quot;Aggregates&amp;quot;))

data$indicatorID[data$indicatorID == &amp;quot;NY.GDP.MKTP.PP.KD&amp;quot;] &amp;lt;- &amp;quot;GDP&amp;quot;
data$indicatorID[data$indicatorID == &amp;quot;IP.PAT.RESD&amp;quot;] &amp;lt;- &amp;quot;patent_applications&amp;quot;
data$indicatorID[data$indicatorID == &amp;quot;SP.POP.TOTL&amp;quot;] &amp;lt;- &amp;quot;population&amp;quot;

data &amp;lt;- dcast(data, iso2c + country + date + region ~ indicatorID,  value.var = &#39;value&#39;)

names(data)[names(data) == &amp;quot;date&amp;quot;] &amp;lt;- &amp;quot;year&amp;quot;
data$year &amp;lt;- as.numeric(data$year)
data &amp;lt;- data %&amp;gt;%
  select(-iso2c, -region)

data$population &amp;lt;- data$population / 10^6 # some rescaling
data$GDP &amp;lt;- data$GDP / 10^9

data &amp;lt;- data[complete.cases(data),]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Without going into the details of the wbstat package we end up with a data frame where country and year are in columns 1 and 2, and the three other variables (GDP in USD bn, patent applications and population in millions) are in columns 3-5. (This is the default format for panels in R.) The panel is only slightly unbalanced: even if a large number of observations is missing the observations with missing values will not make much trouble. (The punbalanced() command measures how unbalanced the panel is, but this is also beyond the scope of this analysis.)&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;robust-standard-errors&#34;&gt;Robust Standard Errors&lt;/h2&gt;

&lt;p&gt;Now let’s do a simple cross sectional regression for 2011.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-.r&#34;&gt;linear_regression &amp;lt;- lm(formula = patent_applications ~ 
                          GDP + population,
                        data = data %&amp;gt;% filter(year == 2011))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now comes the trick! The vcov() function in the sandwich package calculates the robust standard errors that we need for coefficient testing when model residuals are heteroskedastic. Vcov gives us the variable covariance matrix, which measures co-dependencies between the variables. It&amp;rsquo;s diagonal elements are the variances of the variables, which are used for variable importance testing. We need to extract these diagonal elements, take their square root, and feed them into &lt;em&gt;stargazer&lt;/em&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-.r&#34;&gt;robust_standard_errors &amp;lt;- vcov(linear_regression, sandwich)
robust_standard_errors &amp;lt;- sqrt(diag(robust_standard_errors))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Our standard erorrs are the following.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-.r&#34;&gt;df = data.frame(robust_standard_errors)
pander(df)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://peterduronelly.github.io/img/05_stargazer_01_resized.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This is the input We can feed into &lt;em&gt;stargazer&lt;/em&gt; using the &lt;em&gt;‘se’&lt;/em&gt; option from the method’s parameter set.
It is important to note that the object containing the errors should be given as a &lt;em&gt;‘list’&lt;/em&gt;, even if
we only show one model with stargazer.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-.r&#34;&gt;stargazer(linear_regression, title = &amp;quot;Linear regression&amp;quot;, 
          se = list(robust_standard_errors),
          type = &amp;quot;html&amp;quot;, out = &amp;quot;linear regression.html&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;em&gt;&amp;lsquo;out&amp;rsquo;&lt;/em&gt; parameter in &lt;em&gt;stargazer&lt;/em&gt; defines the name of the output file we create, and &lt;em&gt;&amp;lsquo;type&amp;rsquo;&lt;/em&gt; defines the file&amp;rsquo;s extension.
Once we run the command we will get the following (kind of) neatly formatted output which includes our externally fed standard
errors and the relevant t-statistics based on those standard errors.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://peterduronelly.github.io/img/05_stargazer_02_resized.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;clustered-standard-errors&#34;&gt;Clustered Standard Errors&lt;/h2&gt;

&lt;p&gt;Now let’s do two panel regressions using the whole dataset: a fixed-effects (FE) model and a
first difference (FD) model. We will include both models in our stargazer output for which
we need to supply two sets of standard errors. What is robust standard errors in cross sectional
regressions is ‘clustered’ standard errors in panels. This time the vcovHC() function will take
care of the standard errors.&lt;/p&gt;

&lt;p&gt;First the two panels. As I said before, model validity is not an issue here, I just want to have
standard errors.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-.r&#34;&gt;p_data = pdata.frame(data) # we need to create a &#39;panel data frame&#39; for panel regressions

fe_regression &amp;lt;- plm(patent_applications ~ GDP + population, # FE model
                     data = p_data,
                     model = &amp;quot;within&amp;quot;,
                     effect = &amp;quot;twoways&amp;quot;)

fd_regression &amp;lt;- plm(diff(patent_applications) ~ diff(GDP, lag = 2) + # FD model
                       diff(diff(GDP), lag = c(0:1)) +
                       diff(population, lag = 2) + diff(diff(population), lag = c(0:1)),
                     data = p_data,
                     model = &#39;pooling&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;From these two models we extract the clustered standard errors the same way, but now with the
vcocHC() function which handles clustered errors in panel models.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-.r&#34;&gt;clustered_standard_errors_fe &amp;lt;- vcovHC(fe_regression, type = &amp;quot;HC0&amp;quot;, cluster = &amp;quot;group&amp;quot;)
clustered_standard_errors_fe &amp;lt;- sqrt(diag(clustered_standard_errors_fe))

clustered_standard_errors_fd &amp;lt;- vcovHC(fd_regression, type = &amp;quot;HC0&amp;quot;, cluster = &amp;quot;group&amp;quot;)
clustered_standard_errors_fd &amp;lt;- sqrt(diag(clustered_standard_errors_fd))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once we have the standard errors ready, we add them as stargazer parameters. Note, that they are given
as lists again.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-.r&#34;&gt;stargazer(fe_regression, fd_regression, title = &amp;quot;Panel regressions&amp;quot;, 
          se = list(clustered_standard_errors_fe, clustered_standard_errors_fd),
          type = &amp;quot;html&amp;quot;, out = &amp;quot;panel regression.html&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now the &lt;em&gt;stargazer&lt;/em&gt; output looks like this.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://peterduronelly.github.io/img/05_stargazer_03_resized.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We have everything we need in our model output, and &lt;em&gt;stargazer&lt;/em&gt; uses the proper standard
error set for both models to coefficient testing. We even have more than we really need,
and the labels are kind of messy. There are more options to customize &lt;em&gt;stargazer&lt;/em&gt; and make
it look better, but it is the subject of the other post on &lt;em&gt;stargazer&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Codes at the &lt;a href=&#34;https://github.com/peterduronelly/blogcodes/blob/master/05-Robust-And-Clustered-Standard-Errors-In-Stargazer.R&#34; target=&#34;_blank&#34;&gt;usual place&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bootstrap Samples</title>
      <link>https://peterduronelly.github.io/post/bootstrap-samples/</link>
      <pubDate>Sun, 23 Sep 2018 20:28:32 +0200</pubDate>
      
      <guid>https://peterduronelly.github.io/post/bootstrap-samples/</guid>
      <description>&lt;p&gt;Bootstrap sampling is a widely used method in machine learning and in statistics.
The main idea is that we try to decrease overfitting and the chance of myopic tree-building
if run our algorithm multiple times using the same data, but always taking a different sample
with repetitions from our original data. (For instance, random forest builds the trees using
repeated bootstrap samples.) On a machine learning class one of my class mates asked what
percentage of the original data shows up in the bootstrapped sample. Let’s have a look!&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import random
import matplotlib.pyplot as plt
%matplotlib inline
import pandas as pd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In bootstrap samples we select elements from our original sample in the same number as the
size of the original data at hand using repetitions. This is a poster case for Central Limit
Theorem, thus the expected value of our sample mean will be bang in line with the mean of
the original data.&lt;/p&gt;

&lt;p&gt;Let’s have a dataset of 1,000 observations, from which we take 10,000 samples of 1,000 elements.
This number of sampling helps us visualize and calculate our sample properties.&lt;/p&gt;

&lt;p&gt;When bootstrapping, some original observations show up multiple times and some others will
be missing from the bootstrapped samples. We are about finding the number of distinct elements
in the bootstrapped samples. If the samples are lists, then turning these lists into sets
helps us calculate the number of distinct elements, since sets don’t have the same value
more than once.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sample_sizes = []
for i in range(10000):
    sample = []
    for j in range(1000):
        sample.append(round(random.randint(1,1000)))
    sample_sizes.append(len(set(sample)))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Turning the sample size list into a data frame we can easily get the most basic statistics.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = pd.DataFrame(sample_sizes)
df.columns = [&amp;quot;sample sizes&amp;quot;]
df.describe().style.format(&#39;{:.2f}&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://peterduronelly.github.io/img/04_bootstrap_01_resized.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We have taken 10,000 samples of 1,000 observations with repetitions from the 1,000 observations.
On average, 632 observations show up in the bootstrapped samples, and this number is between
625 and 639 in the half of the samples. These results are extremely stable: if you run this
simulation multiple times the average will always be 632.&lt;/p&gt;

&lt;p&gt;It shouldn’t come as a surprise that the distribution of the number of distinct elements is almost
perfectly normal.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig = plt.figure(figsize = (10,6))

plt.hist(df[&amp;quot;sample sizes&amp;quot;], bins = list(range(590, 680, 2)), rwidth=0.9, color = &#39;k&#39;)
plt.title(&amp;quot;Bootstrap sample sizes\noriginal sample size = 1000&amp;quot;)
plt.xlabel(&amp;quot;number of distinct elements in the sample&amp;quot;)
plt.ylabel(&amp;quot;frequency&amp;quot;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://peterduronelly.github.io/img/04_bootstrap_02_resized.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Codes are at the &lt;a href=&#34;https://github.com/peterduronelly/blogcodes/blob/master/04-Bootstrap-Samples.ipynb&#34; target=&#34;_blank&#34;&gt;usual place&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Clustering on a Dissimilarity Matrix</title>
      <link>https://peterduronelly.github.io/post/clustering-on-dissimilarity-matrix/</link>
      <pubDate>Fri, 14 Sep 2018 10:33:36 +0200</pubDate>
      
      <guid>https://peterduronelly.github.io/post/clustering-on-dissimilarity-matrix/</guid>
      <description>&lt;p&gt;Clustering is one of the well-known unsupervised learning tools. In the standard case you have an observation matrix
where observations are in rows and variables which describe them are in columns. But data can also be structured in a different
way, just like the distance matrix on a map. In this case observations are by &lt;em&gt;both rows and columns&lt;/em&gt; and each element in the
observation matrix is a measure of distance, or &lt;em&gt;dissimilarity&lt;/em&gt;, between any two observations. This structure can also serve as
a basis for clustering, just as you can cluster cities based on the respective distances between any two of them.&lt;/p&gt;

&lt;p&gt;When I first encountered this problem I did not find a solution in the standard &lt;em&gt;scikit-learn&lt;/em&gt; library which you automatically call
when doing a clustering exercise. &lt;em&gt;SciPy&lt;/em&gt;, fortunately, has a solution. Let&amp;rsquo;s see how it works!&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import math
from scipy.cluster.hierarchy import linkage, dendrogram, fcluster
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The exercise is done on the hundred largest US cities. The list is coming from
&lt;a href=&#34;https://gist.github.com/Miserlou/c5cd8364bf9b2420bb29#file-cities-json&#34; target=&#34;_blank&#34;&gt;https://gist.github.com/Miserlou/c5cd8364bf9b2420bb29#file-cities-json&lt;/a&gt; in a very neat format.
(Thanks to &lt;a href=&#34;https://gist.github.com/Miserlou&#34; target=&#34;_blank&#34;&gt;Rick Jones&lt;/a&gt; for the data!)
It includes not only the city, the state and the population but the latitude/longitude coordinates of the cities
wich can be used to calculate an approximate distance measure between the cities.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;filepath = ...
cities = pd.read_json(filepath)
cities.head()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://peterduronelly.github.io/img/03_clustering_dissimilarity_01_resized.jpg&#34; alt=&#34;alt text&#34; /&gt;&lt;/p&gt;

&lt;p&gt;By selecting the top 100 cities we get sufficient number of observations and we are still able to interpret the result through simple
visualization.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;top100 = cities[0:100]

fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
ax = plt.scatter(top100[&amp;quot;longitude&amp;quot;], top100[&amp;quot;latitude&amp;quot;])
plt.title(&amp;quot;The Locations of the 100 Largest US Cities&amp;quot;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://peterduronelly.github.io/img/03_clustering_dissimilarity_02_resized.jpg&#34; alt=&#34;alt text&#34; /&gt;&lt;/p&gt;

&lt;p&gt;A more fancy map could be generated using the &lt;a href=&#34;https://python-visualization.github.io/folium/docs-v0.6.0/&#34; target=&#34;_blank&#34;&gt;&lt;em&gt;folium&lt;/em&gt; package&lt;/a&gt; but this
simple scatter plot is sufficient for our purposes. As all real, non-simulated data, our city dataset does not show a very obvious pattern.
Cities are scattered seemingly randomly on the map, and we have two outliers, Anchorage on the North-West and Honolulu on the South-West, which are
very far from all other points.&lt;/p&gt;

&lt;p&gt;Now let&amp;rsquo;s calculate the distances between the cities using the geopy package. The distances can be interpreted as &lt;em&gt;dissimilarities&lt;/em&gt;:
the larger the distance between two observations the less similar they are. This is may literally not be the case with cities
(two distant cities can easily be alike), but when we think in a more abstract space the concept makes sense.
Also, city clusters are interpreted in terms of geographical distances, so using them in this exercise helps us
understand the method and its results.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;distance_matrix = np.empty(shape = [100,100])

for i in range(100):
    for j in range(100):
        coords_i = (top100.iloc[i][&amp;quot;latitude&amp;quot;], top100.iloc[i][&amp;quot;longitude&amp;quot;])
        coords_j = (top100.iloc[j][&amp;quot;latitude&amp;quot;], top100.iloc[j][&amp;quot;longitude&amp;quot;])
        distance_matrix[i,j] = geopy.distance.vincenty(coords_i, coords_j).km
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Before we move on to build the clusters and visualize the results let&amp;rsquo;s create a list of city names where the name of each city is
followed by the state. This helps us interpret the cluster tree, the so-called &lt;em&gt;&amp;lsquo;dendrogram&amp;rsquo;&lt;/em&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cities[&amp;quot;city, state&amp;quot;] = top100[&amp;quot;city&amp;quot;].map(str) + &amp;quot;, &amp;quot; + top100[&amp;quot;state&amp;quot;]
citynames = cities[&amp;quot;city, state&amp;quot;][0:100].tolist()
citynames[0:10]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;[&amp;lsquo;New York, New York&amp;rsquo;,&lt;br&gt;
&amp;nbsp;&amp;lsquo;Los Angeles, California&amp;rsquo;, &lt;br&gt;
&amp;nbsp; &amp;lsquo;Chicago, Illinois&amp;rsquo;, &lt;br&gt;
&amp;nbsp;&amp;lsquo;Houston, Texas&amp;rsquo;, &lt;br&gt;
&amp;nbsp;&amp;lsquo;Philadelphia, Pennsylvania&amp;rsquo;, &lt;br&gt;
&amp;nbsp;&amp;lsquo;Phoenix, Arizona&amp;rsquo;,&lt;br&gt;
&amp;nbsp;&amp;lsquo;San Antonio, Texas&amp;rsquo;,&lt;br&gt;
&amp;nbsp;&amp;lsquo;San Diego, California&amp;rsquo;,&lt;br&gt;
&amp;nbsp;&amp;lsquo;Dallas, Texas&amp;rsquo;,&lt;br&gt;
&amp;nbsp;&amp;lsquo;San Jose, California&amp;rsquo;]&lt;/p&gt;

&lt;p&gt;No we get to the most important part! The meat of the module is the &lt;em&gt;linkage function&lt;/em&gt; (&lt;em&gt;scipy.cluster.hierarchy.linkage(…)&lt;/em&gt;),
which runs the actual clustering algorithms. It can handle both an &lt;em&gt;n x n&lt;/em&gt; distance matrix and a regular &lt;em&gt;n x m&lt;/em&gt; observation matrix
as input. If we form the clusters on a distance matrix, we need its condensed format: the upper diagonal elements
(excluding the diagonals) have to be converted into a vector (a list) with a length of &lt;em&gt;n(n-1)/2&lt;/em&gt;.
The output is a &lt;em&gt;(n-1)x4&lt;/em&gt; NumPy array &lt;strong&gt;&lt;em&gt;Z&lt;/em&gt;&lt;/strong&gt;, called &lt;em&gt;linkage matrix&lt;/em&gt;, which contains the hierarchical clustering algorithm’s encoded results.&lt;/p&gt;

&lt;p&gt;Each row in &lt;strong&gt;&lt;em&gt;Z&lt;/em&gt;&lt;/strong&gt; represents one iteration. In each row &lt;em&gt;i&lt;/em&gt;, clusters with indices of &lt;strong&gt;&lt;em&gt;Z&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;[i,0]&lt;/em&gt; and &lt;strong&gt;&lt;em&gt;Z&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;[i,1]&lt;/em&gt; are combined in a
new cluster, based on the distance between them which is in &lt;strong&gt;&lt;em&gt;Z&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;[i,2]&lt;/em&gt;. The number of elements in the newly formed cluster is &lt;strong&gt;&lt;em&gt;Z&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;[i,3]&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;This is a &lt;em&gt;hierarchical clustering&lt;/em&gt; method: we start with elementary clusters (the observations) which are
merged into larger and larger clusters. At the end all observations form a single cluster.&lt;/p&gt;

&lt;p&gt;Clusters contain observations which are close to each other. Closeness can be defined between observations (cluster elements) and the
clusters themselves. To calculate distances between two clusters we need to define two parameters:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We need to define a &lt;em&gt;distance metric&lt;/em&gt;, which measures the distances between the elements of one cluster and
the elements of the other cluster. The default is the Euclidean distance which is sufficient for our clustering project.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;We also need to define a &lt;em&gt;method&lt;/em&gt; to sum up the distances between the individual elements of two clusters to come up
with a single value for the distance between them. Just as in the case of the measures of central tendency, we are looking
for the &lt;em&gt;typical distance&lt;/em&gt; between the elements of any two clusters. The &lt;em&gt;method&lt;/em&gt; sets how this typical distance is defined.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While the distance metric can also make a difference, the most important parameter in cluster formation is the &lt;em&gt;method&lt;/em&gt;.
There are various options (methods) to define how far two clusters are from each other: the distance between the two closest
elements, or the one between two most distant ones, or some sort of average of the distances between elements of cluster
&lt;em&gt;a&lt;/em&gt; and cluster &lt;em&gt;b&lt;/em&gt;, etc. If cluster &lt;em&gt;a&lt;/em&gt; and cluster &lt;em&gt;b&lt;/em&gt; have &lt;em&gt;u&lt;/em&gt; and &lt;em&gt;v&lt;/em&gt; number of elements, respectively, we will have &lt;em&gt;u*v&lt;/em&gt;
distances which we can use to find the distance between clusters &lt;em&gt;a&lt;/em&gt; and &lt;em&gt;b&lt;/em&gt;. This is the modeler’s choice and, as I will
show it later, it can create markedly different cluster structures. The methods are very well explained in the &lt;a href=&#34;http://scipy.github.io/devdocs/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage&#34; target=&#34;_blank&#34;&gt;SciPy
documentation&lt;/a&gt;
and I urge you to check it out. At this point, however, we only need to keep in mind that the method is the most important
parameter we need to set.&lt;/p&gt;

&lt;p&gt;The most evident difference between clustering methods is how they handle outliers (Anchorage and Honolulu). We will see
that merging the mainland and the coastal areas will be completely different in some cases.&lt;/p&gt;

&lt;p&gt;And now comes the trick! SciPy does not use the whole dissimilarity matrix for the calculations, only a list of the upper
diagonal elements. The length of the list is &lt;em&gt;n(n-1)/2&lt;/em&gt; and SciPy automatically calculates the number of observations
it needs to cluster.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;l = [] # upper triangular elements of the distance matrix 

for i in range(0, distance_matrix.shape[0]):
    for j in range(i + 1, distance_matrix.shape[0]):
        l.append(distance_matrix[i,j])

Z = linkage(l, method=&amp;quot;ward&amp;quot;) # this is the meat of the thing!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once we have the linkage matrix, which is basically a log of which observation or cluster got merged in each step,
we can visualize the results.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig = plt.figure(figsize=(25, 10))
plt.title(&amp;quot;Hierarchical Clustering Dendrogram\nLinkage = ward&amp;quot;)
plt.xlabel(&#39;city, state&#39;)
plt.ylabel(&#39;distance&#39;)
dn = dendrogram(Z,
               labels = citynames,
    leaf_rotation=90.,
    leaf_font_size=10.,
    show_contracted=True)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://peterduronelly.github.io/img/03_clustering_dissimilarity_03_resized.jpg&#34; alt=&#34;alt text&#34; /&gt;&lt;/p&gt;

&lt;p&gt;First we used the &lt;em&gt;&amp;lsquo;ward&amp;rsquo;&lt;/em&gt; method which is an optimization algorithm: it aims to minimize within-cluster variance.
Cities belong to two major clusters: the ones with green linkages can be called &amp;lsquo;The West&amp;rsquo;, while the ones with red linkages
can be considered &amp;lsquo;The East&amp;rsquo; or &amp;lsquo;East + Midwest&amp;rsquo;.
Our outliers (Anchorage and Honolulu) belong to the West, which makes sense, and they form a
cluster together before being linked to the other Western cities. Before they are linked, however, mainland Western/West Coast
cities are merged into clusters at higher and higher levels of aggregation. Only then come the two overseas cities.&lt;/p&gt;

&lt;p&gt;Cities in Arizona are merged into their own clusters, then Nevada and California. Boise City, Idaho goes to Nevada/California
after being merged with the cluster formed by Seattle and Portland. Everything else belong to the East Coast,
including cities in Minnesota, Wisconsin and Texas. The dendrogram is sort of ‘smooth’ or ‘balanced’, thanks to
the algorithm&amp;rsquo;s variance minimalization.&lt;/p&gt;

&lt;p&gt;The vertical axis shows the distance, or the level of dissimilarities between the clusters. The longer the vertical lines the more
dissimilar the two clusters which are merged in that step. The lenght of the two vertical blue lines shows
that this method considers the West Coast, which includes the outlier cities, to be very different from everything else.&lt;/p&gt;

&lt;p&gt;The structure changes a lot when cluster distances are defined as the average distance between the members of the two clusters.
The fact that cluster formation is not variance-optimized results in an unbalanced-looking tree.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Z = linkage(l, method=&amp;quot;average&amp;quot;)

fig = plt.figure(figsize=(25, 10))
plt.title(&amp;quot;Hierarchical Clustering Dendrogram\nLinkage = average&amp;quot;)
plt.xlabel(&#39;city, state&#39;)
plt.ylabel(&#39;distance&#39;)
dn = dendrogram(Z,
               labels = citynames,
    leaf_rotation=90.,
    leaf_font_size=10.,
    show_contracted=True)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://peterduronelly.github.io/img/03_clustering_dissimilarity_04_resized.jpg&#34; alt=&#34;alt text&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The most visible difference, as I mentioned before, is how the outliers are handled. Here they are merged
with other mainland cities only at the end, &lt;em&gt;only after&lt;/em&gt; the other 98 cities have been joined together. There is
more emphasis on their uniqueness than before, when within-cluster variance ruled cluster formation. East
Coast and West Coast are made up from mostly the same cities as before, but their aggregation, from individual
cities to larger and larger formations has a somewhat different profile.&lt;/p&gt;

&lt;p&gt;It is also interesting to see how the trio of Boise City and Seattle &amp;amp; Portland is linked to Arizona and California.
In the ‘ward’ optimization mechanism they are merged with California before they together are put together with
Arizona. In this latter case Arizona, California and Nevada are merged before the trio of Boise, Seattle &amp;amp; Portland
joins them as the last West Coast areas.&lt;/p&gt;

&lt;p&gt;No one is better than the other, and both the nature of the data and the target of the modeling exercise are
important in selecting the best method. In the case of the US cities, for example, if we think that the mainland
cities share more with each other than with Anchorage or Honolulu, ‘average’ is a better choice. If we simply want
a split into East and West, then ‘ward’ will be our method.&lt;/p&gt;

&lt;p&gt;As a summary: clustering is possible in Python when the data does not come as an &lt;em&gt;n x p&lt;/em&gt; matrix of &lt;em&gt;n&lt;/em&gt; observations
and &lt;em&gt;p&lt;/em&gt; variables, but as an &lt;em&gt;n x n&lt;/em&gt; dissimilarity or distance matrix. The home of the algorithm is the SciPy package,
and depending on the method, we can have very different results.&lt;/p&gt;

&lt;p&gt;The codes are at the &lt;a href=&#34;https://github.com/peterduronelly/blogcodes/blob/master/03-Clustering-Based-On-A-Distance-Matrix.ipynb&#34; target=&#34;_blank&#34;&gt;usual place&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Text Complexity Analysis of Beatles Lyrics With R</title>
      <link>https://peterduronelly.github.io/post/text-complexity-analysis-of-beatles-lyrics-with-r/</link>
      <pubDate>Thu, 13 Sep 2018 13:39:08 +0200</pubDate>
      
      <guid>https://peterduronelly.github.io/post/text-complexity-analysis-of-beatles-lyrics-with-r/</guid>
      <description>

&lt;p&gt;The Beatles became a hit through its sometimes simple but always powerful music but it has never been famous for its poetry. The group&amp;rsquo;s
lyrics, however, did change during the band&amp;rsquo;s short existence and we can use text analysis to track these changes.
This post is about measuring the change in the complexity of the group&amp;rsquo;s lyrics, from the Please, Please Me
to the Abbey Road albums, showing how we can use basic data secience tools to find really fancy patterns in unstructured
text data.&lt;/p&gt;

&lt;p&gt;This piece is a shortened version of a final project for a Data Science
on Unstructured Text Data course held by Facebook&amp;rsquo;s Eduardo Ariño de la
Rubia. The course introduced the tidytext package and the basics of text
analysis in R. At the end of the course students had to present their
skills through a freely chosen analysis project. Although tidytext does
not directly cover text complexity, to me it was somehow an obvious
choice.&lt;/p&gt;

&lt;p&gt;This is a technical post, but most of the hard stuff is concentrated in the
code blocks. If you are only interested in the power of data science, feel free
to disregard these blocks and concentrate on the text and the plots only. You
will still be able to get the message.&lt;/p&gt;

&lt;p&gt;When you learn English as a foreign language you inevitably bump into
The Beatles early on. The songs are well known, and even a beginner
student can easily understand the lyrics. This is not only because the
members were singing in nice English, but because their early text is
damned simple. &amp;lsquo;She loves you, yeah, yeah, yeah.&amp;rsquo; Not that of a
challenging text, right?&lt;/p&gt;

&lt;p&gt;But when you listen to The Beatles a little more, you realize that as
time went by their songs got more and more sophisticated. &amp;lsquo;Strawberry
Fields Forever&amp;rsquo; does have more depth than &amp;lsquo;A Hard Day&amp;rsquo;s Night&amp;rsquo;. Since we
are into data science, it is obvious to ask: can we measure this change
in sophistication? Can we trace the development also in their lyrics? As
the members went from their early twenties towards their thirties, did
they move from their simple but powerful origins towards something more
mature?&lt;/p&gt;

&lt;p&gt;In the next few lines I am analyzing The Beatles&amp;rsquo; thirteen albums of
&amp;lsquo;core catalogues&amp;rsquo; from &amp;lsquo;Please Please Me&amp;rsquo; to &amp;lsquo;Let It Be&amp;rsquo;, published
between 1964 and 1970. It is amazing but the most influential pop group
of all times existed for less than a decade, and this short period was
enough to issue thirteen albums and to turn the world upside down. The
group had quite a few extra collections, live recordings and greatest
hit compilations (the last one, according to wikipedia, in 2013), but
these thirteen albums make up the the actual works of the group.&lt;/p&gt;

&lt;p&gt;For the project I used the newly developed &lt;a href=&#34;https://github.com/JosiahParry/geniusR&#34; target=&#34;_blank&#34;&gt;geniusR package&lt;/a&gt; by Josiah Parry, which downloads lyrics and
metadata from the genius.com homepage. This package was of enormous help
for the analysis.
&lt;br&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;a-glance-at-the-beatles&#34;&gt;A Glance At The Beatles&lt;/h2&gt;

&lt;p&gt;As a starter I imported the necessary packages. I like starting all
analysis with the packages, having them in one single chunk for a better
overview. Also, when I later need to add further packages I just scroll
back to the first chunk to enter the extra library command.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-.r&#34;&gt;library(geniusR)
library(tidyverse)
library(tidytext)
library(tidyr)
library(tibble)
library(dplyr)
library(purrr)
library(stringr)
library(syllable)
library(ggplot2)
library(scales)
library(gridExtra)
library(lsa)
library(rlist)
library(data.table)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Downloading the text is simple with geniusR: you define the artist, the
number of albums to download, and the album titles. Album titles should
be entered as the last part of the urls on the genius.com webpage
without hyphens. Apostrophes are omitted. You can also download albums
of multiple artists entering the author, # of albums multiple
times as a vector. See the documentation and Josiah&amp;rsquo;s github for
details.&lt;/p&gt;

&lt;p&gt;It takes a while until your text downloads, but you end up with a nice
(tidy!) tibble which serves as the basis for further analysis.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-.r&#34;&gt;albums &amp;lt;-  tibble(
  artist = 
    rep(&amp;quot;The Beatles&amp;quot;, 13),
  album = c(
    &amp;quot;Please Please Me&amp;quot;, &amp;quot;With The Beatles&amp;quot;, &amp;quot;A Hard Day s Night&amp;quot;,
    &amp;quot;Beatles For Sale&amp;quot;, &amp;quot;Help&amp;quot;, &amp;quot;Rubber Soul&amp;quot;,
    &amp;quot;Revolver&amp;quot;, &amp;quot;Sgt Pepper s Lonely Hearts Club Band&amp;quot;, &amp;quot;Magical Mystery Tour&amp;quot;,
    &amp;quot;The Beatles The White Album&amp;quot;, &amp;quot;Yellow Submarine&amp;quot;, &amp;quot;Abbey Road&amp;quot;,
    &amp;quot;Let It Be&amp;quot;
  )
)

album_lyrics &amp;lt;- albums %&amp;gt;% 
  mutate(tracks = map2(artist, album, genius_album))

beatles_lyrics &amp;lt;- album_lyrics %&amp;gt;% 
  unnest(tracks) 

beatles_albums &amp;lt;- beatles_lyrics %&amp;gt;%
  distinct(album)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As the most obvious starting point of any text analysis, I checked the
per album frequencies of non-stop words across these albums. (For the wider
audience: &lt;a href=&#34;https://en.wikipedia.org/wiki/Stop_words&#34; target=&#34;_blank&#34;&gt;stop words&lt;/a&gt; are the most common, &amp;lsquo;functional&amp;rsquo; words in a language.) In order to
draw an arch of change, I plotted simple word frequency charts for
Please Please Me (1963), Help (1965), Magical Mystery Tour (1967) and
Let It Be (1970). Can we see any difference in the words used?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-.r&#34;&gt;tidy_beatles &amp;lt;- beatles_lyrics %&amp;gt;%
  unnest_tokens(word, text) %&amp;gt;%
  filter(nchar(word)&amp;gt;2)%&amp;gt;%
  anti_join(stop_words) %&amp;gt;%
  group_by(album) %&amp;gt;%
  count(word, sort = TRUE) %&amp;gt;%
  ungroup()

total_words &amp;lt;- tidy_beatles %&amp;gt;% 
  group_by(album) %&amp;gt;% 
  summarize(total = sum(n))

tidy_beatles &amp;lt;- left_join(tidy_beatles, total_words)

tidy_beatles &amp;lt;- tidy_beatles %&amp;gt;%
  mutate(freq = n / total)
  
ppm &amp;lt;- tidy_beatles %&amp;gt;%
  filter(str_detect(album, &amp;quot;Please&amp;quot;))%&amp;gt;%
  arrange(desc(freq)) %&amp;gt;%
  top_n(10)%&amp;gt;%
  mutate(word = factor(word, levels = rev(unique(word)))) %&amp;gt;% 
  ggplot(aes(word, freq, fill=I(&amp;quot;steelblue3&amp;quot;), col=I(&amp;quot;black&amp;quot;))) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = &amp;quot;frequency&amp;quot;) +
  coord_flip() + 
  theme_bw() + 
  labs(title = &amp;quot;Word frequency in Please Please Me&amp;quot;) + 
  theme(plot.title = element_text(size = rel(1))) +
  scale_y_continuous(labels = percent)

help &amp;lt;- tidy_beatles %&amp;gt;%
  filter(str_detect(album, &amp;quot;Help&amp;quot;))%&amp;gt;%
  arrange(desc(freq)) %&amp;gt;%
  top_n(10)%&amp;gt;%
  mutate(word = factor(word, levels = rev(unique(word)))) %&amp;gt;% 
  ggplot(aes(word, freq, fill=I(&amp;quot;steelblue3&amp;quot;), col=I(&amp;quot;black&amp;quot;))) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = &amp;quot;frequency&amp;quot;) +
  coord_flip() + 
  theme_bw() + 
  labs(title = &amp;quot;Word frequency in Help&amp;quot;) + 
  theme(plot.title = element_text(size = rel(1))) +
  scale_y_continuous(labels = percent)

mys &amp;lt;- tidy_beatles %&amp;gt;%
  filter(str_detect(album, &amp;quot;Mystery&amp;quot;))%&amp;gt;%
  arrange(desc(freq)) %&amp;gt;%
  top_n(10)%&amp;gt;%
  mutate(word = factor(word, levels = rev(unique(word)))) %&amp;gt;% 
  ggplot(aes(word, freq, fill=I(&amp;quot;steelblue3&amp;quot;), col=I(&amp;quot;black&amp;quot;))) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = &amp;quot;frequency&amp;quot;) +
  coord_flip() + 
  theme_bw() + 
  labs(title = &amp;quot;Word frequency in Magical Myster Tour&amp;quot;) + 
  theme(plot.title = element_text(size = rel(1))) +
  scale_y_continuous(labels = percent)

lib &amp;lt;- tidy_beatles %&amp;gt;%
  filter(str_detect(album, &amp;quot;Let&amp;quot;))%&amp;gt;%
  arrange(desc(freq)) %&amp;gt;%
  top_n(10)%&amp;gt;%
  mutate(word = factor(word, levels = rev(unique(word)))) %&amp;gt;% 
  ggplot(aes(word, freq, fill=I(&amp;quot;steelblue3&amp;quot;), col=I(&amp;quot;black&amp;quot;))) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = &amp;quot;frequency&amp;quot;) +
  coord_flip() + 
  theme_bw() + 
  labs(title = &amp;quot;Word frequency in Let It Be&amp;quot;) + 
  theme(plot.title = element_text(size = rel(1))) +
  scale_y_continuous(labels = percent)

grid.arrange(ppm, help, mys, lib, nrow = 2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://peterduronelly.github.io/img/02_beatles_01_resize.jpg&#34; alt=&#34;alt text&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Love makes it into the first 10 in three of the albums, leading the pack
in Please Please Me and, to my little surprise, Magical Mystery Tour.
Interestingly, it is missing from the top 10 in Let It Be, the last
album. It looks, love was not of primary interest by 1970, the year when
the members decided to go their own separate ways. Of course, per album
word frequency depends largely on the songs&amp;rsquo; topic selection: &amp;lsquo;mother&amp;rsquo;
goes to number 2 in Magical Mystery Tour due to the many repetitions of
the line &amp;lsquo;Your mother should know&amp;rsquo; in the song of the same title.&lt;/p&gt;

&lt;p&gt;This simple exercise shows that working with lyrics can be very tricky.
Lines are repeated very often, and melody dominates sentence building.
As a matter of fact, sentences can only be poorly defined by regular
text analysis algorithms in songs, which, as we will see later, makes
measuring text complexity somewhat difficult.
&lt;br&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;measuring-similarity-across-core-albums&#34;&gt;Measuring Similarity Across Core Albums&lt;/h2&gt;

&lt;p&gt;In order to asses how much the group changed over the course of these
seven years I measured the similarity of each album to Please Please Me,
the very first LP. More and more sophisticated lyrics would result in
larger and larger differences in text, measured by cosine similarity.&lt;/p&gt;

&lt;p&gt;I calculated cosine similarity based on word frequency vectors, where
each album is vector of frequencies of words in a union of sets of words
from each album. The word list is a product of a full join of all words
from the all the albums, and the cosine for each album is a similarity
measure between that particular album and the benchmark Please Please
Me. This word list excludes stop words, of course.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-.r&#34;&gt;cos &amp;lt;- tidy_beatles %&amp;gt;%
  select(album, word, freq)

cos_w &amp;lt;- spread(cos, key = album, value = freq) # This is a matrix where entries are frequencies of words in the the various albums. NAs are replaced by zeros in the next command. 

cos_w[is.na(cos_w)] &amp;lt;- 0

cos_w &amp;lt;- cos_w %&amp;gt;%
  select(-word)

title &amp;lt;- beatles_albums[[1]][1]
a &amp;lt;- cos_w %&amp;gt;% select(as.character(title))


cosines &amp;lt;- data.table(album = character(),
                      cosines = numeric())

for(i in 2:nrow(beatles_albums)){
  title1 &amp;lt;- beatles_albums[[1]][i]
  l &amp;lt;- list(title1)
  b &amp;lt;- cos_w %&amp;gt;% select(as.character(title1))
  l &amp;lt;- list.append(l, round(sum(a*b)/sqrt(sum(a^2)*sum(b^2)),3))
  cosines &amp;lt;- rbind(cosines, l)
}

cosines &amp;lt;- data.frame(cosines)

cosines &amp;lt;- cosines%&amp;gt;%
  arrange(desc(cosines))

cosines$album &amp;lt;- factor(cosines$album, levels = cosines$album[order(cosines$cosines)])

ggplot(cosines) + 
  geom_col(aes(album, cosines, fill=I(&amp;quot;steelblue3&amp;quot;), col=I(&amp;quot;black&amp;quot;)),show.legend = F) +
  theme_bw() + coord_flip() + 
  labs(title = &amp;quot;Cosine similarities with Please Please Me&amp;quot;, y = &amp;quot;cosine similarity&amp;quot;) + 
  theme(plot.title = element_text(size = rel(1.25))) + 
  ylim(0,1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://peterduronelly.github.io/img/02_beatles_02_resize.jpg&#34; alt=&#34;alt text&#34; /&gt;&lt;/p&gt;

&lt;p&gt;It is not surprising that &amp;lsquo;A Hard Day&amp;rsquo;s Night&amp;rsquo; is very similar to Please
Please Me, but Abbey Road (no. 12) also shares a lot with it.
Sgt. Pepper&amp;rsquo;s (no. 8) is an interesting album: it is the most distinct
one amongst the core and, as we will see it later, by some measure it
has more complex lyrics than any of the other LPs. (St. Pepper&amp;rsquo;s was the
first album of the &amp;lsquo;studio years&amp;rsquo;, when the band was finally freed from
the burden of permanent touring. It was an experimental album which took
700 hours to record.)
&lt;br&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;text-complexity&#34;&gt;Text Complexity&lt;/h2&gt;

&lt;p&gt;Next I turned to text complexity. Can we see an arch of change as the
group gets older, grows confidence, and starts to have something else to
say than &amp;lsquo;I love you&amp;rsquo;?&lt;/p&gt;

&lt;p&gt;There are various measures of text complexity, and all aims to assess
the readability of prosaic test. Since these metrics have been developed
to prose, applying them to lyrics, which is basically poetry, is not
straightforward. In songs and poems text serves the melody (rhythm),
and, for the lack of proper punctuation, standard algorithms cannot
detect where sentences start and end. The basis of complexity metrics is
usually the number of words in a sentence, the lengths of the words, and
the ratio of complex words within all words. These measures are focusing
on how the text is built, and they don&amp;rsquo;t filter for stop words, as the
use of these stop words is also a sign of sophistication. These are,
obviously, the simplest measures and they do not account for other
dimensions of complexity, like the frequency with which people use these
words or the types of texts where these words are typical. By these
metrics, for instance, the word &amp;lsquo;championship&amp;rsquo; is way more elaborate
than the word &amp;lsquo;cat&amp;rsquo;, although the context where the former is mostly
used may not be more academic than the one for the latter.&lt;/p&gt;

&lt;p&gt;In this exercise I use the modified version of two complexity measures.
The &amp;lsquo;&lt;strong&gt;Automated Readability Index&lt;/strong&gt;&amp;rsquo; uses characters, words and
sentences so, that&lt;/p&gt;

&lt;p&gt;&lt;div class=text-center&gt;
ARI = 4.71 x (characters/words) + 0.5 x (words/sentences) - 21.43
&lt;/div&gt;
&lt;br&gt;
The &amp;lsquo;&lt;strong&gt;Gunning Fog&lt;/strong&gt;&amp;rsquo; score is based on words, sentences, and the ratio
of complex words:
&lt;br&gt;
&lt;div class=text-center&gt;
Gunning Fog = 0.4 x ( (words/sentences) + 100 x (complex words/words) )
&lt;/div&gt;
&lt;br&gt;
A word is considered to be complex if it has at least 3 syllables.
Higher complexity scores indicate more complicated text.&lt;/p&gt;

&lt;p&gt;Since sentences are loosely defined in lyrics, I replaced them by lines,
despite that sentences can be of multiple lines. Lines are the main
building blocks of song text, so using them as a proxy is a viable
option. As the original formulas are this way &amp;lsquo;modified&amp;rsquo;, I denote these
measures as mARI and mGunningFog. In this form they are probably more
imperfect as they originally are, but they do show meaningful patterns
even in our case.&lt;/p&gt;

&lt;p&gt;In order to calculate these measures for each album I looped through the
album titles and selected distinct lines for analysis. The reason for
using distinct lines is that in songs lines are many times repeated for
the sake of the melody, serving as chorus, and complete verses can be
reused to fill the melody with sufficient amount of text for the vocals.&lt;/p&gt;

&lt;p&gt;For the complexity metrics I used the &amp;lsquo;syllable&amp;rsquo; package, which produces
certain text statistics from which these scores can be calculated. The
result is in the next chart.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-.r&#34;&gt;text_complexity &amp;lt;- data.table(album = character(),
                              mGunningFog = numeric(),
                              mARI = numeric())

for(i in 1:nrow(beatles_albums)){
  name &amp;lt;- beatles_albums[[1]][i]
  l &amp;lt;-  list(name)
  temp_lyr &amp;lt;- beatles_lyrics %&amp;gt;%
    filter(album == as.character(name)) %&amp;gt;%
    distinct(text)
  rwstat &amp;lt;- readability_word_stats(temp_lyr[,1])
  l &amp;lt;-  list.append(l, 0.4*(rwstat$n.words/nrow(temp_lyr)) +
                      100*(rwstat$n.complexes/rwstat$n.words))
  l &amp;lt;- list.append(l, 4.71*(rwstat$n.chars/rwstat$n.words) +
                     0.5*(rwstat$n.words/nrow(temp_lyr)) - 21.43)
  text_complexity &amp;lt;- rbind(text_complexity, l)
}

ggplot(data = text_complexity, aes(mARI, mGunningFog)) + 
  geom_point(color = &amp;quot;darkblue&amp;quot;) +
  geom_text(aes(x = mARI, y = mGunningFog, label = album), hjust=1, vjust=-0.5) + 
  theme_bw() + labs(title = &amp;quot;Text complexity of Beatles albums&amp;quot;) + 
  theme(plot.title = element_text(size = rel(1.25)))  +
  ylim(4,8) + xlim(-3,1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://peterduronelly.github.io/img/02_beatles_03.jpg&#34; alt=&#34;alt text&#34; /&gt;&lt;/p&gt;

&lt;p&gt;While the two metrics put the albums in slightly different orders, the
trend is obvious. The first albums (Please Please Me, A Hard Day&amp;rsquo;s
Night, With The Beatles) are of fairly simple text, but as time goes by
sophistication increases. Let It Be (no. 13) and Abbey Road (no. 12)
have fairly high readings by ARI, and Let It Be is the single most
complex according to Gunning Fox. Note that Sgt. Pepper (no. 8) is of
relatively high complexity: it is the most complex by ARI and the third
most complex by Gunning Fog. Remember, Sgt. Pepper is the most
dissimilar album compared to the Please Please Me benchmark by cosine
similarity.&lt;/p&gt;

&lt;p&gt;If we look at the publication dates of these albums (not shown here) we
do see a development in the group&amp;rsquo;s artistic performance. Four out of
the last five albums (&amp;lsquo;Magical Mystery Tour&amp;rsquo;, &amp;lsquo;The Beatles The White
Album&amp;rsquo;, &amp;lsquo;Abbey Road&amp;rsquo;, &amp;lsquo;Let It Be&amp;rsquo;) are in the top right corner of the
chart, while the first five LPs are in the bottom left.
&lt;br&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;

&lt;p&gt;This short analysis tries to provide an insight into the carrier of The
Beatles through their lyrics. Songs were very simple at the beginning,
but as their music got more mature, their lyrics followed on. Measures
of text complexity, just as those of similarity, can also reveal the
uniqueness of the St. Pepper&amp;rsquo;s album.&lt;/p&gt;

&lt;p&gt;Among the tools, tidytext, syllable, and of course the geniusR packages
were used to compile this report.&lt;/p&gt;

&lt;p&gt;Codes can be found at &lt;a href=&#34;https://github.com/peterduronelly/blogcodes/blob/master/02-Text-Complexity-Analysis-Of-Beatles_Lyrics-With-R.R&#34; target=&#34;_blank&#34;&gt;https://github.com/peterduronelly/blogcodes/blob/master/02-Text-Complexity-Analysis-Of-Beatles_Lyrics-With-R.R&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sparse Matrices in Python</title>
      <link>https://peterduronelly.github.io/post/sparse-matrices-in-python/</link>
      <pubDate>Thu, 13 Sep 2018 11:17:13 +0200</pubDate>
      
      <guid>https://peterduronelly.github.io/post/sparse-matrices-in-python/</guid>
      <description>&lt;p&gt;One of the things we need to manage in data analysis is recources. When we have large amounts of (&amp;lsquo;big&amp;rsquo;) data this can become a serious issue.
One of the cases when we need to consider whether we really need all the data we have is when we have a lot of zeros in our database, and these zeroes
happen to be irrelevant for our calculations. Python&amp;rsquo;s &lt;em&gt;SciPy&lt;/em&gt; library has a solution to store and handle &lt;strong&gt;&lt;em&gt;sparse&lt;/em&gt;&lt;/strong&gt; data matrices which contain a
large number of irrelevant zero values.&lt;/p&gt;

&lt;p&gt;In order to demonstrate how this works let&amp;rsquo;s first import the necessary packages.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
from scipy.sparse import coo_matrix
from scipy.sparse import save_npz
import matplotlib
import matplotlib.pyplot as plt
%matplotlib inline
import random
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Where would we come across sparse data? Let&amp;rsquo;s imagine you are running an e-commerce site where people buy various products. Some poeple buy only one item, others buy multiple products. You want to see how clients are related, or linked, to each other based on their purchase patterns: if some customers buy the same or almost the same set of products, these clients are similar.&lt;/p&gt;

&lt;p&gt;Moreover, these linkages create a network clients, and this network can be used for recommendations, coupons, etc. Without going into the details how such a network can be built and used let&amp;rsquo;s see how we get to sparse data when we analyze buying patterns.&lt;/p&gt;

&lt;p&gt;For the sake of simplicity let&amp;rsquo;s assume that there are 15 clients and 30 products. Each client buys between 2 and 8 pieces of items.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;clients = list(range(0,15)) # 15 clients
dic = {} # a dictionary of purchases: keys are clients and values are lists of items bought
for i in range(0,15):
    key = clients[i]
    random.seed(i)
    l = random.sample(range(0,30), random.randint(2,8))
    dic[key] = l
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can print out what itemms clients have purchased.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for key in dic.keys():
    print(dic[key])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;[12, 24, 13, 1, 8, 16, 15, 29]&lt;br&gt;
[18, 27, 25]&lt;br&gt;
[27, 1, 2, 29, 11, 5, 23, 21]&lt;br&gt;
[18, 17, 4]&lt;br&gt;
[9, 3, 23]&lt;br&gt;
[8, 23, 11, 25, 22, 28]&lt;br&gt;
[18, 26, 2, 15, 24, 8, 1, 0]&lt;br&gt;
[4, 12, 20, 1]&lt;br&gt;
[11, 12, 4]&lt;br&gt;
[19, 11, 8, 4, 5]&lt;br&gt;
[1, 13, 15, 18, 0, 6]&lt;br&gt;
[27, 17, 29, 24, 14]&lt;br&gt;
[8, 21, 16, 11, 4]&lt;br&gt;
[9, 21, 29, 25]&lt;br&gt;
[19, 22]&lt;br&gt;&lt;/p&gt;

&lt;p&gt;Clients No. 0, 2 and 6 have bought eight items, and only one has bought two. Now we can build a matrix of clients where the matrix elements show the
number of common items on their shopping history. Each  a&lt;sub&gt;ij&lt;/sub&gt;  element of this matrix tells the number of items bought both by client &lt;em&gt;i&lt;/em&gt;
and client &lt;em&gt;j&lt;/em&gt; . The  i&lt;sup&gt;th&lt;/sup&gt;  diagonal element shows the number of items bought buy client &lt;em&gt;i&lt;/em&gt;. This is going to be a symmetric matrix,
of course.&lt;/p&gt;

&lt;p&gt;To build the matrix we need to define the row and column indices of the non-zero matrix elements, and the values of the elements. These are going to be
lists of equal sizes, which serve as inputs for the sparse matrix. Since the matrix is symmetric, we don&amp;rsquo;t need to calculate and store the lower
diagonal elements to save space.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;r = 0 # the index of the row of the matrix
c = 0 # the index of the columns of the matrix
counter = 0
row_indices = [] # row indices of the non-zero values
column_indices = []  # column indices of the non-zero values
matrix_elements = [] # the non-zero values themselves

for key_r in dic.keys(): # key_r is the key for the rows
    x = dic[key_r]
    for key_c in dic.keys(): # key_c is the key for the columns
        if c &amp;gt;= r:
            y = dic[key_c]
            common_set = list(set(x) &amp;amp; set(y))
            common_set_size = len(common_set)
            if common_set_size &amp;gt; 0:
                row_indices.append(r)
                column_indices.append(c)
                matrix_elements.append(common_set_size)
        c = c + 1
    r = r + 1
    c = 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once we have the lists we can build the sparse matrix using the &lt;em&gt;coo_matrix&lt;/em&gt; function. This function takes numpy arrays as inputs so we need to
convert our lists to arrays. When building the matrix we need to add the non-zero elements first, then the indices as a tuple. Finally we need to
define the size (shape) of the matrix. The resulting object will be of type &lt;em&gt;scipy.sparse.coo.coo_matrix&lt;/em&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;row  = np.array(row_indices)
col  = np.array(column_indices)
val = np.array(matrix_elements)
mx = coo_matrix((val, (row, col)), shape=(15, 15))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can view the content of the matrix using the .toarray() command.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mx.toarray()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;array([[8, 0, 2, 0, 0, 1, 4, 2, 1, 1, 3, 2, 2, 1, 0],&lt;br&gt;
    &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;   [0, 3, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0],&lt;br&gt;
       &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; [0, 0, 8, 0, 1, 2, 2, 1, 1, 2, 1, 2, 2, 2, 0],&lt;br&gt;
       &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; [0, 0, 0, 3, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0],&lt;br&gt;
       &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; [0, 0, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0],&lt;br&gt;
       &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; [0, 0, 0, 0, 0, 6, 1, 0, 1, 2, 0, 0, 2, 1, 1],&lt;br&gt;
       &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; [0, 0, 0, 0, 0, 0, 8, 1, 0, 1, 4, 1, 1, 0, 0],&lt;br&gt;
       &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; [0, 0, 0, 0, 0, 0, 0, 4, 2, 1, 1, 0, 1, 0, 0],&lt;br&gt;
       &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; [0, 0, 0, 0, 0, 0, 0, 0, 3, 2, 0, 0, 2, 0, 0],&lt;br&gt;
       &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; [0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 3, 0, 1],&lt;br&gt;
       &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0],&lt;br&gt;
       &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 1, 0],&lt;br&gt;
       &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 1, 0],&lt;br&gt;
       &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0],&lt;br&gt;
       &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]])&lt;/p&gt;

&lt;p&gt;We can also visualize the matrix using the matplotlib library. This visualization shows where the non-zero values are located in the matrix,
but it does not make a difference based on their values. All non-zero entries are indicated by the same color.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.spy(mx)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://peterduronelly.github.io/img/spars.png&#34; alt=&#34;alternative text for search engines&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The chart has a slighlty different outlook if we use the matrix as a numpy array, and not as scipy.sparse.coo.coo_matrix object as plot input.
The chart is built a little faster and has a different coloring scheme. The visualization, however, does not help much when we have really large
amount of data, with thousands of rows and columns in our matrix: the screen resolution will not be able to differentiate between the white and
non-white areas. This makes this visulization tool kind of useless in those cases when we really need to use the sparse matrix fuctionality for our resource
management.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mx_as_array = mx.toarray()
plt.spy(mx_as_array)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://peterduronelly.github.io/img/spars_2.png&#34; alt=&#34;alt text&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Finally we can save the scipy.sparse.coo.coo_matrix object using the &lt;em&gt;save_npz()&lt;/em&gt; command. For later use, the load_npz() command imports the
sparse matrix to our project.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;save_npz(&#39;/path...&#39;, mx)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The codes can be find at &lt;a href=&#34;https://github.com/peterduronelly/blogcodes/blob/master/01-Sparse-Matrices-in-Python.ipynb&#34; target=&#34;_blank&#34;&gt;my github&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introduction</title>
      <link>https://peterduronelly.github.io/post/test/</link>
      <pubDate>Mon, 10 Sep 2018 11:55:58 +0200</pubDate>
      
      <guid>https://peterduronelly.github.io/post/test/</guid>
      <description>

&lt;h2 id=&#34;who-am-i&#34;&gt;Who Am I?&lt;/h2&gt;

&lt;div class=text-justify&gt;
I am originally trainded as an economist with a Finance major and a Statistics minor. I spent 21 years on financial markets as an analyst and asset manager. 
After a while, though, I got interested in disruptive technologies, mostly data and data science, which are closely linked to my previous academic studies. 
Statistics is especially close to my heart as I used to teach it to sophomore students on the Budapest University of Economics (currently Corvinus University).

To do something about my new passion, I quit my job in 2017 and completed a data science-focused master&#39;s program on the Central European University in Budapest (MSc in Business 
Analytics). The core of the program was Statistics, Machine Learning, some intro to neural networks, and data infrastructure in the cloud. The program was 
built in R and I also learnt Python (which I like better than R). 


&lt;/div&gt;

&lt;h2 id=&#34;why-do-i-write-a-blog&#34;&gt;Why Do I Write a Blog?&lt;/h2&gt;

&lt;div class=text-justify&gt;
I have encountered a lot of interesting things during my studies. I had superinteresting assignments for which I had to dig out a bunch of fancy solutions
in the docs, on stackoverflow, and from other people&#39;s blogs. This was when I decided to add some more  to the already large and rapidly expanding content
space on the net, so that I may be able to give some help to those who are looking for those things I was struggling with. Also, some of the things I did
was about really cool topics (like text complexity analysis, or clustering on a dissimilarity matrix) and I think they are worth to be shared. Finally,
there were some issues which took me months to solve and I hope these posts will shorten this time for those who find my posts during their search. 
&lt;/div&gt;

&lt;div class=text-justify&gt;
All blog codes will be on github in their entirety, under https://github.com/peterduronelly/blogcodes. 

&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
